{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deepDTnet.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wgqj9kraRjS8",
        "dkczPipwRjS-"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvSeRlr7RjSm"
      },
      "source": [
        "# Implémentation en diverses étapes de l'algorithme deepDTnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_3w6Pe9RjSu"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.autograd as ag\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "from scipy.optimize import minimize, NonlinearConstraint\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from math import sqrt\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DL_tOkk9PoIi"
      },
      "source": [
        "def timeit(method):\n",
        "    def timed(*args, **kw):\n",
        "        ts = time.time()\n",
        "        result = method(*args, **kw)\n",
        "        te = time.time()\n",
        "        if 'log_time' in kw:\n",
        "            name = kw.get('log_name', method.__name__.upper())\n",
        "            kw['log_time'][name] = int((te - ts) * 1000)\n",
        "        else:\n",
        "            print('%r  %2.2f ms' % \\\n",
        "                  (method.__name__, (te - ts) * 1000))\n",
        "        return result\n",
        "    return timed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ9Ap3tnRjSw"
      },
      "source": [
        "### Computing co-occurence matrices (PCO) with random surfing with return"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRQZJuukRjSx"
      },
      "source": [
        "def normalize(M):\n",
        "    #Put diagonal elements to 0\n",
        "    M  = M - np.diag(np.diag(M))\n",
        "    \n",
        "    #Normalizing by row\n",
        "    D_inv = np.diag(np.reciprocal(np.sum(M,axis=0)))\n",
        "    M = np.dot(D_inv,  M)\n",
        "\n",
        "    return M"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOTgjQ6nRjSx"
      },
      "source": [
        "def PCO(A, K, alpha):\n",
        "    \"\"\"\n",
        "    For a graph represented by its adjacency matrix *A*, computes the co-occurence matrix by random \n",
        "    surfing on the graph with returns. 1-alpha is the probability to make, at each step, a return \n",
        "    to the original step.\n",
        "    \"\"\"\n",
        "    A=np.array(A, dtype=float)\n",
        "    \n",
        "    #The adjacency matrix A is first normalized\n",
        "    A=normalize(A) \n",
        "    \n",
        "    n=A.shape[0]\n",
        "    \n",
        "    I=np.eye(n)\n",
        "    \n",
        "    P=I\n",
        "    M=np.zeros((n, n))\n",
        "    \n",
        "    for i in range(K):\n",
        "        P = alpha*np.dot(P,A) + (1-alpha)*I\n",
        "        M = M+P\n",
        "    \n",
        "    return(M)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kx3dbRO_RjSy",
        "outputId": "de630aaa-e379-420e-ce6c-0c1f61e2acd1"
      },
      "source": [
        "PCO([[0,1,1],[1,0,1],[1,1,0]], 4, 0.6)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.0482, 0.9759, 0.9759],\n",
              "       [0.9759, 2.0482, 0.9759],\n",
              "       [0.9759, 0.9759, 2.0482]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLKh_0AnRjS0"
      },
      "source": [
        "### From co-occurence matrices (PCO) to shifted positive pointwise mutual information (PPMI) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4tgEYiKRjS0"
      },
      "source": [
        "def PPMI(M):\n",
        "    \"\"\"Computes the shifted positive pointwise mutual information (PPMI) matrix\n",
        "    from the co-occurence matrix (PCO) of a graph.\"\"\"\n",
        "    \n",
        "    M=normalize(M)\n",
        "    cols = np.sum(M, axis=0)\n",
        "    rows = np.sum(M, axis=1).reshape((-1,1))\n",
        "    s = np.sum(rows)\n",
        "    \n",
        "    P = s*M\n",
        "    P /= cols\n",
        "    P /= rows\n",
        "    \n",
        "    #P[np.where(P<0)] = 1.0\n",
        "    P = np.log(P)\n",
        "\n",
        "    #To avoid NaN when applying log\n",
        "    P[np.isnan(P)] = 0.0\n",
        "    P[np.isinf(P)] = 0.0\n",
        "    P[np.isneginf(P)] = 0.0\n",
        "    P[np.where(P<0)] = 0.0\n",
        "    \n",
        "    return(P)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ds5P-tPBRjS1",
        "outputId": "2113c675-f1eb-4ace-f9c1-0865fcdb3ff1"
      },
      "source": [
        "PPMI(PCO([[0,1,1],[1,0,1],[1,1,0]], 4, 0.4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: RuntimeWarning: divide by zero encountered in log\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.40546511, 0.40546511],\n",
              "       [0.40546511, 0.        , 0.40546511],\n",
              "       [0.40546511, 0.40546511, 0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isQ5vZj6RjS1"
      },
      "source": [
        "### Embedding with stacked denoising autoencoders (SDAE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjXntppYRjS2"
      },
      "source": [
        "class GaussianNoise(nn.Module):\n",
        "    def __init__(self, stddev):\n",
        "        super().__init__()\n",
        "        self.stddev = stddev\n",
        "\n",
        "    def forward(self, din):\n",
        "        if self.training:\n",
        "            return din + torch.randn(din.size()) * self.stddev\n",
        "        return din"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koJYjk-2RjS2"
      },
      "source": [
        "class DropoutNoise(nn.Module):\n",
        "    def __init__(self, p):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    def forward(self, x):\n",
        "        t = torch.rand(x.size()).to(self.device)\n",
        "        a = t > self.p\n",
        "        \n",
        "        return(x*a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv0BqEnTRjS3"
      },
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, input_shape, n_neurons, activation='relu', noise=None, noise_arg=None):\n",
        "        super().__init__()\n",
        "        self.n_neurons = n_neurons\n",
        "        self.input_shape = input_shape\n",
        "        \n",
        "        self.has_noise = False\n",
        "        \n",
        "        if noise=='gaussian':\n",
        "            self.has_noise = True\n",
        "            self.noise = GaussianNoise(noise_arg)\n",
        "        elif noise=='dropout':\n",
        "            self.has_noise = True\n",
        "            self.noise = DropoutNoise(noise_arg)\n",
        "            \n",
        "        self.dense_layer = nn.Linear(self.input_shape, self.n_neurons)\n",
        "        \n",
        "        activations_map = {'relu':nn.ReLU, 'tanh':nn.Tanh, 'sigmoid':nn.Sigmoid}\n",
        "        self.activation = activations_map[activation]()\n",
        "\n",
        "    def forward(self, features):\n",
        "        x=features\n",
        "        \n",
        "        if self.has_noise:\n",
        "            x = self.noise(x)\n",
        "\n",
        "        x = self.dense_layer(features)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        return(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zXD66rIRjS3"
      },
      "source": [
        "class SDAE(nn.Module):\n",
        "    def __init__(self, input_shape, hidden_layers, activation='relu', last_activation='relu', noise_type='dropout', noise_arg=0.2):\n",
        "        super().__init__()\n",
        "        self.inputs = [input_shape] + hidden_layers\n",
        "        \n",
        "        n = len(self.inputs)\n",
        "        encoder_units = [BasicBlock(self.inputs[0], self.inputs[1], activation=activation, noise=noise_type, noise_arg=noise_arg)]\n",
        "        encoder_units.extend([BasicBlock(self.inputs[i], self.inputs[i+1], activation=activation) for i in range(1, n-1)])\n",
        "        \n",
        "        self.encoder = nn.Sequential(*encoder_units)\n",
        "        \n",
        "        decoder_units = [BasicBlock(self.inputs[i], self.inputs[i-1], activation=activation) for i in range(n-1,1,-1)]\n",
        "        decoder_units.append(BasicBlock(self.inputs[1], self.inputs[0], activation=last_activation))\n",
        "        \n",
        "        self.decoder = nn.Sequential(*decoder_units)\n",
        "        \n",
        "    def forward(self, features):\n",
        "        encoded = self.encoder(features)\n",
        "        \n",
        "        decoded = self.decoder(encoded)\n",
        "        \n",
        "        return(decoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OY1QMhHrRjS6"
      },
      "source": [
        "### PU-Learning via matrix completion and convex optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypdNhPO3jfXe"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def compute_auc(P,S):\n",
        "    y_pred = S.clone().detach()\n",
        "\n",
        "    y_pred[y_pred < 0.] = 0.\n",
        "    y_pred[y_pred > 1.] = 1.\n",
        "\t\n",
        "    y_pred = S.cpu().detach().numpy().reshape((-1,))\n",
        "    y_true = P.cpu().detach().numpy().reshape((-1,))\n",
        "\n",
        "    return(roc_auc_score(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIsHOEAkRjS6"
      },
      "source": [
        "def old_pu_learning(x, y, P, k = 7, alpha = 0.2, gamma = 0.3, maxiter=1000):\n",
        "    Fd = x.shape[1]\n",
        "    Ft = y.shape[1]\n",
        "    Nd = x.shape[0]\n",
        "    Nt = y.shape[0]\n",
        "    \n",
        "    #Number of variables\n",
        "    N_variables = Fd * k + Ft * k\n",
        "    \n",
        "    print(\"Number of variables:\", N_variables)    \n",
        "    print(\"Finding positive and negative examples...\")\n",
        "    \n",
        "    Ipos = np.where(P==1.)\n",
        "    Ineg = np.where(P==0.)\n",
        "\n",
        "    print(\"Number of positive examples:\", Ipos[0].shape[0])\n",
        "    print(\"Number of negative/unlabelled examples:\", Ineg[0].shape[0])\n",
        "    \n",
        "    alpha_rac = sqrt(alpha)\n",
        "    \n",
        "    @timeit\n",
        "    def objective(z):\n",
        "        H = z[:Fd*k].reshape((Fd,k))\n",
        "        W = z[-Ft*k:].reshape((Ft,k))\n",
        "        \n",
        "        M = P - (x @ H @ np.transpose(W) @ np.transpose(y))\n",
        "        \n",
        "        M[Ineg] *= alpha_rac\n",
        "        \n",
        "        L = torch.sum(M**2) + gamma/2 * (np.sum(H**2, axis=(0,1)) + np.sum(W**2, axis=(0,1)))\n",
        "        print(L)\n",
        "\n",
        "        return(L)\n",
        "    \n",
        "    def constraint(z):\n",
        "        H = z[:Fd*k].reshape((Fd,k))\n",
        "        W = z[-Ft*k:].reshape((Ft,k))\n",
        "        S = x @ H @ np.transpose(W) @ np.transpose(y)\n",
        "        S = S.reshape((-1,))\n",
        "        \n",
        "        return(S)\n",
        "    \n",
        "    nlc = NonlinearConstraint(constraint, np.zeros(Nt*Nd), np.ones(Nt*Nd))\n",
        "\n",
        "    print(\"Going to minimize... Maximum number of iterations:\", maxiter)\n",
        "    res=minimize(objective, x0 = np.random.randn(N_variables), options={'maxiter':maxiter, 'disp':'True'}, constraints=[nlc], method='trust-constr')\n",
        "    \n",
        "    print(\"\\n\\nSolved.\")\n",
        "    \n",
        "    z=res['x']\n",
        "    H = z[:Fd*k].reshape((Fd,k))\n",
        "    W = z[-Ft*k:].reshape((Ft,k))\n",
        "\n",
        "    print(\"Now computing Z=HW^T, then will compute S...\")\n",
        "    \n",
        "    S = x @ H @ np.transpose(W) @ np.transpose(y)\n",
        "    \n",
        "    return(S)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IT0t_9SQdGtR"
      },
      "source": [
        "def pu_learning(x, y, P, pos_train_mask, neg_train_mask, k = 7, alpha = 0.2, gamma = 0.3, maxiter=1000, lr=0.1, batch_size=100):\n",
        "    Fd = x.shape[1]\n",
        "    Ft = y.shape[1]\n",
        "    Nd = x.shape[0]\n",
        "    Nt = y.shape[0]\n",
        "    \n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    #Number of variables\n",
        "    N_variables = Fd * k + Ft * k\n",
        "    N_examples = Nd*Nt\n",
        "\n",
        "    print(\"Number of variables:\", N_variables)    \n",
        "    print(\"Finding positive and negative examples...\")\n",
        "    \n",
        "    P = torch.Tensor(P).to(device)\n",
        "    x = torch.Tensor(x).to(device)\n",
        "    y = torch.Tensor(y).to(device)\n",
        "    \n",
        "    x_norm = torch.linalg.norm(x)\n",
        "    y_norm = torch.linalg.norm(y)\n",
        "\n",
        "    Ipos = pos_train_mask\n",
        "    Ineg = neg_train_mask\n",
        "    train_mask = torch.logical_or(Ipos,Ineg)\n",
        "    \n",
        "    print(\"Number of train examples:\", P[train_mask].size()[0])\n",
        "    print(\"Number of positive examples in train set:\", P[Ipos].size()[0])\n",
        "    print(\"Number of negative/unlabelled examples in train set:\", P[Ineg].size()[0])\n",
        "    \n",
        "    alpha_rac = sqrt(alpha)\n",
        "    \n",
        "    def objective(H,W):\n",
        "        S = torch.chain_matmul(x, H, torch.transpose(W, 0, 1), torch.transpose(y, 0, 1))\n",
        "        M = P - S\n",
        "        \n",
        "        M[Ineg] *= alpha_rac\n",
        "        M = M[train_mask]\n",
        "        \n",
        "        L = torch.mean(M**2) + gamma/2 * (torch.sum(H**2) + torch.sum(W**2)) #+ 25000*(1-torch.std(S))\n",
        "\n",
        "        return(L)\n",
        "\n",
        "    def objective_batch(H, W, batch_mask):\n",
        "        S = torch.chain_matmul(x, H, torch.transpose(W, 0, 1), torch.transpose(y, 0, 1))\n",
        "        M = P - S\n",
        "        \n",
        "        M[Ineg] *= alpha_rac\n",
        "        M = M[batch_mask]\n",
        "        \n",
        "        L = torch.mean(M**2) + gamma/2 * (torch.sum(H**2) + torch.sum(W**2)) #+ 25000*(1-torch.std(S))\n",
        "\n",
        "        return(L)\n",
        "        \n",
        "    #nlc = NonlinearConstraint(constraint, np.zeros(Nt*Nd), np.ones(Nt*Nd))\n",
        "\n",
        "    print(\"Going to minimize... Maximum number of iterations:\", maxiter)\n",
        "    #res=minimize(objective, x0 = np.random.randn(N_variables), options={'maxiter':maxiter, 'disp':'True'}, constraints=[nlc], method='trust-constr')\n",
        "    W = ag.Variable(torch.rand(Ft,k).to(device)/y_norm, requires_grad=True)\n",
        "    H = ag.Variable(torch.rand(Fd,k).to(device)/x_norm, requires_grad=True)\n",
        "    \n",
        "    opt = torch.optim.Adam([H,W], lr=lr)\n",
        "    \n",
        "    batch_num = N_examples//batch_size\n",
        "\n",
        "    for i in range(maxiter):\n",
        "        for j in range(batch_num):\n",
        "            # Zeroing gradients\n",
        "            opt.zero_grad()\n",
        "\n",
        "            batch = torch.zeros((Nd,Nt), dtype=torch.bool)\n",
        "            rng_batch = torch.arange(j*batch_size, min((j+1)*batch_size, N_examples))\n",
        "\n",
        "            batch[rng_batch//Nt, rng_batch%Nt] = True\n",
        "            batch_mask = torch.logical_and(train_mask, batch)\n",
        "        \n",
        "            # Evaluating the objective\n",
        "            obj = objective_batch(H,W, batch_mask)\n",
        "\n",
        "            # Calculate gradients\n",
        "            obj.backward() \n",
        "            opt.step()\n",
        "\n",
        "        if i%1000==0:\n",
        "            S = torch.chain_matmul(x, H, torch.transpose(W,0,1), torch.transpose(y,0,1))\n",
        "            stdm = torch.std(P-S)\n",
        "            auc = compute_auc(P[train_mask],S[train_mask])\n",
        "            obj = objective_batch(H,W,train_mask)\n",
        "\n",
        "            print(\"Objective: \", obj, \"(standard deviation: \", stdm, \", auc: \", auc, \")\")\n",
        "\n",
        "    print(\"\\n\\nSolved.\")\n",
        "    \n",
        "    print(\"Now computing Z=HW^T, then will compute S...\")\n",
        "    \n",
        "    S = torch.chain_matmul(x, H, torch.transpose(W,0,1), torch.transpose(y,0,1))\n",
        "    \n",
        "    return(S, H, W)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACJPxqCZ9vDE"
      },
      "source": [
        "def get_train_test_masks(P, train_size=0.8):\n",
        "    P = torch.Tensor(P)\n",
        "    Ipos = (P == 1.)\n",
        "    Ineg = (P == 0.)\n",
        "\n",
        "    pos_train = Ipos * torch.rand(P.size())\n",
        "    pos_train[Ineg] = 1.\n",
        "    pos_train = pos_train < train_size\n",
        "    train_neg_rel_size = torch.sum(pos_train) / torch.sum(Ineg)\n",
        "    \n",
        "    neg_train = Ineg * torch.rand(P.size())\n",
        "    neg_train[Ipos] = 1.\n",
        "    neg_train = neg_train < train_neg_rel_size\n",
        "\n",
        "    train = pos_train + neg_train\n",
        "    test = ~train\n",
        "    pos_test = torch.logical_and(test, Ipos)\n",
        "    neg_test = torch.logical_and(test, Ineg)\n",
        "\n",
        "    return(pos_train, neg_train, pos_test, neg_test)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcOOXlXPkgqf"
      },
      "source": [
        "def new_pu_learning(x, y, P, k = 7, alpha = 0.2, gamma = 0.3, maxiter=1000, lr=1e-3, batch_size=100):\n",
        "    Fd = x.shape[1]\n",
        "    Ft = y.shape[1]\n",
        "    Nd = x.shape[0]\n",
        "    Nt = y.shape[0]\n",
        "    \n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    #Number of variables\n",
        "    N_variables = Fd * k + Ft * k\n",
        "    N_examples = Nd*Nt\n",
        "    \n",
        "    print(\"Spliting train and test sets...\")\n",
        "    pos_train, neg_train, pos_test, neg_test = get_train_test_masks(P, train_size=0.8)\n",
        "\n",
        "    P = torch.Tensor(P).to(device)\n",
        "    cartesian_product = torch.Tensor([[u, v] for u in x for v in y]).to(device)\n",
        "    \n",
        "    train_mask = torch.logical_or(pos_train, neg_train)\n",
        "\n",
        "    flat_train_mask = train_mask.flatten()\n",
        "\n",
        "    print(\"Building the train loader...\")\n",
        "    train = torch.utils.data.TensorDataset(cartesian_product[flat_train_mask], P[train_mask].flatten())\n",
        "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    print(\"Number of variables:\", N_variables)\n",
        "    print(\"Finding positive and negative examples...\")\n",
        "    \n",
        "    x = torch.Tensor(x).to(device)\n",
        "    y = torch.Tensor(y).to(device)\n",
        "    \n",
        "    print(\"Number of train examples:\", P.size()[0])\n",
        "    print(\"Number of positive examples in train set:\", P[pos_train].size()[0])\n",
        "    print(\"Number of negative/unlabelled examples in train set:\", P[neg_train].size()[0])\n",
        "    \n",
        "    alpha_rac = sqrt(alpha)\n",
        "    \n",
        "    def objective(a, H, W, b, P):\n",
        "        M = torch.zeros(P.size(), requires_grad=True).to(device)\n",
        "        for i in range(P.size()[0]):\n",
        "            s = torch.chain_matmul(a[i].reshape(1,Fd), H, torch.transpose(W, 0, 1), torch.transpose(b[i].reshape(1,Ft), 0, 1))\n",
        "            M[i] = P[i] - s.item()\n",
        "\n",
        "        Ineg = (P==0.)\n",
        "\n",
        "        M[Ineg] *= alpha_rac\n",
        "        \n",
        "        L = torch.mean(M**2) + gamma/2 * (torch.sum(H**2) + torch.sum(W**2))\n",
        "\n",
        "        return(L)\n",
        "    \n",
        "    def total_loss(x,H,W,y,P):\n",
        "        M = P - torch.chain_matmul(x, H, torch.transpose(W, 0, 1), torch.transpose(y, 0, 1))\n",
        "\n",
        "        M[~train_mask] = 0.\n",
        "\n",
        "        M[neg_train] *= alpha_rac\n",
        "\n",
        "        L = torch.mean(M**2) + gamma/2 * (torch.sum(H**2) + torch.sum(W**2))\n",
        "\n",
        "        return(L)\n",
        "\n",
        "    print(\"Going to minimize... Maximum number of epochs:\", maxiter)\n",
        "    \n",
        "    W = torch.rand(Ft,k, requires_grad=True).to(device)\n",
        "    W /= torch.linalg.norm(W) #for initial W and H not to be too big\n",
        "    H = torch.rand(Fd,k, requires_grad=True).to(device)\n",
        "    H /= torch.linalg.norm(H)\n",
        "\n",
        "    W = ag.Variable(W, requires_grad=True)\n",
        "    H = ag.Variable(H, requires_grad=True)\n",
        "    \n",
        "    print(W, H)\n",
        "    opt = torch.optim.Adam([H,W], lr=lr)\n",
        "    \n",
        "    batch_num = N_examples//batch_size\n",
        "\n",
        "    for k in range(maxiter):\n",
        "        for i, batch in enumerate(train_loader, 0):\n",
        "            # Zeroing gradients\n",
        "            opt.zero_grad()\n",
        "\n",
        "            inputs, labels = batch\n",
        "\n",
        "            a, b = inputs[:,0,:], inputs[:,1,:]\n",
        "        \n",
        "            # Evaluating the objective\n",
        "            obj = objective(a,H,W,b,labels)\n",
        "\n",
        "            # Calculate gradients\n",
        "            obj.backward() \n",
        "            opt.step()\n",
        "            \n",
        "            if i%(batch_size//4) == 0:\n",
        "                S = torch.chain_matmul(x, H, torch.transpose(W,0,1), torch.transpose(y,0,1))\n",
        "                stdm = torch.std(S[train_mask])\n",
        "                auc = compute_auc(P[train_mask],S[train_mask])\n",
        "                loss = total_loss(x,H,W,y,P)\n",
        "                print(H.grad, W.grad)\n",
        "                print(\"[epoch=\",k,\", batch_num=\",i,\"]\",\"Objective: \", loss.item(), \"(standard deviation: \", stdm.item(), \", auc: \", auc.item(), \")\")\n",
        "\n",
        "    print(\"\\n\\nSolved.\")\n",
        "    \n",
        "    print(W,H)\n",
        "    print(\"Now computing Z=HW^T, then will compute S...\")\n",
        "    \n",
        "    S = torch.chain_matmul(x, H, torch.transpose(W,0,1), torch.transpose(y,0,1))\n",
        "    \n",
        "    return(S, H, W)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPeV3DG_qvBF"
      },
      "source": [
        "\n",
        "class PU_Learner(nn.Module):\n",
        "    def __init__(self, k, Fd, Ft):\n",
        "        super().__init__()\n",
        "        self.k = k\n",
        "        self.Fd = Fd\n",
        "        self.Ft = Ft\n",
        "\n",
        "        self.H = torch.nn.Parameter(torch.randn(Fd, k))\n",
        "        self.W = torch.nn.Parameter(torch.randn(k, Ft))\n",
        "\n",
        "    def forward(self, x, y):\n",
        "    \treturn(torch.einsum('ij,jk,kl,li->i', x, self.H, self.W, torch.transpose(y, 1, 0)))\n",
        "\n",
        "def pu_learning_new(k, x, y, P, n_epochs=100, batch_size=100, lr=1e-3, print_step=4):\n",
        "    #hidden_layers=[500,200,100]\n",
        "    #input_numer=784\n",
        "    #  use gpu if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    Fd = x.shape[1]\n",
        "    Ft = y.shape[1]\n",
        "    Nd = x.shape[0]\n",
        "    Nt = y.shape[0]\n",
        "    \n",
        "    #Number of variables\n",
        "    N_variables = Fd * k + Ft * k\n",
        "    N_examples = Nd*Nt\n",
        "\n",
        "    cartesian_product = torch.Tensor([[u, v] for u in x for v in y]).to(device)\n",
        "\n",
        "    x = torch.Tensor(x).to(device)\n",
        "    y = torch.Tensor(y).to(device)\n",
        "\n",
        "    print(\"Spliting train and test sets...\")\n",
        "    pos_train, neg_train, pos_test, neg_test = get_train_test_masks(P, train_size=0.8)\n",
        "\n",
        "    P = torch.Tensor(P).to(device)\n",
        "\n",
        "    train_mask = torch.logical_or(pos_train, neg_train)\n",
        "\n",
        "    flat_train_mask = train_mask.flatten()\n",
        "\n",
        "    print(\"Building the train loader...\")\n",
        "    train = torch.utils.data.TensorDataset(cartesian_product[flat_train_mask], P[train_mask].flatten())\n",
        "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "    \n",
        "    batch_num = len(train_loader)\n",
        "\n",
        "    print(\"Number of variables:\", N_variables)\n",
        "    print(\"Finding positive and negative examples...\")    \n",
        "\n",
        "    print(\"Number of train examples:\", P.size()[0])\n",
        "    print(\"Number of positive examples in train set:\", P[pos_train].size()[0])\n",
        "    print(\"Number of negative/unlabelled examples in train set:\", P[neg_train].size()[0])\n",
        "\n",
        "    model = PU_Learner(k, Fd, Ft).to(device)\n",
        "\n",
        "    # create an optimizer object\n",
        "    # Adam optimizer with learning rate 1e-3\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # mean-squared error loss\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "            \n",
        "            inputs=inputs.to(device)\n",
        "            \n",
        "            x_batch, y_batch = inputs[:,0,:], inputs[:,1,:]\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(x_batch, y_batch)\n",
        "            \n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % batch_num == batch_num-1:\n",
        "                print('[%d, %5d] loss: %.3f' %\n",
        "                      (epoch + 1, i + 1, running_loss / (i+1)))\n",
        "                running_loss = 0.0\n",
        "\n",
        "    print('Finished Training')\n",
        "    print(\"Now computing Z=HW^T, then will compute S...\")\n",
        "    \n",
        "    print(x.size(), model.H.size(), model.W.size(), y.size())\n",
        "    S = torch.chain_matmul(x, model.H, model.W, torch.transpose(y,0,1))\n",
        "    \n",
        "    return(S, model.H, model.W)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgqj9kraRjS8"
      },
      "source": [
        "### Generate random networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4HpNjF5RjS8"
      },
      "source": [
        "def random_graph(p, size=(100,100)):\n",
        "    return(np.array([[int(random.random() < p) for i in range(size[1])] for j in range(size[0])]))\n",
        "\n",
        "def random_undirected_graph(p, size=(100,100)):\n",
        "    graph = random_graph(p, size=size)\n",
        "    graph[np.arange(size[0]),np.arange(size[1])]=0 #nullify the diagonal\n",
        "    graph = np.maximum(graph, graph.T) #make it symmetric\n",
        "    \n",
        "    return(graph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuPKe8J7RjS8"
      },
      "source": [
        "def random_graph_with_fixed_components(p, nodes_per_component=[50,50]):\n",
        "    nodes_per_component = np.array(nodes_per_component)\n",
        "    n_nodes = nodes_per_component.sum()\n",
        "    n_cmp = nodes_per_component.shape[0]\n",
        "    \n",
        "    graph = np.zeros((n_nodes, n_nodes))\n",
        "    nodes = np.arange(n_nodes)\n",
        "    np.random.shuffle(nodes)\n",
        "    \n",
        "    cmp_nodes = []\n",
        "    acc=0\n",
        "    \n",
        "    for i in range(n_cmp):\n",
        "        cmp = nodes[acc:(acc+nodes_per_component[i])]\n",
        "        cmp_nodes.append(cmp)\n",
        "        acc += nodes_per_component[i]\n",
        "        \n",
        "        size = cmp.shape[0]\n",
        "        submatrix=np.ix_(cmp,cmp)\n",
        "\n",
        "        graph[submatrix] = random_undirected_graph(p, (size,size))\n",
        "        \n",
        "    return(graph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdrtB3xYRjS9"
      },
      "source": [
        "def neighbors(adj, i):\n",
        "    return (np.where(adj[i,:]==1)[0])\n",
        "\n",
        "def dfs(adj, i):\n",
        "    n = adj.shape[0] #number of nodes in the graph\n",
        "    visited = [False for k in range(n)]\n",
        "    \n",
        "    stack = [i]\n",
        "    \n",
        "    while(len(stack)>0):\n",
        "        k = stack.pop()\n",
        "        neighborhood = neighbors(adj, k)\n",
        "        visited[k] = True\n",
        "        \n",
        "        for n in neighborhood:\n",
        "            if not visited[n]:\n",
        "                stack.append(n)\n",
        "    \n",
        "    return(np.where(visited))\n",
        "\n",
        "def connected_components(adj):\n",
        "    n = adj.shape[0]\n",
        "    \n",
        "    visited = np.array([0 for k in range(n)])\n",
        "    s = np.sum(visited)\n",
        "    \n",
        "    comp=[]\n",
        "    \n",
        "    while s<n:\n",
        "        i = np.where(1-visited)[0][0]\n",
        "        \n",
        "        cmp = dfs(adj, i)\n",
        "        visited[cmp] = 1\n",
        "        s = np.sum(visited)\n",
        "        \n",
        "        comp.append(list(cmp[0]))\n",
        "    \n",
        "    return(np.array(comp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTLxK-OmRjS9",
        "outputId": "23d4772b-a9e9-4034-e766-8b96eecf210e"
      },
      "source": [
        "N_rand_drugs = 784\n",
        "N_rand_targets = 1000\n",
        "\n",
        "density=0.2\n",
        "density_dp = 0.05\n",
        "#generate random matrices in M({0,1})\n",
        "#1. a drug-drug network (drug similarities)\n",
        "#2. a protein-protein network (protein similarities)\n",
        "#3. a drug-protein network (drug-target known relationships)\n",
        "dd_net = random_graph_with_fixed_components(density, [196,196,196,196])\n",
        "pp_net = random_graph_with_fixed_components(density, [100 for i in range(10)])\n",
        "dp_net = random_graph(density_dp,size=(N_rand_drugs,N_rand_targets))\n",
        "\n",
        "np.sum(dp_net)/(N_rand_drugs*N_rand_targets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.05011862244897959"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bW3Utn4lRjS9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0b3a482-fe61-48a3-93b3-7827eb5fed48"
      },
      "source": [
        "np.sum(np.sum(dd_net))/(dd_net[0].shape[0])**2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.08907746772178259"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkahyGsdRjS9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfabad6f-884a-46d2-dd03-f95a6b5f1a0b"
      },
      "source": [
        "dd_net[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "       0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
              "       0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "       0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
              "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
              "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkczPipwRjS-"
      },
      "source": [
        "### Encoding the drug-drug network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZATeBeSRjS-"
      },
      "source": [
        "def sdae(input_net, input_number, hidden_layers, n_epochs=100, batch_size=1, activation='sigmoid', last_activation='sigmoid'):\n",
        "    #hidden_layers=[500,200,100]\n",
        "    #input_numer=784\n",
        "    #  use gpu if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = SDAE(input_number, hidden_layers, activation=activation, last_activation=last_activation).to(device)\n",
        "\n",
        "    # create an optimizer object\n",
        "    # Adam optimizer with learning rate 1e-3\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    # mean-squared error loss\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    summary(model, (input_number,))\n",
        "    \n",
        "    tensor_net = torch.Tensor(input_net).to(device)\n",
        "    train = torch.utils.data.TensorDataset(tensor_net, tensor_net)\n",
        "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "            \n",
        "            inputs=inputs.to(device)\n",
        "            \n",
        "            inputs=torch.flatten(inputs)\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "        \n",
        "            loss = criterion(outputs, inputs)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % input_number == input_number-1: \n",
        "                print('[%d, %5d] loss: %.3f' %\n",
        "                      (epoch + 1, i + 1, running_loss / input_number))\n",
        "                running_loss = 0.0\n",
        "\n",
        "    print('Finished Training')\n",
        "    \n",
        "    return(model, train_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opliyCSJRjS_"
      },
      "source": [
        "### Embedding evaluation with t-SNE visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhpOJc-4RjS_"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def visualize_TSNE(embeddings,target):\n",
        "    tsne = TSNE(n_components=2, init='pca',\n",
        "                         random_state=0, perplexity=30)\n",
        "    data = tsne.fit_transform(embeddings)\n",
        "    #plt.figure(figsize=(12, 6))\n",
        "    plt.title(\"TSNE visualization of the embeddings\")\n",
        "    plt.scatter(data[:,0],data[:,1],c=target)\n",
        "\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bSdG2NORjS_"
      },
      "source": [
        "def get_embeddings(train_loader, N, model, size_encoded=100):\n",
        "    trainiter = iter(train_loader)\n",
        "    embeddings = np.zeros((N, size_encoded))\n",
        "\n",
        "    for i,q in enumerate(trainiter):\n",
        "        embedded = model.encoder(q[0]).cpu().detach().numpy()\n",
        "        embeddings[i,:] = embedded.reshape((size_encoded,))\n",
        "    \n",
        "    return(embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQz3Ap-ORjTA"
      },
      "source": [
        "@timeit\n",
        "def dngr_pipeline(network, N, hidden_layers, K=10, alpha=0.2, n_epochs=100, batch_size=1, activation='sigmoid', last_activation='sigmoid'):\n",
        "    ppmi_net = PPMI(PCO(network, K, alpha))\n",
        "    model, train_loader = sdae(ppmi_net, N, hidden_layers, n_epochs=n_epochs, batch_size=batch_size, activation=activation, last_activation=last_activation)\n",
        "    \n",
        "    print(\"[*] Visualizing an example's output...\")\n",
        "    trainiter = iter(train_loader)\n",
        "    inputs, _ = trainiter.next()\n",
        "\n",
        "    print(inputs)\n",
        "    print(model(inputs))\n",
        "\n",
        "    print(mean_squared_error(inputs.cpu().detach().numpy(), model(inputs).cpu().detach().numpy()))\n",
        "    \n",
        "    print(\"[*] Getting the embeddings and visualizing t-SNE...\")\n",
        "    embeddings=get_embeddings(train_loader, N, model, size_encoded=hidden_layers[-1])\n",
        "    \n",
        "    cmps = connected_components(network)\n",
        "    targets = [0 for i in range(N)]\n",
        "\n",
        "    for i, cmp in enumerate(cmps):\n",
        "        for n in cmp:\n",
        "            targets[n] = i\n",
        "    \n",
        "    visualize_TSNE(embeddings, targets)\n",
        "    \n",
        "    return(embeddings, model, train_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UBFoLXR8RjTA",
        "outputId": "02ac53eb-04e5-4b63-c9f6-4dd1b4d24b01"
      },
      "source": [
        "embeddings_drugs, _, _ = dngr_pipeline(dd_net, N_rand_drugs, [500, 200, 100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: RuntimeWarning: divide by zero encountered in log\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "      DropoutNoise-1                  [-1, 784]               0\n",
            "            Linear-2                  [-1, 500]         392,500\n",
            "           Sigmoid-3                  [-1, 500]               0\n",
            "        BasicBlock-4                  [-1, 500]               0\n",
            "            Linear-5                  [-1, 200]         100,200\n",
            "           Sigmoid-6                  [-1, 200]               0\n",
            "        BasicBlock-7                  [-1, 200]               0\n",
            "            Linear-8                  [-1, 100]          20,100\n",
            "           Sigmoid-9                  [-1, 100]               0\n",
            "       BasicBlock-10                  [-1, 100]               0\n",
            "           Linear-11                  [-1, 200]          20,200\n",
            "          Sigmoid-12                  [-1, 200]               0\n",
            "       BasicBlock-13                  [-1, 200]               0\n",
            "           Linear-14                  [-1, 500]         100,500\n",
            "          Sigmoid-15                  [-1, 500]               0\n",
            "       BasicBlock-16                  [-1, 500]               0\n",
            "           Linear-17                  [-1, 784]         392,784\n",
            "          Sigmoid-18                  [-1, 784]               0\n",
            "       BasicBlock-19                  [-1, 784]               0\n",
            "================================================================\n",
            "Total params: 1,026,284\n",
            "Trainable params: 1,026,284\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.06\n",
            "Params size (MB): 3.91\n",
            "Estimated Total Size (MB): 3.98\n",
            "----------------------------------------------------------------\n",
            "[1,   784] loss: 0.410\n",
            "[2,   784] loss: 0.402\n",
            "[3,   784] loss: 0.388\n",
            "[4,   784] loss: 0.387\n",
            "[5,   784] loss: 0.391\n",
            "[6,   784] loss: 0.385\n",
            "[7,   784] loss: 0.350\n",
            "[8,   784] loss: 0.345\n",
            "[9,   784] loss: 0.313\n",
            "[10,   784] loss: 0.304\n",
            "[11,   784] loss: 0.303\n",
            "[12,   784] loss: 0.303\n",
            "[13,   784] loss: 0.303\n",
            "[14,   784] loss: 0.303\n",
            "[15,   784] loss: 0.303\n",
            "[16,   784] loss: 0.303\n",
            "[17,   784] loss: 0.303\n",
            "[18,   784] loss: 0.303\n",
            "[19,   784] loss: 0.303\n",
            "[20,   784] loss: 0.303\n",
            "[21,   784] loss: 0.303\n",
            "[22,   784] loss: 0.303\n",
            "[23,   784] loss: 0.303\n",
            "[24,   784] loss: 0.303\n",
            "[25,   784] loss: 0.303\n",
            "[26,   784] loss: 0.303\n",
            "[27,   784] loss: 0.303\n",
            "[28,   784] loss: 0.303\n",
            "[29,   784] loss: 0.303\n",
            "[30,   784] loss: 0.302\n",
            "[31,   784] loss: 0.303\n",
            "[32,   784] loss: 0.303\n",
            "[33,   784] loss: 0.303\n",
            "[34,   784] loss: 0.303\n",
            "[35,   784] loss: 0.303\n",
            "[36,   784] loss: 0.303\n",
            "[37,   784] loss: 0.303\n",
            "[38,   784] loss: 0.302\n",
            "[39,   784] loss: 0.302\n",
            "[40,   784] loss: 0.303\n",
            "[41,   784] loss: 0.303\n",
            "[42,   784] loss: 0.303\n",
            "[43,   784] loss: 0.303\n",
            "[44,   784] loss: 0.302\n",
            "[45,   784] loss: 0.302\n",
            "[46,   784] loss: 0.303\n",
            "[47,   784] loss: 0.302\n",
            "[48,   784] loss: 0.302\n",
            "[49,   784] loss: 0.302\n",
            "[50,   784] loss: 0.302\n",
            "[51,   784] loss: 0.302\n",
            "[52,   784] loss: 0.302\n",
            "[53,   784] loss: 0.301\n",
            "[54,   784] loss: 0.301\n",
            "[55,   784] loss: 0.301\n",
            "[56,   784] loss: 0.301\n",
            "[57,   784] loss: 0.300\n",
            "[58,   784] loss: 0.300\n",
            "[59,   784] loss: 0.300\n",
            "[60,   784] loss: 0.300\n",
            "[61,   784] loss: 0.300\n",
            "[62,   784] loss: 0.299\n",
            "[63,   784] loss: 0.299\n",
            "[64,   784] loss: 0.299\n",
            "[65,   784] loss: 0.299\n",
            "[66,   784] loss: 0.298\n",
            "[67,   784] loss: 0.298\n",
            "[68,   784] loss: 0.298\n",
            "[69,   784] loss: 0.298\n",
            "[70,   784] loss: 0.298\n",
            "[71,   784] loss: 0.300\n",
            "[72,   784] loss: 0.299\n",
            "[73,   784] loss: 0.298\n",
            "[74,   784] loss: 0.298\n",
            "[75,   784] loss: 0.297\n",
            "[76,   784] loss: 0.298\n",
            "[77,   784] loss: 0.297\n",
            "[78,   784] loss: 0.297\n",
            "[79,   784] loss: 0.297\n",
            "[80,   784] loss: 0.297\n",
            "[81,   784] loss: 0.297\n",
            "[82,   784] loss: 0.297\n",
            "[83,   784] loss: 0.296\n",
            "[84,   784] loss: 0.296\n",
            "[85,   784] loss: 0.296\n",
            "[86,   784] loss: 0.296\n",
            "[87,   784] loss: 0.298\n",
            "[88,   784] loss: 0.295\n",
            "[89,   784] loss: 0.296\n",
            "[90,   784] loss: 0.295\n",
            "[91,   784] loss: 0.295\n",
            "[92,   784] loss: 0.295\n",
            "[93,   784] loss: 0.295\n",
            "[94,   784] loss: 0.294\n",
            "[95,   784] loss: 0.294\n",
            "[96,   784] loss: 0.295\n",
            "[97,   784] loss: 0.294\n",
            "[98,   784] loss: 0.294\n",
            "[99,   784] loss: 0.294\n",
            "[100,   784] loss: 0.294\n",
            "Finished Training\n",
            "[*] Visualizing an example's output...\n",
            "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 2.2258, 0.0000, 0.0000, 0.0000, 2.1179,\n",
            "         2.1241, 0.0000, 0.0000, 0.0000, 2.2239, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 2.1804, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.3730, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         2.0797, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 2.1980, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 2.2681, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         2.2142, 2.2075, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         1.9649, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.9980, 0.0000, 0.0000,\n",
            "         2.0191, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.1652, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 2.1309, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         2.1338, 0.0000, 0.0000, 0.0000, 2.2326, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         2.2254, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 2.0705, 0.0000, 0.0000, 2.0835, 0.0000, 0.0000, 0.0000,\n",
            "         2.1372, 2.2399, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.2323,\n",
            "         0.0000, 0.0000, 2.1535, 2.3753, 0.0000, 0.0000, 0.0000, 0.0000, 2.0578,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.1049, 2.1898, 0.0000,\n",
            "         0.0000, 0.0000, 2.0287, 2.0732, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 2.1277, 0.0000, 0.0000, 2.0652, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         2.3132, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.0644,\n",
            "         0.0000, 2.0950, 0.0000, 0.0000, 0.0000, 0.0000, 2.0794, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 2.2427, 0.0000, 0.0000, 0.0000, 0.0000, 2.2136, 0.0000,\n",
            "         2.2459, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.0389, 2.1521, 0.0000,\n",
            "         2.3810, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.0360,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.3110, 0.0000,\n",
            "         2.3330, 0.0000, 0.0000, 2.1958, 0.0000, 2.1454, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.1268, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 2.1015, 2.1822, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.0230,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 2.2136, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 2.2795, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 2.2506, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 2.0761, 2.2138, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 2.4584, 0.0000, 2.0614, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         2.2350, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.1235, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 1.9752, 2.0817, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 2.2174, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 2.1644, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         2.1944, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 2.1447, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 2.0667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 2.1122, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 2.0565, 0.0000, 2.2352, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.1019,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.1707,\n",
            "         0.0000, 0.0000, 2.1227, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.2278, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.2283, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.1589,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.0414, 0.0000, 2.0997, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.9965, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 2.3022, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000]], device='cuda:0')\n",
            "tensor([[8.4005e-01, 2.2488e-05, 8.4820e-01, 2.0176e-09, 9.5696e-01, 7.6845e-09,\n",
            "         3.7179e-08, 6.6073e-05, 9.9996e-01, 9.8298e-01, 8.8639e-04, 3.3229e-07,\n",
            "         1.7515e-06, 8.8286e-01, 5.3107e-05, 1.8868e-08, 3.2005e-10, 1.2188e-07,\n",
            "         4.8379e-09, 2.3262e-08, 4.5006e-08, 3.8879e-09, 5.0496e-05, 7.2551e-11,\n",
            "         1.4719e-05, 3.0803e-09, 2.0832e-08, 9.2846e-01, 9.9750e-01, 1.1131e-05,\n",
            "         7.2745e-08, 4.7192e-09, 3.3301e-08, 8.2872e-05, 9.7964e-01, 4.9589e-09,\n",
            "         5.9137e-10, 8.8320e-09, 9.3192e-09, 9.7640e-06, 6.2088e-01, 6.4399e-08,\n",
            "         4.6274e-01, 1.0461e-04, 2.3030e-05, 5.3188e-09, 7.3697e-01, 5.0384e-04,\n",
            "         6.3511e-08, 5.3720e-08, 2.0860e-05, 9.2156e-01, 9.6901e-01, 6.5826e-09,\n",
            "         6.5210e-08, 9.9957e-01, 1.7838e-08, 1.4863e-09, 1.3783e-11, 9.9738e-09,\n",
            "         1.1624e-08, 9.9998e-01, 5.9944e-01, 7.8405e-01, 1.1347e-09, 2.1615e-06,\n",
            "         9.9761e-09, 1.6632e-06, 4.9020e-01, 1.6274e-10, 1.1645e-04, 5.8502e-09,\n",
            "         5.0720e-08, 6.2021e-01, 1.7362e-06, 1.4034e-07, 1.4655e-09, 4.6755e-08,\n",
            "         6.6741e-09, 3.2061e-07, 1.4369e-09, 1.7146e-01, 8.9784e-09, 1.3623e-08,\n",
            "         5.4161e-08, 9.9174e-09, 1.2801e-08, 4.8461e-08, 7.6762e-05, 3.6619e-07,\n",
            "         4.1835e-08, 8.7313e-08, 4.8801e-04, 3.3310e-08, 7.9290e-01, 2.6501e-09,\n",
            "         3.8574e-04, 8.0724e-08, 1.0822e-07, 7.6290e-01, 5.5736e-01, 1.6053e-08,\n",
            "         2.0482e-05, 2.2507e-08, 1.1101e-06, 3.2247e-06, 1.4566e-08, 5.9874e-09,\n",
            "         1.5793e-08, 1.1895e-10, 8.1685e-01, 2.5180e-04, 7.4068e-01, 3.6418e-08,\n",
            "         7.9030e-06, 2.3458e-11, 7.8505e-01, 8.2242e-10, 1.0536e-05, 3.0578e-01,\n",
            "         6.3405e-09, 9.1725e-01, 5.8251e-01, 2.0869e-09, 8.2730e-08, 1.8504e-08,\n",
            "         1.1002e-04, 8.2272e-01, 4.7877e-09, 1.1843e-07, 6.5893e-08, 3.9406e-07,\n",
            "         1.0620e-09, 1.7920e-08, 2.8743e-07, 3.6040e-09, 5.2215e-08, 2.6541e-09,\n",
            "         4.9102e-10, 1.0579e-07, 1.7123e-07, 4.3113e-04, 9.3491e-05, 3.6284e-08,\n",
            "         4.0654e-09, 1.6692e-08, 3.3944e-09, 7.7881e-01, 4.6010e-09, 1.3221e-03,\n",
            "         4.1762e-08, 1.8967e-09, 8.7872e-09, 1.1802e-07, 9.3850e-01, 4.1406e-07,\n",
            "         7.7865e-01, 1.7573e-08, 5.5858e-07, 1.0198e-08, 1.5063e-08, 4.5773e-04,\n",
            "         1.1083e-07, 2.1749e-04, 3.5357e-08, 1.1818e-08, 3.6425e-05, 1.3461e-08,\n",
            "         4.0335e-07, 5.6593e-08, 6.7291e-08, 9.9973e-01, 7.2434e-01, 6.7276e-10,\n",
            "         5.2345e-09, 9.0019e-01, 1.0205e-05, 8.4939e-01, 2.8176e-08, 1.5034e-04,\n",
            "         2.6417e-09, 2.3200e-09, 9.6855e-01, 4.6080e-09, 1.6443e-08, 2.2715e-06,\n",
            "         3.7376e-08, 6.7855e-08, 2.1075e-11, 9.9162e-01, 2.5796e-05, 9.2962e-01,\n",
            "         3.2391e-09, 1.0450e-07, 5.5553e-09, 8.6892e-01, 2.5256e-07, 9.3497e-01,\n",
            "         7.5561e-01, 6.6506e-09, 1.3367e-08, 1.2924e-05, 2.4942e-06, 6.3635e-01,\n",
            "         3.0272e-09, 9.8900e-01, 6.5198e-01, 1.2948e-09, 1.8940e-05, 3.2107e-08,\n",
            "         3.8799e-08, 2.4374e-08, 1.3480e-03, 8.7173e-01, 5.7875e-09, 1.1576e-09,\n",
            "         8.7825e-09, 7.3312e-10, 3.1944e-07, 9.4635e-09, 6.4466e-01, 1.2961e-04,\n",
            "         3.6719e-04, 6.6562e-09, 1.4022e-08, 6.2622e-01, 3.0103e-09, 1.1737e-09,\n",
            "         9.0690e-09, 9.7414e-01, 8.7336e-09, 7.4061e-01, 2.5226e-09, 6.6698e-09,\n",
            "         9.9589e-01, 2.8455e-08, 2.5476e-06, 7.0520e-01, 4.8023e-08, 8.5157e-05,\n",
            "         1.1825e-04, 1.2250e-05, 8.0449e-08, 5.8797e-10, 3.6456e-09, 6.8658e-01,\n",
            "         2.0867e-09, 2.0576e-07, 9.9868e-01, 6.9444e-05, 1.0958e-07, 9.9916e-01,\n",
            "         7.7582e-01, 9.5991e-01, 3.5051e-06, 1.9442e-09, 6.7865e-10, 2.0389e-09,\n",
            "         1.5745e-08, 3.6851e-08, 9.8271e-01, 2.0322e-08, 8.4953e-09, 9.5570e-01,\n",
            "         9.8020e-01, 2.9237e-04, 1.4550e-08, 8.7568e-10, 2.9780e-06, 4.8892e-01,\n",
            "         4.1737e-12, 9.0806e-01, 4.8127e-01, 5.1590e-09, 2.6835e-09, 2.2636e-03,\n",
            "         9.2549e-01, 7.8679e-01, 2.6770e-08, 8.5301e-01, 6.0790e-09, 7.3898e-01,\n",
            "         9.6831e-01, 1.7080e-04, 2.6700e-09, 1.8049e-05, 2.3762e-09, 3.2981e-01,\n",
            "         5.6839e-04, 9.9952e-01, 6.6429e-11, 8.3753e-09, 9.5351e-01, 4.0823e-08,\n",
            "         8.3680e-09, 6.8137e-08, 1.6705e-04, 5.9315e-01, 9.6595e-09, 8.4694e-01,\n",
            "         1.5538e-08, 1.9751e-09, 1.0316e-07, 4.3013e-07, 3.8037e-09, 8.8339e-01,\n",
            "         1.8925e-08, 9.6478e-01, 6.9012e-04, 1.2594e-09, 4.9784e-05, 3.7746e-05,\n",
            "         7.3547e-01, 9.5708e-01, 1.0516e-08, 1.4152e-08, 7.5505e-05, 7.4066e-01,\n",
            "         8.1953e-10, 1.4719e-05, 8.0144e-01, 6.7769e-10, 9.9889e-01, 9.9989e-01,\n",
            "         6.7596e-01, 1.4540e-07, 9.8297e-01, 8.8350e-01, 1.6965e-07, 6.2810e-09,\n",
            "         8.5038e-01, 3.3300e-06, 9.6095e-01, 1.2684e-06, 9.6628e-01, 2.5119e-08,\n",
            "         4.7490e-09, 3.3030e-09, 6.3167e-08, 3.4897e-07, 4.0549e-08, 6.9147e-10,\n",
            "         1.5699e-08, 3.0346e-10, 3.5995e-08, 3.1310e-04, 9.7222e-01, 3.1841e-05,\n",
            "         9.9874e-01, 9.9045e-01, 2.2327e-10, 9.5466e-01, 4.7264e-08, 3.9698e-11,\n",
            "         9.9484e-01, 9.6585e-01, 1.1992e-08, 4.3752e-08, 2.4105e-05, 3.7446e-01,\n",
            "         3.2579e-09, 1.6209e-07, 2.4459e-04, 8.2257e-04, 7.9660e-07, 2.9553e-09,\n",
            "         6.1391e-09, 7.6834e-01, 6.3123e-01, 2.9462e-07, 5.8039e-04, 1.8025e-08,\n",
            "         9.9859e-01, 7.7361e-01, 1.4271e-09, 8.4528e-08, 9.8443e-01, 2.0938e-08,\n",
            "         5.8021e-01, 1.2460e-04, 9.1162e-09, 9.2970e-01, 9.9508e-01, 7.9362e-01,\n",
            "         1.0771e-08, 4.1786e-07, 2.3436e-07, 9.9924e-01, 1.4970e-09, 2.2802e-08,\n",
            "         2.3932e-04, 2.1638e-06, 2.5614e-04, 7.6989e-01, 9.4859e-05, 9.6572e-01,\n",
            "         4.5834e-10, 6.8632e-08, 2.2447e-08, 9.3738e-06, 8.7316e-01, 3.0079e-08,\n",
            "         3.2805e-04, 2.4718e-05, 1.7657e-05, 7.4658e-01, 8.6430e-01, 9.9100e-01,\n",
            "         4.2388e-07, 2.0211e-03, 6.2851e-01, 9.4580e-04, 6.8814e-08, 9.7066e-01,\n",
            "         5.4517e-10, 1.0874e-08, 5.7466e-05, 3.5407e-08, 2.7155e-09, 2.2228e-08,\n",
            "         8.7773e-09, 5.4907e-09, 1.0067e-09, 1.8430e-04, 1.8909e-07, 9.5659e-01,\n",
            "         7.4701e-01, 5.5063e-04, 1.8653e-08, 3.4401e-04, 9.6960e-01, 6.0618e-10,\n",
            "         6.9942e-01, 1.5342e-08, 3.5317e-05, 3.9173e-05, 9.6258e-01, 8.5033e-04,\n",
            "         8.5043e-01, 2.3278e-04, 5.7456e-09, 1.7024e-08, 3.0387e-08, 3.9839e-08,\n",
            "         2.7222e-08, 4.1092e-09, 5.4150e-01, 4.8832e-08, 4.8916e-08, 2.1565e-08,\n",
            "         9.9292e-01, 2.4905e-06, 9.2017e-01, 4.4955e-08, 6.5554e-01, 2.4960e-07,\n",
            "         7.0091e-11, 3.2699e-09, 1.8905e-09, 9.1303e-09, 5.1701e-09, 9.9141e-01,\n",
            "         5.9390e-09, 6.4973e-04, 2.8321e-08, 1.5951e-08, 4.6311e-09, 1.7108e-07,\n",
            "         9.0653e-01, 7.8532e-10, 1.6547e-08, 5.9871e-01, 9.9276e-01, 9.6213e-01,\n",
            "         1.5883e-08, 9.0794e-01, 7.9278e-08, 1.8760e-08, 9.5508e-08, 7.6185e-01,\n",
            "         1.1465e-04, 8.8541e-01, 2.9385e-09, 6.6847e-04, 4.3968e-06, 1.0603e-04,\n",
            "         5.7207e-04, 1.7402e-07, 3.0456e-08, 9.6892e-01, 9.7081e-01, 2.6628e-08,\n",
            "         9.0631e-09, 1.4284e-09, 2.1126e-09, 1.9257e-05, 6.6625e-08, 2.8884e-04,\n",
            "         7.8405e-01, 5.9668e-09, 8.4900e-01, 3.5124e-04, 7.1678e-09, 3.1555e-01,\n",
            "         2.0360e-08, 4.2284e-10, 5.5217e-05, 3.4902e-05, 7.1983e-01, 5.4332e-09,\n",
            "         8.4158e-09, 5.1940e-10, 2.2386e-05, 1.4107e-09, 8.0859e-08, 4.7744e-07,\n",
            "         4.7207e-05, 5.5121e-08, 1.4917e-08, 8.6046e-04, 9.2897e-01, 1.9978e-08,\n",
            "         6.6287e-09, 1.3976e-08, 4.1813e-01, 9.2475e-01, 9.5739e-01, 3.2699e-08,\n",
            "         1.8110e-05, 4.5828e-08, 9.6273e-08, 8.2273e-01, 4.8930e-09, 2.4912e-08,\n",
            "         8.3251e-10, 5.4085e-05, 1.6022e-03, 1.3040e-04, 8.8697e-01, 9.7421e-01,\n",
            "         1.1327e-09, 2.0383e-08, 4.5687e-11, 1.5147e-07, 1.0428e-07, 5.2537e-09,\n",
            "         6.4948e-01, 5.9660e-05, 2.0189e-09, 5.8491e-04, 5.2441e-06, 1.3994e-05,\n",
            "         8.3323e-01, 9.9984e-01, 6.6691e-11, 1.1827e-04, 3.3796e-06, 8.7650e-05,\n",
            "         3.7402e-07, 2.9536e-08, 4.2147e-09, 2.0255e-10, 1.4997e-08, 2.1109e-08,\n",
            "         1.8497e-09, 1.6057e-04, 9.1105e-11, 8.4030e-09, 1.2831e-09, 9.2319e-01,\n",
            "         5.3157e-04, 8.1099e-01, 7.9998e-09, 5.7623e-01, 2.7147e-09, 1.5770e-07,\n",
            "         4.8513e-09, 8.2266e-01, 1.5712e-07, 1.1007e-07, 8.9703e-09, 5.9842e-09,\n",
            "         6.9562e-06, 5.4816e-07, 3.0024e-10, 9.2763e-01, 1.1047e-04, 1.5214e-06,\n",
            "         4.9318e-01, 7.6142e-10, 3.7999e-04, 4.8514e-08, 9.9156e-08, 2.7987e-10,\n",
            "         5.3215e-09, 9.5469e-01, 1.0577e-05, 9.2253e-01, 6.0607e-10, 1.4792e-04,\n",
            "         1.1321e-08, 9.9770e-05, 7.3981e-01, 2.1744e-05, 4.9395e-04, 1.0179e-05,\n",
            "         8.0500e-01, 5.6667e-09, 5.7949e-09, 4.4563e-09, 1.7099e-09, 1.0388e-04,\n",
            "         1.6512e-03, 1.6726e-08, 9.9916e-01, 6.1472e-09, 1.2258e-08, 8.6444e-01,\n",
            "         9.0696e-07, 1.5732e-08, 2.0292e-09, 2.9445e-08, 2.4948e-07, 9.8611e-09,\n",
            "         8.3539e-09, 7.1169e-01, 3.5461e-06, 3.8422e-09, 5.2201e-08, 2.9814e-08,\n",
            "         2.6804e-07, 3.3538e-08, 2.7513e-08, 9.1901e-01, 6.3325e-09, 9.9798e-01,\n",
            "         7.7862e-01, 4.1843e-05, 7.9861e-06, 3.0308e-09, 8.4315e-01, 1.2954e-09,\n",
            "         1.2460e-08, 7.7180e-10, 2.2996e-05, 1.2215e-08, 6.2110e-09, 8.2655e-01,\n",
            "         3.6945e-05, 3.2417e-01, 1.0283e-03, 2.7580e-08, 4.0542e-09, 3.4691e-08,\n",
            "         6.6108e-07, 3.0105e-09, 9.7552e-01, 1.7708e-05, 1.7415e-09, 9.4157e-01,\n",
            "         3.7882e-05, 3.5874e-06, 5.5053e-04, 2.0658e-09, 1.5401e-08, 2.0007e-07,\n",
            "         2.2479e-04, 1.1229e-09, 1.6151e-08, 1.1804e-08, 7.7223e-01, 3.7277e-10,\n",
            "         4.4656e-04, 1.2306e-08, 2.9837e-07, 3.5961e-10, 5.3784e-09, 2.3095e-08,\n",
            "         6.1856e-06, 6.2845e-09, 1.1142e-08, 1.0907e-04, 9.2745e-01, 4.9273e-01,\n",
            "         1.2354e-08, 1.5155e-08, 1.4644e-10, 1.0459e-06, 7.5067e-08, 3.8596e-08,\n",
            "         3.3255e-08, 7.7763e-01, 5.6209e-01, 5.0998e-04, 1.1840e-08, 1.8874e-08,\n",
            "         1.9257e-05, 2.9781e-07, 1.7950e-10, 1.8190e-09, 6.6008e-01, 4.4122e-09,\n",
            "         1.9925e-09, 6.6644e-01, 9.3827e-01, 1.4854e-09, 2.1911e-09, 6.7236e-01,\n",
            "         9.4360e-01, 1.6255e-05, 8.7386e-08, 3.0791e-08, 3.1166e-08, 1.9991e-09,\n",
            "         2.5457e-04, 4.5782e-07, 8.0648e-08, 8.4303e-10, 2.2592e-08, 8.5823e-01,\n",
            "         3.1556e-08, 4.7744e-10, 7.6690e-01, 6.6088e-07, 5.3052e-09, 4.3816e-10,\n",
            "         7.5970e-09, 3.2836e-05, 4.7483e-04, 3.5539e-07, 7.7957e-08, 1.1541e-08,\n",
            "         9.3230e-01, 2.9801e-07, 4.7235e-09, 9.3478e-01, 4.2123e-09, 2.7761e-09,\n",
            "         8.0007e-01, 9.0503e-07, 2.0986e-10, 6.1346e-10, 9.3591e-09, 9.8755e-01,\n",
            "         6.3972e-01, 9.8713e-01, 3.5377e-09, 9.4293e-01, 2.5506e-09, 4.0700e-09,\n",
            "         6.6268e-01, 2.3433e-04, 8.3910e-01, 2.0671e-09, 1.4989e-10, 5.9096e-10,\n",
            "         1.1920e-06, 2.6699e-04, 1.0412e-03, 5.6589e-01, 6.5476e-01, 5.2166e-09,\n",
            "         9.7100e-09, 9.9437e-01, 6.8317e-09, 7.5631e-09, 4.1578e-10, 1.1008e-08,\n",
            "         4.1241e-07, 9.3433e-01, 1.4254e-07, 1.0066e-04, 6.1767e-09, 3.1816e-08,\n",
            "         1.9677e-09, 1.5040e-08, 6.7157e-01, 7.5990e-07, 2.5154e-04, 3.3128e-06,\n",
            "         7.9674e-01, 8.3975e-09, 3.2029e-04, 3.4434e-09]], device='cuda:0',\n",
            "       grad_fn=<SigmoidBackward>)\n",
            "0.27256677\n",
            "[*] Getting the embeddings and visualizing t-SNE...\n",
            "'dngr_pipeline'  198145.60 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDDFUlB0RjTA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da763ee3-432e-43f2-8aae-c3c4f20b6cc3"
      },
      "source": [
        "embeddings_targets, _, _ = dngr_pipeline(pp_net, N_rand_targets, [500, 200, 100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: RuntimeWarning: divide by zero encountered in log\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "      DropoutNoise-1                 [-1, 1000]               0\n",
            "            Linear-2                  [-1, 500]         500,500\n",
            "           Sigmoid-3                  [-1, 500]               0\n",
            "        BasicBlock-4                  [-1, 500]               0\n",
            "            Linear-5                  [-1, 200]         100,200\n",
            "           Sigmoid-6                  [-1, 200]               0\n",
            "        BasicBlock-7                  [-1, 200]               0\n",
            "            Linear-8                  [-1, 100]          20,100\n",
            "           Sigmoid-9                  [-1, 100]               0\n",
            "       BasicBlock-10                  [-1, 100]               0\n",
            "           Linear-11                  [-1, 200]          20,200\n",
            "          Sigmoid-12                  [-1, 200]               0\n",
            "       BasicBlock-13                  [-1, 200]               0\n",
            "           Linear-14                  [-1, 500]         100,500\n",
            "          Sigmoid-15                  [-1, 500]               0\n",
            "       BasicBlock-16                  [-1, 500]               0\n",
            "           Linear-17                 [-1, 1000]         501,000\n",
            "          Sigmoid-18                 [-1, 1000]               0\n",
            "       BasicBlock-19                 [-1, 1000]               0\n",
            "================================================================\n",
            "Total params: 1,242,500\n",
            "Trainable params: 1,242,500\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.06\n",
            "Params size (MB): 4.74\n",
            "Estimated Total Size (MB): 4.81\n",
            "----------------------------------------------------------------\n",
            "[1,  1000] loss: 0.347\n",
            "[2,  1000] loss: 0.285\n",
            "[3,  1000] loss: 0.267\n",
            "[4,  1000] loss: 0.246\n",
            "[5,  1000] loss: 0.244\n",
            "[6,  1000] loss: 0.231\n",
            "[7,  1000] loss: 0.234\n",
            "[8,  1000] loss: 0.198\n",
            "[9,  1000] loss: 0.189\n",
            "[10,  1000] loss: 0.188\n",
            "[11,  1000] loss: 0.188\n",
            "[12,  1000] loss: 0.188\n",
            "[13,  1000] loss: 0.187\n",
            "[14,  1000] loss: 0.187\n",
            "[15,  1000] loss: 0.187\n",
            "[16,  1000] loss: 0.187\n",
            "[17,  1000] loss: 0.187\n",
            "[18,  1000] loss: 0.187\n",
            "[19,  1000] loss: 0.187\n",
            "[20,  1000] loss: 0.187\n",
            "[21,  1000] loss: 0.187\n",
            "[22,  1000] loss: 0.187\n",
            "[23,  1000] loss: 0.187\n",
            "[24,  1000] loss: 0.187\n",
            "[25,  1000] loss: 0.187\n",
            "[26,  1000] loss: 0.187\n",
            "[27,  1000] loss: 0.187\n",
            "[28,  1000] loss: 0.187\n",
            "[29,  1000] loss: 0.187\n",
            "[30,  1000] loss: 0.187\n",
            "[31,  1000] loss: 0.187\n",
            "[32,  1000] loss: 0.187\n",
            "[33,  1000] loss: 0.187\n",
            "[34,  1000] loss: 0.187\n",
            "[35,  1000] loss: 0.187\n",
            "[36,  1000] loss: 0.187\n",
            "[37,  1000] loss: 0.187\n",
            "[38,  1000] loss: 0.187\n",
            "[39,  1000] loss: 0.187\n",
            "[40,  1000] loss: 0.187\n",
            "[41,  1000] loss: 0.187\n",
            "[42,  1000] loss: 0.187\n",
            "[43,  1000] loss: 0.187\n",
            "[44,  1000] loss: 0.187\n",
            "[45,  1000] loss: 0.187\n",
            "[46,  1000] loss: 0.187\n",
            "[47,  1000] loss: 0.187\n",
            "[48,  1000] loss: 0.187\n",
            "[49,  1000] loss: 0.187\n",
            "[50,  1000] loss: 0.187\n",
            "[51,  1000] loss: 0.187\n",
            "[52,  1000] loss: 0.187\n",
            "[53,  1000] loss: 0.187\n",
            "[54,  1000] loss: 0.187\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vF5J3xU0RjTB"
      },
      "source": [
        "#pos_train, neg_train, pos_test, neg_test = train_test_split_pu(dp_net, train_size=0.5)\n",
        "S, H, W =pu_learning_new(50, embeddings_drugs, embeddings_targets, dp_net, batch_size=300, lr=1e-3, n_epochs=1000)\n",
        "S"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40nSSBPgrq7J"
      },
      "source": [
        "test = torch.logical_or(pos_test, neg_test)\n",
        "train = torch.logical_or(pos_train, neg_train)\n",
        "\n",
        "print(torch.sum(test), torch.sum(train))\n",
        "print(torch.std(S), torch.std(S[test]), torch.std(S[train]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lq5nYGV7dqw0"
      },
      "source": [
        "S[S>1.]=1.\n",
        "S[S<0.]=0."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86PTYvrRouQO"
      },
      "source": [
        "def binarize(S, threshold=0.5):\n",
        "    P = (S > threshold).float()\n",
        "    return(P)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RhnpOm_pbw1"
      },
      "source": [
        "P=binarize(S)\n",
        "print(P)\n",
        "dp_net=torch.Tensor(dp_net)\n",
        "\n",
        "TP=dp_net==1\n",
        "print(TP)\n",
        "\n",
        "print(P[TP].size())\n",
        "print(torch.sum(TP))\n",
        "\n",
        "torch.sum(abs(P[TP]-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzqLVr4HuMXY"
      },
      "source": [
        "compute_auc(torch.Tensor(dp_net), S)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjJWiAc3RjTB"
      },
      "source": [
        "# [References]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xhpyoOpRjTB"
      },
      "source": [
        "[1] X. Zeng, S. Zhu, W. Lu, Z. Liu, J. Huang, Y. Zhou, J. Fang, Y. Huang, H. Guo, L. Li, B. D. Trapp, R. Nussinov, C. Eng, J. Loscalzo, F. Cheng, Target identification among known drugs by deep learning from heterogeneous networks. Chem. Sci.11, 1775–1797 (2020).\n",
        "\n",
        "[2] Shaosheng Cao, Wei Lu, and Qiongkai Xu. 2016. Deep neural networks for learning graph representations. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI'16). AAAI Press, 1145–1152."
      ]
    }
  ]
}