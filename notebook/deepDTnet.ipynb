{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deepDTnet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvSeRlr7RjSm"
      },
      "source": [
        "# Implémentation en diverses étapes de l'algorithme deepDTnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_3w6Pe9RjSu"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.autograd as ag\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "from scipy.optimize import minimize, NonlinearConstraint\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from math import sqrt\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DL_tOkk9PoIi"
      },
      "source": [
        "def timeit(method):\r\n",
        "    def timed(*args, **kw):\r\n",
        "        ts = time.time()\r\n",
        "        result = method(*args, **kw)\r\n",
        "        te = time.time()\r\n",
        "        if 'log_time' in kw:\r\n",
        "            name = kw.get('log_name', method.__name__.upper())\r\n",
        "            kw['log_time'][name] = int((te - ts) * 1000)\r\n",
        "        else:\r\n",
        "            print('%r  %2.2f ms' % \\\r\n",
        "                  (method.__name__, (te - ts) * 1000))\r\n",
        "        return result\r\n",
        "    return timed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ9Ap3tnRjSw"
      },
      "source": [
        "### Computing co-occurence matrices (PCO) with random surfing with return"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRQZJuukRjSx"
      },
      "source": [
        "def normalize(M):\n",
        "    #Put diagonal elements to 0\n",
        "    M  = M - np.diag(np.diag(M))\n",
        "    \n",
        "    #Normalizing by row\n",
        "    D_inv = np.diag(np.reciprocal(np.sum(M,axis=0)))\n",
        "    M = np.dot(D_inv,  M)\n",
        "\n",
        "    return M"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOTgjQ6nRjSx"
      },
      "source": [
        "def PCO(A, K, alpha):\n",
        "    \"\"\"\n",
        "    For a graph represented by its adjacency matrix *A*, computes the co-occurence matrix by random \n",
        "    surfing on the graph with returns. 1-alpha is the probability to make, at each step, a return \n",
        "    to the original step.\n",
        "    \"\"\"\n",
        "    A=np.array(A, dtype=float)\n",
        "    \n",
        "    #The adjacency matrix A is first normalized\n",
        "    A=normalize(A) \n",
        "    \n",
        "    n=A.shape[0]\n",
        "    \n",
        "    I=np.eye(n)\n",
        "    \n",
        "    P=I\n",
        "    M=np.zeros((n, n))\n",
        "    \n",
        "    for i in range(K):\n",
        "        P = alpha*np.dot(P,A) + (1-alpha)*I\n",
        "        M = M+P\n",
        "    \n",
        "    return(M)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kx3dbRO_RjSy",
        "outputId": "854336f3-75cf-4ef2-f2db-0047509506c7"
      },
      "source": [
        "PCO([[0,1,1],[1,0,1],[1,1,0]], 4, 0.6)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.0482, 0.9759, 0.9759],\n",
              "       [0.9759, 2.0482, 0.9759],\n",
              "       [0.9759, 0.9759, 2.0482]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLKh_0AnRjS0"
      },
      "source": [
        "### From co-occurence matrices (PCO) to shifted positive pointwise mutual information (PPMI) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4tgEYiKRjS0"
      },
      "source": [
        "def PPMI(M):\n",
        "    \"\"\"Computes the shifted positive pointwise mutual information (PPMI) matrix\n",
        "    from the co-occurence matrix (PCO) of a graph.\"\"\"\n",
        "    \n",
        "    M=normalize(M)\n",
        "    cols = np.sum(M, axis=0)\n",
        "    rows = np.sum(M, axis=1).reshape((-1,1))\n",
        "    s = np.sum(rows)\n",
        "    \n",
        "    P = s*M\n",
        "    P /= cols\n",
        "    P /= rows\n",
        "    \n",
        "    #P[np.where(P<0)] = 1.0\n",
        "    P = np.log(P)\n",
        "\n",
        "    #To avoid NaN when applying log\n",
        "    P[np.isnan(P)] = 0.0\n",
        "    P[np.isinf(P)] = 0.0\n",
        "    P[np.isneginf(P)] = 0.0\n",
        "    P[np.where(P<0)] = 0.0\n",
        "    \n",
        "    return(P)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ds5P-tPBRjS1",
        "outputId": "8109d8f1-e24a-44b7-d49c-f67c550cc5d1"
      },
      "source": [
        "PPMI(PCO([[0,1,1],[1,0,1],[1,1,0]], 4, 0.4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: RuntimeWarning: divide by zero encountered in log\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.40546511, 0.40546511],\n",
              "       [0.40546511, 0.        , 0.40546511],\n",
              "       [0.40546511, 0.40546511, 0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isQ5vZj6RjS1"
      },
      "source": [
        "### Embedding with stacked denoising autoencoders (SDAE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjXntppYRjS2"
      },
      "source": [
        "class GaussianNoise(nn.Module):\n",
        "    def __init__(self, stddev):\n",
        "        super().__init__()\n",
        "        self.stddev = stddev\n",
        "\n",
        "    def forward(self, din):\n",
        "        if self.training:\n",
        "            return din + torch.randn(din.size()) * self.stddev\n",
        "        return din"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koJYjk-2RjS2"
      },
      "source": [
        "class DropoutNoise(nn.Module):\n",
        "    def __init__(self, p):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    def forward(self, x):\n",
        "        t = torch.rand(x.size()).to(self.device)\n",
        "        a = t > self.p\n",
        "        \n",
        "        return(x*a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv0BqEnTRjS3"
      },
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, input_shape, n_neurons, activation='relu', noise=None, noise_arg=None):\n",
        "        super().__init__()\n",
        "        self.n_neurons = n_neurons\n",
        "        self.input_shape = input_shape\n",
        "        \n",
        "        self.has_noise = False\n",
        "        \n",
        "        if noise=='gaussian':\n",
        "            self.has_noise = True\n",
        "            self.noise = GaussianNoise(noise_arg)\n",
        "        elif noise=='dropout':\n",
        "            self.has_noise = True\n",
        "            self.noise = DropoutNoise(noise_arg)\n",
        "            \n",
        "        self.dense_layer = nn.Linear(self.input_shape, self.n_neurons)\n",
        "        \n",
        "        activations_map = {'relu':nn.ReLU, 'tanh':nn.Tanh, 'sigmoid':nn.Sigmoid}\n",
        "        self.activation = activations_map[activation]()\n",
        "\n",
        "    def forward(self, features):\n",
        "        x=features\n",
        "        \n",
        "        if self.has_noise:\n",
        "            x = self.noise(x)\n",
        "\n",
        "        x = self.dense_layer(features)\n",
        "        x = self.activation(x)\n",
        "        \n",
        "        return(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zXD66rIRjS3"
      },
      "source": [
        "class SDAE(nn.Module):\n",
        "    def __init__(self, input_shape, hidden_layers, activation='relu', last_activation='relu', noise_type='dropout', noise_arg=0.2):\n",
        "        super().__init__()\n",
        "        self.inputs = [input_shape] + hidden_layers\n",
        "        \n",
        "        n = len(self.inputs)\n",
        "        encoder_units = [BasicBlock(self.inputs[0], self.inputs[1], activation=activation, noise=noise_type, noise_arg=noise_arg)]\n",
        "        encoder_units.extend([BasicBlock(self.inputs[i], self.inputs[i+1], activation=activation) for i in range(1, n-1)])\n",
        "        \n",
        "        self.encoder = nn.Sequential(*encoder_units)\n",
        "        \n",
        "        decoder_units = [BasicBlock(self.inputs[i], self.inputs[i-1], activation=activation) for i in range(n-1,1,-1)]\n",
        "        decoder_units.append(BasicBlock(self.inputs[1], self.inputs[0], activation=last_activation))\n",
        "        \n",
        "        self.decoder = nn.Sequential(*decoder_units)\n",
        "        \n",
        "    def forward(self, features):\n",
        "        encoded = self.encoder(features)\n",
        "        \n",
        "        decoded = self.decoder(encoded)\n",
        "        \n",
        "        return(decoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OY1QMhHrRjS6"
      },
      "source": [
        "### PU-Learning via matrix completion and convex optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIsHOEAkRjS6"
      },
      "source": [
        "def old_pu_learning(x, y, P, k = 7, alpha = 0.2, gamma = 0.3, maxiter=1000):\n",
        "    Fd = x.shape[1]\n",
        "    Ft = y.shape[1]\n",
        "    Nd = x.shape[0]\n",
        "    Nt = y.shape[0]\n",
        "    \n",
        "    #Number of variables\n",
        "    N_variables = Fd * k + Ft * k\n",
        "    \n",
        "    print(\"Number of variables:\", N_variables)    \n",
        "    print(\"Finding positive and negative examples...\")\n",
        "    \n",
        "    Ipos = np.where(P==1.)\n",
        "    Ineg = np.where(P==0.)\n",
        "\n",
        "    print(\"Number of positive examples:\", Ipos[0].shape[0])\n",
        "    print(\"Number of negative/unlabelled examples:\", Ineg[0].shape[0])\n",
        "    \n",
        "    alpha_rac = sqrt(alpha)\n",
        "    \n",
        "    @timeit\n",
        "    def objective(z):\n",
        "        H = z[:Fd*k].reshape((Fd,k))\n",
        "        W = z[-Ft*k:].reshape((Ft,k))\n",
        "        \n",
        "        M = P - (x @ H @ np.transpose(W) @ np.transpose(y))\n",
        "        \n",
        "        M[Ineg] *= alpha_rac\n",
        "        \n",
        "        L = torch.sum(M**2) + gamma/2 * (np.sum(H**2, axis=(0,1)) + np.sum(W**2, axis=(0,1)))\n",
        "        print(L)\n",
        "\n",
        "        return(L)\n",
        "    \n",
        "    def constraint(z):\n",
        "        H = z[:Fd*k].reshape((Fd,k))\n",
        "        W = z[-Ft*k:].reshape((Ft,k))\n",
        "        S = x @ H @ np.transpose(W) @ np.transpose(y)\n",
        "        S = S.reshape((-1,))\n",
        "        \n",
        "        return(S)\n",
        "    \n",
        "    nlc = NonlinearConstraint(constraint, np.zeros(Nt*Nd), np.ones(Nt*Nd))\n",
        "\n",
        "    print(\"Going to minimize... Maximum number of iterations:\", maxiter)\n",
        "    res=minimize(objective, x0 = np.random.randn(N_variables), options={'maxiter':maxiter, 'disp':'True'}, constraints=[nlc], method='trust-constr')\n",
        "    \n",
        "    print(\"\\n\\nSolved.\")\n",
        "    \n",
        "    z=res['x']\n",
        "    H = z[:Fd*k].reshape((Fd,k))\n",
        "    W = z[-Ft*k:].reshape((Ft,k))\n",
        "\n",
        "    print(\"Now computing Z=HW^T, then will compute S...\")\n",
        "    \n",
        "    S = x @ H @ np.transpose(W) @ np.transpose(y)\n",
        "    \n",
        "    return(S)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IT0t_9SQdGtR"
      },
      "source": [
        "def pu_learning(x, y, P, pos_train_mask, neg_train_mask, k = 7, alpha = 0.2, gamma = 0.3, maxiter=1000, lr=0.1):\r\n",
        "    Fd = x.shape[1]\r\n",
        "    Ft = y.shape[1]\r\n",
        "    Nd = x.shape[0]\r\n",
        "    Nt = y.shape[0]\r\n",
        "    \r\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "\r\n",
        "    #Number of variables\r\n",
        "    N_variables = Fd * k + Ft * k\r\n",
        "    \r\n",
        "    print(\"Number of variables:\", N_variables)    \r\n",
        "    print(\"Finding positive and negative examples...\")\r\n",
        "    \r\n",
        "    P = torch.Tensor(P).to(device)\r\n",
        "    x = torch.Tensor(x).to(device)\r\n",
        "    y = torch.Tensor(y).to(device)\r\n",
        "    \r\n",
        "    x_norm = torch.linalg.norm(x)\r\n",
        "    y_norm = torch.linalg.norm(y)\r\n",
        "\r\n",
        "    Ipos = pos_train_mask\r\n",
        "    Ineg = neg_train_mask\r\n",
        "    train_mask = torch.logical_or(Ipos,Ineg)\r\n",
        "\r\n",
        "    print(\"Number of positive examples:\", P[Ipos].size()[0])\r\n",
        "    print(\"Number of negative/unlabelled examples:\", P[Ineg].size()[0])\r\n",
        "    \r\n",
        "    alpha_rac = sqrt(alpha)\r\n",
        "    \r\n",
        "    def objective(H,W):\r\n",
        "        M = P - torch.chain_matmul(x, H, torch.transpose(W, 0, 1), torch.transpose(y, 0, 1))\r\n",
        "        \r\n",
        "        M[Ineg] *= alpha_rac\r\n",
        "        M[~train_mask] = 0.\r\n",
        "        \r\n",
        "        L = torch.sum(M**2) + gamma/2 * (torch.sum(H**2) + torch.sum(W**2))\r\n",
        "\r\n",
        "        return(L)\r\n",
        "    \r\n",
        "    def constraint(z):\r\n",
        "        z = torch.Tensor(z).to(device)\r\n",
        "        H = z[:Fd*k].resize(Fd,k)\r\n",
        "        W = z[-Ft*k:].resize(Ft,k)\r\n",
        "\r\n",
        "        S = torch.chain_matmul(x, H, torch.transpose(W, 0, 1), torch.transpose(y, 0, 1))\r\n",
        "        S = S.reshape((-1,)).cpu().detach().numpy()\r\n",
        "        \r\n",
        "        return(S)\r\n",
        "    \r\n",
        "    #nlc = NonlinearConstraint(constraint, np.zeros(Nt*Nd), np.ones(Nt*Nd))\r\n",
        "\r\n",
        "    print(\"Going to minimize... Maximum number of iterations:\", maxiter)\r\n",
        "    #res=minimize(objective, x0 = np.random.randn(N_variables), options={'maxiter':maxiter, 'disp':'True'}, constraints=[nlc], method='trust-constr')\r\n",
        "    W = ag.Variable(torch.rand(Ft,k).to(device)/y_norm, requires_grad=True)\r\n",
        "    H = ag.Variable(torch.rand(Fd,k).to(device)/x_norm, requires_grad=True)\r\n",
        "    \r\n",
        "    opt = torch.optim.Adam([H,W], lr=lr)\r\n",
        "\r\n",
        "    for i in range(maxiter):\r\n",
        "        # Zeroing gradients\r\n",
        "        opt.zero_grad()\r\n",
        "\r\n",
        "        # Evaluating the objective\r\n",
        "        obj = objective(H,W)\r\n",
        "\r\n",
        "        # Calculate gradients\r\n",
        "        obj.backward() \r\n",
        "        opt.step()\r\n",
        "        if i%1000==0:  \r\n",
        "            print(\"Objective: \", obj)\r\n",
        "\r\n",
        "    print(\"\\n\\nSolved.\")\r\n",
        "    \r\n",
        "    print(\"Now computing Z=HW^T, then will compute S...\")\r\n",
        "    \r\n",
        "    S = torch.chain_matmul(x, H, torch.transpose(W,0,1), torch.transpose(y,0,1))\r\n",
        "    \r\n",
        "    return(S, H, W)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACJPxqCZ9vDE"
      },
      "source": [
        "def train_test_split_pu(P, train_size=0.8):\r\n",
        "    P = torch.Tensor(P)\r\n",
        "    Ipos = (P == 1.)\r\n",
        "    Ineg = (P == 0.)\r\n",
        "\r\n",
        "    pos_train = Ipos * torch.rand(P.size())\r\n",
        "    pos_train[Ineg] = 1.\r\n",
        "    pos_train = pos_train < train_size\r\n",
        "    train_neg_rel_size = torch.sum(pos_train) / torch.sum(Ineg)\r\n",
        "    \r\n",
        "    neg_train = Ineg * torch.rand(P.size())\r\n",
        "    neg_train[Ipos] = 1.\r\n",
        "    neg_train = neg_train < train_neg_rel_size\r\n",
        "\r\n",
        "    train = pos_train + neg_train\r\n",
        "    test = ~train\r\n",
        "    pos_test = torch.logical_and(test, Ipos)\r\n",
        "    neg_test = torch.logical_and(test, Ineg)\r\n",
        "\r\n",
        "    return(pos_train, neg_train, pos_test, neg_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgqj9kraRjS8"
      },
      "source": [
        "### Generate random networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4HpNjF5RjS8"
      },
      "source": [
        "def random_graph(p, size=(100,100)):\n",
        "    return(np.array([[int(random.random() < p) for i in range(size[1])] for j in range(size[0])]))\n",
        "\n",
        "def random_undirected_graph(p, size=(100,100)):\n",
        "    graph = random_graph(p, size=size)\n",
        "    graph[np.arange(size[0]),np.arange(size[1])]=0 #nullify the diagonal\n",
        "    graph = np.maximum(graph, graph.T) #make it symmetric\n",
        "    \n",
        "    return(graph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuPKe8J7RjS8"
      },
      "source": [
        "def random_graph_with_fixed_components(p, nodes_per_component=[50,50]):\n",
        "    nodes_per_component = np.array(nodes_per_component)\n",
        "    n_nodes = nodes_per_component.sum()\n",
        "    n_cmp = nodes_per_component.shape[0]\n",
        "    \n",
        "    graph = np.zeros((n_nodes, n_nodes))\n",
        "    nodes = np.arange(n_nodes)\n",
        "    np.random.shuffle(nodes)\n",
        "    \n",
        "    cmp_nodes = []\n",
        "    acc=0\n",
        "    \n",
        "    for i in range(n_cmp):\n",
        "        cmp = nodes[acc:(acc+nodes_per_component[i])]\n",
        "        cmp_nodes.append(cmp)\n",
        "        acc += nodes_per_component[i]\n",
        "        \n",
        "        size = cmp.shape[0]\n",
        "        submatrix=np.ix_(cmp,cmp)\n",
        "\n",
        "        graph[submatrix] = random_undirected_graph(p, (size,size))\n",
        "        \n",
        "    return(graph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdrtB3xYRjS9"
      },
      "source": [
        "def neighbors(adj, i):\n",
        "    return (np.where(adj[i,:]==1)[0])\n",
        "\n",
        "def dfs(adj, i):\n",
        "    n = adj.shape[0] #number of nodes in the graph\n",
        "    visited = [False for k in range(n)]\n",
        "    \n",
        "    stack = [i]\n",
        "    \n",
        "    while(len(stack)>0):\n",
        "        k = stack.pop()\n",
        "        neighborhood = neighbors(adj, k)\n",
        "        visited[k] = True\n",
        "        \n",
        "        for n in neighborhood:\n",
        "            if not visited[n]:\n",
        "                stack.append(n)\n",
        "    \n",
        "    return(np.where(visited))\n",
        "\n",
        "def connected_components(adj):\n",
        "    n = adj.shape[0]\n",
        "    \n",
        "    visited = np.array([0 for k in range(n)])\n",
        "    s = np.sum(visited)\n",
        "    \n",
        "    comp=[]\n",
        "    \n",
        "    while s<n:\n",
        "        i = np.where(1-visited)[0][0]\n",
        "        \n",
        "        cmp = dfs(adj, i)\n",
        "        visited[cmp] = 1\n",
        "        s = np.sum(visited)\n",
        "        \n",
        "        comp.append(list(cmp[0]))\n",
        "    \n",
        "    return(np.array(comp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTLxK-OmRjS9",
        "outputId": "9fae677d-5aaa-477f-d567-762d2dc3c094"
      },
      "source": [
        "N_rand_drugs = 784\n",
        "N_rand_targets = 1000\n",
        "\n",
        "density=0.2\n",
        "#generate random matrices in M({0,1})\n",
        "#1. a drug-drug network (drug similarities)\n",
        "#2. a protein-protein network (protein similarities)\n",
        "#3. a drug-protein network (drug-target known relationships)\n",
        "dd_net = random_graph_with_fixed_components(density, [196,196,196,196])\n",
        "pp_net = random_graph_with_fixed_components(density, [100 for i in range(10)])\n",
        "dp_net = random_graph(density,size=(N_rand_drugs,N_rand_targets))\n",
        "\n",
        "np.sum(dp_net)/(N_rand_drugs*N_rand_targets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1997155612244898"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bW3Utn4lRjS9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a498f535-3279-4562-e07d-3952b775125b"
      },
      "source": [
        "np.sum(np.sum(dd_net))/(dd_net[0].shape[0])**2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.08923039879216993"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkahyGsdRjS9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22b75c9f-c1a3-4438-c9ce-eb4c1e2f5297"
      },
      "source": [
        "dd_net[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
              "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
              "       0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "       0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
              "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
              "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
              "       0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkczPipwRjS-"
      },
      "source": [
        "### Encoding the drug-drug network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZATeBeSRjS-"
      },
      "source": [
        "def sdae(input_net, input_number, hidden_layers, n_epochs=100, batch_size=1, activation='sigmoid', last_activation='sigmoid'):\n",
        "    #hidden_layers=[500,200,100]\n",
        "    #input_numer=784\n",
        "    #  use gpu if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = SDAE(input_number, hidden_layers, activation=activation, last_activation=last_activation).to(device)\n",
        "\n",
        "    # create an optimizer object\n",
        "    # Adam optimizer with learning rate 1e-3\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    # mean-squared error loss\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    summary(model, (input_number,))\n",
        "    \n",
        "    tensor_net = torch.Tensor(input_net).to(device)\n",
        "    train = torch.utils.data.TensorDataset(tensor_net, tensor_net)\n",
        "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "            \n",
        "            inputs=inputs.to(device)\n",
        "            \n",
        "            inputs=torch.flatten(inputs)\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "        \n",
        "            loss = criterion(outputs, inputs)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % input_number == input_number-1: \n",
        "                print('[%d, %5d] loss: %.3f' %\n",
        "                      (epoch + 1, i + 1, running_loss / input_number))\n",
        "                running_loss = 0.0\n",
        "\n",
        "    print('Finished Training')\n",
        "    \n",
        "    return(model, train_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opliyCSJRjS_"
      },
      "source": [
        "### Embedding evaluation with t-SNE visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhpOJc-4RjS_"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def visualize_TSNE(embeddings,target):\n",
        "    tsne = TSNE(n_components=2, init='pca',\n",
        "                         random_state=0, perplexity=30)\n",
        "    data = tsne.fit_transform(embeddings)\n",
        "    #plt.figure(figsize=(12, 6))\n",
        "    plt.title(\"TSNE visualization of the embeddings\")\n",
        "    plt.scatter(data[:,0],data[:,1],c=target)\n",
        "\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bSdG2NORjS_"
      },
      "source": [
        "def get_embeddings(train_loader, N, model, size_encoded=100):\n",
        "    trainiter = iter(train_loader)\n",
        "    embeddings = np.zeros((N, size_encoded))\n",
        "\n",
        "    for i,q in enumerate(trainiter):\n",
        "        embedded = model.encoder(q[0]).cpu().detach().numpy()\n",
        "        embeddings[i,:] = embedded.reshape((size_encoded,))\n",
        "    \n",
        "    return(embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQz3Ap-ORjTA"
      },
      "source": [
        "@timeit\n",
        "def dngr_pipeline(network, N, hidden_layers, K=10, alpha=0.2, n_epochs=100, batch_size=1, activation='sigmoid', last_activation='sigmoid'):\n",
        "    ppmi_net = PPMI(PCO(network, K, alpha))\n",
        "    model, train_loader = sdae(ppmi_net, N, hidden_layers, n_epochs=n_epochs, batch_size=batch_size, activation=activation, last_activation=last_activation)\n",
        "    \n",
        "    print(\"[*] Visualizing an example's output...\")\n",
        "    trainiter = iter(train_loader)\n",
        "    inputs, _ = trainiter.next()\n",
        "\n",
        "    print(inputs)\n",
        "    print(model(inputs))\n",
        "\n",
        "    print(mean_squared_error(inputs.cpu().detach().numpy(), model(inputs).cpu().detach().numpy()))\n",
        "    \n",
        "    print(\"[*] Getting the embeddings and visualizing t-SNE...\")\n",
        "    embeddings=get_embeddings(train_loader, N, model, size_encoded=hidden_layers[-1])\n",
        "    \n",
        "    cmps = connected_components(network)\n",
        "    targets = [0 for i in range(N)]\n",
        "\n",
        "    for i, cmp in enumerate(cmps):\n",
        "        for n in cmp:\n",
        "            targets[n] = i\n",
        "    \n",
        "    visualize_TSNE(embeddings, targets)\n",
        "    \n",
        "    return(embeddings, model, train_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UBFoLXR8RjTA",
        "outputId": "1ccb8698-290f-4323-f49c-cad81647257b"
      },
      "source": [
        "embeddings_drugs, _, _ = dngr_pipeline(dd_net, N_rand_drugs, [500, 200, 100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: RuntimeWarning: divide by zero encountered in log\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "      DropoutNoise-1                  [-1, 784]               0\n",
            "            Linear-2                  [-1, 500]         392,500\n",
            "           Sigmoid-3                  [-1, 500]               0\n",
            "        BasicBlock-4                  [-1, 500]               0\n",
            "            Linear-5                  [-1, 200]         100,200\n",
            "           Sigmoid-6                  [-1, 200]               0\n",
            "        BasicBlock-7                  [-1, 200]               0\n",
            "            Linear-8                  [-1, 100]          20,100\n",
            "           Sigmoid-9                  [-1, 100]               0\n",
            "       BasicBlock-10                  [-1, 100]               0\n",
            "           Linear-11                  [-1, 200]          20,200\n",
            "          Sigmoid-12                  [-1, 200]               0\n",
            "       BasicBlock-13                  [-1, 200]               0\n",
            "           Linear-14                  [-1, 500]         100,500\n",
            "          Sigmoid-15                  [-1, 500]               0\n",
            "       BasicBlock-16                  [-1, 500]               0\n",
            "           Linear-17                  [-1, 784]         392,784\n",
            "          Sigmoid-18                  [-1, 784]               0\n",
            "       BasicBlock-19                  [-1, 784]               0\n",
            "================================================================\n",
            "Total params: 1,026,284\n",
            "Trainable params: 1,026,284\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.06\n",
            "Params size (MB): 3.91\n",
            "Estimated Total Size (MB): 3.98\n",
            "----------------------------------------------------------------\n",
            "[1,   784] loss: 0.405\n",
            "[2,   784] loss: 0.309\n",
            "[3,   784] loss: 0.306\n",
            "[4,   784] loss: 0.305\n",
            "[5,   784] loss: 0.305\n",
            "[6,   784] loss: 0.305\n",
            "[7,   784] loss: 0.306\n",
            "[8,   784] loss: 0.306\n",
            "[9,   784] loss: 0.305\n",
            "[10,   784] loss: 0.305\n",
            "[11,   784] loss: 0.305\n",
            "[12,   784] loss: 0.304\n",
            "[13,   784] loss: 0.304\n",
            "[14,   784] loss: 0.303\n",
            "[15,   784] loss: 0.303\n",
            "[16,   784] loss: 0.304\n",
            "[17,   784] loss: 0.303\n",
            "[18,   784] loss: 0.303\n",
            "[19,   784] loss: 0.303\n",
            "[20,   784] loss: 0.302\n",
            "[21,   784] loss: 0.303\n",
            "[22,   784] loss: 0.302\n",
            "[23,   784] loss: 0.303\n",
            "[24,   784] loss: 0.303\n",
            "[25,   784] loss: 0.303\n",
            "[26,   784] loss: 0.302\n",
            "[27,   784] loss: 0.302\n",
            "[28,   784] loss: 0.302\n",
            "[29,   784] loss: 0.302\n",
            "[30,   784] loss: 0.302\n",
            "[31,   784] loss: 0.302\n",
            "[32,   784] loss: 0.302\n",
            "[33,   784] loss: 0.302\n",
            "[34,   784] loss: 0.301\n",
            "[35,   784] loss: 0.301\n",
            "[36,   784] loss: 0.302\n",
            "[37,   784] loss: 0.301\n",
            "[38,   784] loss: 0.301\n",
            "[39,   784] loss: 0.301\n",
            "[40,   784] loss: 0.301\n",
            "[41,   784] loss: 0.301\n",
            "[42,   784] loss: 0.301\n",
            "[43,   784] loss: 0.300\n",
            "[44,   784] loss: 0.301\n",
            "[45,   784] loss: 0.300\n",
            "[46,   784] loss: 0.300\n",
            "[47,   784] loss: 0.300\n",
            "[48,   784] loss: 0.300\n",
            "[49,   784] loss: 0.299\n",
            "[50,   784] loss: 0.299\n",
            "[51,   784] loss: 0.299\n",
            "[52,   784] loss: 0.300\n",
            "[53,   784] loss: 0.299\n",
            "[54,   784] loss: 0.299\n",
            "[55,   784] loss: 0.299\n",
            "[56,   784] loss: 0.299\n",
            "[57,   784] loss: 0.300\n",
            "[58,   784] loss: 0.298\n",
            "[59,   784] loss: 0.298\n",
            "[60,   784] loss: 0.298\n",
            "[61,   784] loss: 0.298\n",
            "[62,   784] loss: 0.298\n",
            "[63,   784] loss: 0.298\n",
            "[64,   784] loss: 0.298\n",
            "[65,   784] loss: 0.298\n",
            "[66,   784] loss: 0.297\n",
            "[67,   784] loss: 0.297\n",
            "[68,   784] loss: 0.297\n",
            "[69,   784] loss: 0.297\n",
            "[70,   784] loss: 0.296\n",
            "[71,   784] loss: 0.296\n",
            "[72,   784] loss: 0.296\n",
            "[73,   784] loss: 0.296\n",
            "[74,   784] loss: 0.296\n",
            "[75,   784] loss: 0.297\n",
            "[76,   784] loss: 0.296\n",
            "[77,   784] loss: 0.296\n",
            "[78,   784] loss: 0.295\n",
            "[79,   784] loss: 0.296\n",
            "[80,   784] loss: 0.295\n",
            "[81,   784] loss: 0.295\n",
            "[82,   784] loss: 0.295\n",
            "[83,   784] loss: 0.294\n",
            "[84,   784] loss: 0.295\n",
            "[85,   784] loss: 0.294\n",
            "[86,   784] loss: 0.294\n",
            "[87,   784] loss: 0.293\n",
            "[88,   784] loss: 0.293\n",
            "[89,   784] loss: 0.293\n",
            "[90,   784] loss: 0.293\n",
            "[91,   784] loss: 0.293\n",
            "[92,   784] loss: 0.293\n",
            "[93,   784] loss: 0.292\n",
            "[94,   784] loss: 0.293\n",
            "[95,   784] loss: 0.292\n",
            "[96,   784] loss: 0.292\n",
            "[97,   784] loss: 0.292\n",
            "[98,   784] loss: 0.292\n",
            "[99,   784] loss: 0.292\n",
            "[100,   784] loss: 0.291\n",
            "Finished Training\n",
            "[*] Visualizing an example's output...\n",
            "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 2.3244, 0.0000, 0.0000, 0.0000, 0.0000, 2.5056, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.4824, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 2.3613, 2.2148, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.2222, 2.1526,\n",
            "         0.0000, 0.0000, 2.4672, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.3659,\n",
            "         0.0000, 0.0000, 0.0000, 2.2319, 2.4385, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.4569, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.4175, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0389, 2.3515, 0.0000, 0.0000, 2.4283, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.2615, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         2.3788, 0.0000, 2.3761, 0.0000, 2.4567, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.3696, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.2990, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         2.3087, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.3318, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.3271, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.2719, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 2.3632, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         2.2226, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 2.5569, 0.0000, 2.5149, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 2.4027, 2.4526, 0.0000, 0.0000, 0.0000,\n",
            "         2.4402, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.3709, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.3047, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.2418, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 2.3884, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         2.4763, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 2.3104, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.4895, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.1745, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.4637, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.4967, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.4485,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.2716, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.3288, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 2.2590, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 2.4272, 0.0000, 0.0000, 0.0000, 2.3558, 0.0000, 0.0000,\n",
            "         2.3635, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 2.3190, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.2448, 0.0000, 0.0000, 0.0000,\n",
            "         2.4481, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.3292, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.4018, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 2.3990, 0.0000, 0.0000, 0.0000, 2.4107, 0.0000,\n",
            "         2.4584, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         2.3971, 0.0000, 0.0000, 0.0000, 2.4417, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.4381, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 2.4164, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.3724, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 2.3415, 0.0000, 0.0000, 2.1928, 0.0000, 0.0000,\n",
            "         0.0000]], device='cuda:0')\n",
            "tensor([[7.6749e-01, 2.5863e-09, 1.3394e-05, 1.9288e-08, 3.8137e-06, 8.2840e-01,\n",
            "         5.5328e-04, 9.2560e-03, 6.4573e-05, 6.3829e-01, 9.1458e-01, 1.1783e-03,\n",
            "         1.3397e-03, 6.6463e-04, 7.9461e-04, 8.9687e-01, 3.2601e-09, 1.0297e-05,\n",
            "         1.4149e-03, 1.0133e-07, 4.6643e-08, 2.5663e-06, 1.0140e-09, 1.6160e-06,\n",
            "         8.4752e-08, 5.0547e-07, 7.7265e-04, 5.0435e-08, 4.1372e-03, 1.7077e-05,\n",
            "         9.7849e-07, 2.1178e-09, 9.3297e-01, 9.2226e-01, 7.2618e-08, 4.3835e-07,\n",
            "         1.7771e-04, 4.0787e-09, 1.9397e-09, 1.8226e-09, 1.1027e-06, 7.8971e-01,\n",
            "         8.8481e-01, 6.4132e-06, 5.5420e-10, 7.8404e-06, 7.6296e-06, 9.5734e-07,\n",
            "         9.5693e-01, 9.1033e-01, 9.6450e-01, 9.3786e-01, 5.4375e-06, 8.8027e-01,\n",
            "         1.3217e-08, 2.1316e-07, 2.6593e-06, 1.0373e-05, 4.0414e-06, 2.5195e-06,\n",
            "         8.6613e-01, 9.0518e-01, 8.5558e-01, 6.8000e-10, 1.9793e-03, 7.3817e-01,\n",
            "         6.0423e-06, 8.9369e-01, 8.3016e-04, 3.8553e-06, 1.3706e-04, 7.1044e-10,\n",
            "         2.3726e-07, 8.6993e-01, 9.6801e-04, 1.0393e-03, 3.6274e-06, 2.5357e-06,\n",
            "         3.6446e-06, 2.7809e-03, 9.1368e-01, 8.7559e-01, 2.0413e-06, 2.7000e-03,\n",
            "         9.2699e-01, 9.4454e-01, 2.9233e-09, 6.9994e-04, 1.8159e-07, 1.2953e-05,\n",
            "         8.7596e-06, 7.7336e-01, 1.2620e-05, 1.8427e-03, 9.5591e-01, 2.9650e-04,\n",
            "         2.2874e-11, 2.0129e-03, 4.7809e-03, 2.1292e-06, 5.0122e-09, 9.1294e-01,\n",
            "         4.1627e-09, 4.3713e-03, 2.7509e-06, 7.9714e-01, 2.5885e-03, 1.0349e-08,\n",
            "         3.0338e-05, 2.5325e-03, 2.8566e-07, 8.2734e-04, 1.8175e-06, 1.4183e-03,\n",
            "         1.7506e-03, 4.4138e-04, 2.0345e-06, 3.5595e-03, 1.9984e-04, 5.4301e-08,\n",
            "         8.6743e-01, 3.1674e-06, 2.5409e-03, 3.5211e-03, 2.4958e-03, 9.1187e-01,\n",
            "         7.5809e-09, 8.5626e-01, 7.1028e-04, 1.5556e-03, 1.7472e-08, 2.6093e-03,\n",
            "         3.5168e-06, 7.6948e-01, 4.2160e-05, 8.6416e-01, 5.0489e-06, 6.4820e-01,\n",
            "         7.8086e-01, 8.8083e-01, 9.6181e-01, 8.5521e-01, 8.5510e-01, 4.4033e-11,\n",
            "         4.9021e-06, 2.6593e-06, 7.0065e-06, 7.1108e-07, 6.8945e-07, 5.7249e-07,\n",
            "         1.6903e-09, 5.0495e-03, 3.1778e-08, 8.9257e-01, 2.9442e-06, 7.5169e-05,\n",
            "         2.9221e-04, 9.8838e-06, 9.4164e-01, 1.6256e-03, 1.7456e-03, 3.3523e-06,\n",
            "         3.0827e-06, 1.1593e-08, 3.9133e-06, 1.8513e-07, 3.6164e-11, 7.4429e-06,\n",
            "         4.6490e-05, 9.2785e-01, 1.1554e-05, 8.5183e-01, 8.9179e-01, 8.2503e-01,\n",
            "         2.1352e-03, 7.5498e-03, 1.0620e-09, 5.8324e-06, 1.1859e-05, 2.0407e-05,\n",
            "         6.5973e-06, 9.7952e-05, 2.6215e-09, 9.4666e-01, 3.0675e-08, 2.2404e-05,\n",
            "         8.9589e-01, 5.8911e-06, 1.9358e-04, 8.5177e-01, 7.1861e-06, 8.9639e-01,\n",
            "         4.4641e-06, 7.6121e-01, 4.2338e-03, 3.8341e-08, 9.0365e-01, 3.5043e-06,\n",
            "         5.1491e-09, 5.2497e-06, 5.2279e-05, 8.7027e-01, 1.3322e-06, 9.1435e-01,\n",
            "         8.5840e-01, 9.1017e-01, 4.9958e-04, 3.7914e-06, 8.9876e-01, 1.8799e-06,\n",
            "         2.5064e-06, 3.8177e-06, 5.4739e-06, 3.2702e-06, 8.9501e-01, 9.1272e-09,\n",
            "         1.2314e-03, 9.5723e-04, 2.7940e-09, 8.8815e-01, 2.1427e-03, 3.2227e-06,\n",
            "         8.0384e-07, 4.3866e-05, 3.9154e-03, 2.6151e-09, 3.4814e-06, 2.1479e-08,\n",
            "         1.8638e-05, 1.8875e-05, 9.1963e-01, 8.2680e-01, 4.8213e-11, 4.4920e-03,\n",
            "         9.5830e-01, 2.3911e-10, 3.8460e-09, 2.9025e-03, 1.0136e-05, 1.6479e-08,\n",
            "         5.4505e-06, 2.7503e-08, 6.2770e-04, 1.0574e-02, 2.2211e-03, 4.0750e-03,\n",
            "         2.6295e-09, 1.8560e-03, 2.4620e-03, 3.4094e-09, 9.4095e-01, 7.1893e-04,\n",
            "         9.3623e-01, 1.0368e-06, 6.2389e-08, 9.6223e-08, 1.3865e-10, 7.9562e-01,\n",
            "         3.5373e-06, 9.3647e-01, 2.9738e-07, 6.2157e-08, 3.9856e-10, 1.8716e-04,\n",
            "         1.7073e-07, 2.1942e-10, 3.8133e-06, 9.4400e-01, 7.2142e-07, 6.4511e-03,\n",
            "         7.1987e-05, 8.4737e-01, 2.5234e-08, 1.2401e-03, 8.3815e-01, 1.2169e-05,\n",
            "         8.9569e-01, 4.3117e-08, 8.6674e-06, 1.9441e-09, 3.8400e-08, 2.0313e-06,\n",
            "         4.1922e-06, 2.5025e-06, 1.4610e-07, 8.0604e-01, 4.0437e-06, 4.9765e-04,\n",
            "         7.7871e-01, 9.3915e-01, 7.5495e-01, 3.8248e-06, 6.5358e-06, 5.2694e-04,\n",
            "         4.3683e-05, 4.6649e-03, 7.7417e-01, 9.1728e-01, 1.8178e-05, 7.8679e-01,\n",
            "         8.5807e-01, 1.1859e-07, 3.5995e-08, 1.6633e-03, 1.0900e-05, 8.3179e-10,\n",
            "         7.9166e-04, 9.3143e-01, 8.7908e-09, 2.9531e-06, 9.0263e-01, 8.4116e-01,\n",
            "         8.8063e-01, 9.3827e-04, 2.9196e-06, 9.1106e-01, 3.9556e-06, 9.2329e-01,\n",
            "         8.0862e-04, 8.5338e-01, 5.3731e-08, 6.7276e-07, 1.2161e-07, 2.5132e-06,\n",
            "         1.8367e-03, 6.2890e-06, 3.8857e-04, 9.6615e-09, 8.1780e-01, 9.0011e-01,\n",
            "         2.4842e-06, 2.9273e-03, 9.3058e-13, 8.8060e-01, 8.7590e-01, 1.5940e-06,\n",
            "         1.5542e-06, 3.1127e-06, 1.1690e-07, 9.3379e-01, 2.2587e-06, 9.7365e-01,\n",
            "         7.3805e-08, 7.1950e-05, 3.5230e-08, 2.3575e-06, 2.4396e-07, 8.4411e-01,\n",
            "         6.7402e-01, 9.2698e-01, 7.2263e-10, 8.4579e-01, 3.0734e-07, 4.6843e-06,\n",
            "         8.5199e-11, 7.6333e-06, 8.3305e-03, 4.3892e-06, 4.3953e-04, 3.3119e-09,\n",
            "         8.2863e-01, 3.9562e-06, 8.7630e-01, 2.0600e-08, 6.3704e-04, 8.8374e-01,\n",
            "         4.0447e-06, 1.0301e-05, 1.1497e-06, 9.9722e-07, 8.9570e-01, 7.2682e-01,\n",
            "         4.5104e-06, 2.4000e-07, 2.1334e-08, 9.5917e-01, 8.1725e-01, 4.5422e-03,\n",
            "         2.5606e-07, 2.0725e-06, 5.6317e-08, 8.2148e-01, 8.1316e-04, 1.9270e-03,\n",
            "         8.8403e-01, 2.8184e-03, 1.7181e-03, 9.0893e-01, 2.1372e-06, 1.2743e-08,\n",
            "         4.3734e-03, 2.6191e-09, 2.1211e-09, 2.8176e-06, 7.4888e-08, 9.0524e-01,\n",
            "         1.0808e-03, 2.6728e-06, 1.1113e-03, 9.1127e-01, 7.1645e-06, 3.1372e-03,\n",
            "         1.3411e-06, 8.3623e-01, 3.5586e-11, 9.7516e-01, 1.2469e-08, 3.7045e-03,\n",
            "         6.0296e-06, 8.8324e-01, 7.2637e-04, 3.5853e-03, 3.2840e-04, 6.3286e-06,\n",
            "         1.2217e-09, 8.6630e-01, 9.8312e-08, 7.8138e-03, 5.0750e-09, 7.7929e-08,\n",
            "         8.7220e-01, 1.8867e-03, 7.9231e-01, 3.1465e-10, 9.7522e-01, 1.7020e-06,\n",
            "         6.4380e-03, 1.6212e-08, 9.0853e-01, 2.2384e-07, 1.2415e-05, 8.2934e-07,\n",
            "         7.9491e-01, 1.6606e-07, 2.3275e-08, 3.3862e-04, 7.1923e-10, 6.6869e-08,\n",
            "         4.6640e-08, 1.7368e-05, 1.4262e-03, 7.2696e-05, 1.1960e-10, 3.2558e-08,\n",
            "         3.8171e-08, 6.9310e-03, 2.6522e-03, 1.0336e-05, 3.7819e-04, 5.9367e-01,\n",
            "         9.4581e-01, 1.3509e-09, 4.2018e-06, 4.1800e-09, 8.0957e-06, 8.9940e-01,\n",
            "         2.2815e-05, 1.5233e-06, 9.7723e-01, 9.2049e-01, 8.1790e-09, 1.0861e-06,\n",
            "         2.8124e-07, 3.5654e-03, 8.4363e-01, 8.1377e-01, 2.6940e-09, 4.1025e-03,\n",
            "         8.2554e-01, 4.8573e-09, 2.2938e-05, 1.2023e-03, 1.1488e-03, 8.7718e-01,\n",
            "         6.5678e-07, 8.8232e-03, 5.2978e-04, 3.8667e-11, 1.4447e-06, 8.8891e-01,\n",
            "         9.2544e-01, 9.4449e-01, 2.4659e-03, 1.0364e-05, 8.7632e-01, 8.3700e-01,\n",
            "         1.8650e-05, 1.2791e-04, 7.3286e-09, 8.8007e-09, 1.1023e-06, 7.2981e-06,\n",
            "         1.9985e-08, 8.2950e-01, 2.8004e-03, 2.4000e-06, 1.0326e-03, 8.5136e-01,\n",
            "         5.3824e-04, 1.0804e-04, 8.8014e-06, 6.1526e-06, 2.2273e-06, 1.1093e-08,\n",
            "         9.4502e-10, 5.5216e-09, 3.2292e-06, 7.6542e-04, 1.3391e-03, 1.9162e-06,\n",
            "         4.1809e-04, 7.3142e-06, 1.1381e-05, 6.9601e-04, 9.2753e-01, 6.0191e-06,\n",
            "         2.9486e-09, 4.1674e-04, 2.9967e-06, 5.7203e-07, 2.5120e-06, 6.1158e-03,\n",
            "         8.0851e-07, 1.1987e-04, 6.1886e-03, 5.7189e-07, 4.3255e-08, 7.5425e-01,\n",
            "         1.9186e-03, 1.7107e-03, 2.4399e-10, 1.7547e-03, 4.9762e-03, 6.7337e-09,\n",
            "         9.0921e-01, 1.2906e-09, 2.8099e-06, 1.8759e-10, 8.8546e-01, 4.2461e-06,\n",
            "         3.1338e-09, 8.6003e-06, 1.7097e-06, 6.1777e-04, 1.0958e-09, 1.0387e-03,\n",
            "         9.7703e-09, 1.5174e-03, 1.1594e-06, 8.6891e-01, 1.0502e-05, 4.5932e-03,\n",
            "         1.0322e-07, 1.3353e-04, 4.1646e-10, 1.7543e-09, 4.5787e-04, 7.8020e-01,\n",
            "         7.9772e-01, 1.1847e-08, 8.7960e-01, 1.0815e-02, 7.9128e-11, 8.4201e-06,\n",
            "         5.7684e-04, 7.6233e-07, 3.5716e-08, 3.4503e-06, 9.6538e-06, 8.3960e-05,\n",
            "         4.1095e-03, 9.2888e-01, 9.1607e-01, 2.6038e-03, 9.3771e-01, 1.6516e-06,\n",
            "         1.3139e-07, 1.5115e-06, 3.8918e-10, 3.0211e-06, 9.3121e-01, 1.0196e-03,\n",
            "         9.1233e-01, 1.3844e-09, 1.5052e-06, 8.4552e-01, 1.9294e-03, 7.6199e-06,\n",
            "         3.3612e-09, 1.4603e-06, 2.6265e-09, 1.4356e-03, 8.9879e-01, 2.7778e-08,\n",
            "         2.6819e-03, 1.8140e-03, 3.8594e-09, 1.6308e-03, 9.4159e-01, 1.1230e-05,\n",
            "         9.6442e-01, 8.0785e-10, 3.7854e-08, 5.9142e-09, 7.8590e-08, 9.3450e-01,\n",
            "         5.2979e-08, 7.1020e-09, 3.3537e-04, 7.4871e-01, 9.3375e-01, 3.0066e-06,\n",
            "         9.3431e-01, 7.7962e-10, 6.6034e-08, 1.2082e-09, 1.2759e-08, 8.8636e-01,\n",
            "         1.6307e-06, 2.3983e-03, 4.5464e-06, 8.9653e-01, 1.6392e-06, 9.7207e-09,\n",
            "         1.5078e-03, 7.0788e-07, 5.8150e-07, 7.9556e-03, 2.1865e-06, 3.3821e-06,\n",
            "         9.3542e-01, 8.1658e-04, 8.5590e-01, 6.1359e-11, 1.5203e-05, 2.1872e-09,\n",
            "         8.2660e-01, 3.8690e-10, 7.7160e-01, 4.9597e-03, 7.8499e-04, 7.7084e-08,\n",
            "         3.4984e-09, 7.4897e-09, 3.5012e-03, 9.0025e-01, 8.6210e-01, 7.1389e-04,\n",
            "         8.3810e-10, 1.2877e-04, 2.6826e-06, 1.5921e-05, 1.0298e-08, 8.6747e-01,\n",
            "         9.0118e-01, 8.8600e-01, 1.1698e-06, 8.8756e-01, 1.6948e-06, 9.1914e-01,\n",
            "         1.3822e-05, 9.5795e-01, 6.8920e-03, 9.6644e-08, 1.2554e-05, 3.5967e-06,\n",
            "         5.2622e-08, 2.6665e-05, 2.4666e-03, 3.1004e-06, 3.0612e-06, 9.4660e-01,\n",
            "         9.3494e-03, 8.0377e-04, 6.8497e-03, 7.8530e-04, 1.6023e-09, 1.2501e-06,\n",
            "         9.4987e-06, 5.9820e-06, 8.9283e-01, 7.1895e-01, 9.1094e-01, 1.6731e-03,\n",
            "         7.7212e-01, 1.0139e-03, 3.8134e-09, 8.2290e-01, 1.0059e-02, 3.1844e-08,\n",
            "         3.3173e-08, 8.9713e-01, 8.1593e-01, 6.4379e-01, 7.3686e-06, 4.3004e-03,\n",
            "         9.4034e-01, 1.4274e-03, 3.4829e-03, 1.6807e-03, 2.8119e-06, 3.6575e-07,\n",
            "         9.1834e-01, 2.3061e-04, 7.8803e-01, 6.7824e-03, 7.7335e-01, 8.8858e-01,\n",
            "         1.2090e-08, 6.2418e-06, 3.3721e-06, 9.2793e-04, 1.0790e-05, 6.4819e-08,\n",
            "         8.9779e-06, 5.7111e-09, 6.2308e-06, 9.1249e-08, 3.1313e-06, 5.7791e-06,\n",
            "         1.8671e-06, 5.2167e-08, 7.9869e-05, 4.2865e-06, 1.5356e-05, 2.1235e-06,\n",
            "         7.9418e-01, 9.1443e-01, 8.9236e-01, 9.1317e-01, 6.4735e-01, 9.5465e-01,\n",
            "         8.6805e-01, 2.9415e-06, 4.6988e-06, 5.7152e-10, 1.2291e-06, 5.1356e-04,\n",
            "         1.1891e-04, 9.4109e-11, 5.5485e-03, 1.5144e-03, 4.0746e-06, 4.5769e-06,\n",
            "         8.6013e-01, 5.1804e-06, 1.4759e-03, 2.0466e-10, 5.1208e-05, 2.6184e-03,\n",
            "         3.2841e-11, 8.6455e-06, 3.0490e-03, 6.9693e-09, 4.3909e-07, 3.7658e-06,\n",
            "         4.6514e-06, 3.6567e-05, 3.6966e-06, 2.0511e-04, 7.9901e-06, 9.4220e-01,\n",
            "         2.7826e-04, 1.8877e-06, 8.0269e-08, 1.7614e-06, 9.4424e-04, 8.5081e-01,\n",
            "         2.8809e-08, 7.8393e-01, 4.1969e-03, 3.3491e-07, 8.8061e-01, 1.1370e-02,\n",
            "         1.3088e-03, 6.7986e-06, 4.5082e-04, 9.3747e-01, 4.5551e-03, 9.3192e-01,\n",
            "         9.7115e-01, 8.3111e-01, 2.2812e-03, 3.6017e-06]], device='cuda:0',\n",
            "       grad_fn=<SigmoidBackward>)\n",
            "0.31069335\n",
            "[*] Getting the embeddings and visualizing t-SNE...\n",
            "'dngr_pipeline'  176231.42 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDDFUlB0RjTA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d04940a0-3c1e-49bf-ad1d-0599d31bf163"
      },
      "source": [
        "embeddings_targets, _, _ = dngr_pipeline(pp_net, N_rand_targets, [500, 200, 100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: RuntimeWarning: divide by zero encountered in log\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "      DropoutNoise-1                 [-1, 1000]               0\n",
            "            Linear-2                  [-1, 500]         500,500\n",
            "           Sigmoid-3                  [-1, 500]               0\n",
            "        BasicBlock-4                  [-1, 500]               0\n",
            "            Linear-5                  [-1, 200]         100,200\n",
            "           Sigmoid-6                  [-1, 200]               0\n",
            "        BasicBlock-7                  [-1, 200]               0\n",
            "            Linear-8                  [-1, 100]          20,100\n",
            "           Sigmoid-9                  [-1, 100]               0\n",
            "       BasicBlock-10                  [-1, 100]               0\n",
            "           Linear-11                  [-1, 200]          20,200\n",
            "          Sigmoid-12                  [-1, 200]               0\n",
            "       BasicBlock-13                  [-1, 200]               0\n",
            "           Linear-14                  [-1, 500]         100,500\n",
            "          Sigmoid-15                  [-1, 500]               0\n",
            "       BasicBlock-16                  [-1, 500]               0\n",
            "           Linear-17                 [-1, 1000]         501,000\n",
            "          Sigmoid-18                 [-1, 1000]               0\n",
            "       BasicBlock-19                 [-1, 1000]               0\n",
            "================================================================\n",
            "Total params: 1,242,500\n",
            "Trainable params: 1,242,500\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.06\n",
            "Params size (MB): 4.74\n",
            "Estimated Total Size (MB): 4.81\n",
            "----------------------------------------------------------------\n",
            "[1,  1000] loss: 0.348\n",
            "[2,  1000] loss: 0.286\n",
            "[3,  1000] loss: 0.261\n",
            "[4,  1000] loss: 0.247\n",
            "[5,  1000] loss: 0.245\n",
            "[6,  1000] loss: 0.244\n",
            "[7,  1000] loss: 0.247\n",
            "[8,  1000] loss: 0.244\n",
            "[9,  1000] loss: 0.222\n",
            "[10,  1000] loss: 0.208\n",
            "[11,  1000] loss: 0.196\n",
            "[12,  1000] loss: 0.188\n",
            "[13,  1000] loss: 0.188\n",
            "[14,  1000] loss: 0.187\n",
            "[15,  1000] loss: 0.187\n",
            "[16,  1000] loss: 0.187\n",
            "[17,  1000] loss: 0.187\n",
            "[18,  1000] loss: 0.187\n",
            "[19,  1000] loss: 0.187\n",
            "[20,  1000] loss: 0.187\n",
            "[21,  1000] loss: 0.187\n",
            "[22,  1000] loss: 0.187\n",
            "[23,  1000] loss: 0.187\n",
            "[24,  1000] loss: 0.187\n",
            "[25,  1000] loss: 0.187\n",
            "[26,  1000] loss: 0.187\n",
            "[27,  1000] loss: 0.187\n",
            "[28,  1000] loss: 0.187\n",
            "[29,  1000] loss: 0.187\n",
            "[30,  1000] loss: 0.187\n",
            "[31,  1000] loss: 0.187\n",
            "[32,  1000] loss: 0.187\n",
            "[33,  1000] loss: 0.187\n",
            "[34,  1000] loss: 0.187\n",
            "[35,  1000] loss: 0.187\n",
            "[36,  1000] loss: 0.187\n",
            "[37,  1000] loss: 0.187\n",
            "[38,  1000] loss: 0.187\n",
            "[39,  1000] loss: 0.187\n",
            "[40,  1000] loss: 0.187\n",
            "[41,  1000] loss: 0.187\n",
            "[42,  1000] loss: 0.187\n",
            "[43,  1000] loss: 0.187\n",
            "[44,  1000] loss: 0.187\n",
            "[45,  1000] loss: 0.187\n",
            "[46,  1000] loss: 0.187\n",
            "[47,  1000] loss: 0.187\n",
            "[48,  1000] loss: 0.187\n",
            "[49,  1000] loss: 0.187\n",
            "[50,  1000] loss: 0.187\n",
            "[51,  1000] loss: 0.187\n",
            "[52,  1000] loss: 0.187\n",
            "[53,  1000] loss: 0.187\n",
            "[54,  1000] loss: 0.187\n",
            "[55,  1000] loss: 0.187\n",
            "[56,  1000] loss: 0.187\n",
            "[57,  1000] loss: 0.187\n",
            "[58,  1000] loss: 0.187\n",
            "[59,  1000] loss: 0.187\n",
            "[60,  1000] loss: 0.187\n",
            "[61,  1000] loss: 0.187\n",
            "[62,  1000] loss: 0.187\n",
            "[63,  1000] loss: 0.187\n",
            "[64,  1000] loss: 0.187\n",
            "[65,  1000] loss: 0.187\n",
            "[66,  1000] loss: 0.187\n",
            "[67,  1000] loss: 0.187\n",
            "[68,  1000] loss: 0.187\n",
            "[69,  1000] loss: 0.187\n",
            "[70,  1000] loss: 0.187\n",
            "[71,  1000] loss: 0.187\n",
            "[72,  1000] loss: 0.187\n",
            "[73,  1000] loss: 0.187\n",
            "[74,  1000] loss: 0.187\n",
            "[75,  1000] loss: 0.187\n",
            "[76,  1000] loss: 0.187\n",
            "[77,  1000] loss: 0.187\n",
            "[78,  1000] loss: 0.187\n",
            "[79,  1000] loss: 0.187\n",
            "[80,  1000] loss: 0.187\n",
            "[81,  1000] loss: 0.187\n",
            "[82,  1000] loss: 0.187\n",
            "[83,  1000] loss: 0.187\n",
            "[84,  1000] loss: 0.187\n",
            "[85,  1000] loss: 0.187\n",
            "[86,  1000] loss: 0.187\n",
            "[87,  1000] loss: 0.187\n",
            "[88,  1000] loss: 0.187\n",
            "[89,  1000] loss: 0.187\n",
            "[90,  1000] loss: 0.187\n",
            "[91,  1000] loss: 0.187\n",
            "[92,  1000] loss: 0.187\n",
            "[93,  1000] loss: 0.187\n",
            "[94,  1000] loss: 0.187\n",
            "[95,  1000] loss: 0.187\n",
            "[96,  1000] loss: 0.187\n",
            "[97,  1000] loss: 0.187\n",
            "[98,  1000] loss: 0.187\n",
            "[99,  1000] loss: 0.187\n",
            "[100,  1000] loss: 0.187\n",
            "Finished Training\n",
            "[*] Visualizing an example's output...\n",
            "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.5072, 0.0000, 0.4736, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0308, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5492,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         3.4316, 0.0000, 0.0000, 0.0000, 0.0000, 0.5506, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.9050, 0.1044, 0.0000, 0.0000, 0.0000, 0.0000, 0.6952,\n",
            "         0.0000, 0.0000, 0.0000, 0.2839, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 3.4053, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         3.3524, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.3591, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6054, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.3089, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.3117, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.4860, 3.1124, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 3.3533, 0.6270, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.7272, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 3.3548, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.6807, 0.0000, 0.0000, 0.0000, 0.0000, 3.3692, 0.0000, 0.0000, 3.4049,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.6255, 0.0000, 0.7476, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 3.3659, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4205, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5705, 0.0000, 0.0000,\n",
            "         0.0000, 3.5936, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.6131, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 3.3438, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.2855, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.6762, 0.0000, 0.0000, 0.0000, 0.3369, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.8622, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 3.2509, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.5409, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.4212, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4615,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4342, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 3.3033, 0.7244, 0.0000, 0.0000, 0.6167, 0.0000, 0.4438, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.2928, 0.0000, 0.0000, 0.0000, 0.0000, 0.6875, 3.1627, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.7272, 0.0000, 0.0000, 0.5062, 0.8623,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9076,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         3.2461, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.2419, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.6636, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8622, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7428, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3556, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.4226, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.7051, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 3.3410, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.7035, 0.0000, 0.0000, 0.0000, 0.0000, 0.3947,\n",
            "         0.0000, 0.5782, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 3.4528, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.3008, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.5094, 0.0000, 0.0000, 0.4328, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5953, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 3.3423, 0.0000, 0.6458, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 3.2180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.7007, 0.1111, 0.0000, 0.0000, 0.0000, 0.4710, 0.0000, 0.0000, 3.3782,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.3712, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.2207, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5661, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.2238, 0.0000,\n",
            "         0.0000, 0.0000, 0.4341, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8183, 0.0000,\n",
            "         0.0000, 3.5863, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         3.3367, 0.5139, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.4818, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.5706, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.4592, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.6118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.2123,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6952, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.5778, 0.0000, 0.0000, 3.2906, 0.0000, 0.0000, 0.0000,\n",
            "         0.4366, 0.0000, 0.0000, 0.5107, 0.0000, 0.6631, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.8021, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.4214]], device='cuda:0')\n",
            "tensor([[1.0000e+00, 2.2518e-04, 3.2909e-08, 6.1972e-08, 2.0235e-04, 2.2764e-04,\n",
            "         1.1102e-06, 2.4816e-07, 5.4410e-07, 3.1634e-06, 1.4799e-07, 8.2692e-08,\n",
            "         1.0000e+00, 2.0329e-04, 1.0000e+00, 3.8546e-06, 4.9710e-07, 5.6600e-08,\n",
            "         4.6727e-06, 6.0429e-08, 4.1015e-08, 8.5902e-08, 1.9246e-04, 2.0718e-04,\n",
            "         2.1148e-04, 3.4572e-07, 1.9967e-04, 2.0199e-04, 6.2611e-06, 5.5874e-07,\n",
            "         1.1110e-06, 5.8426e-08, 4.8038e-06, 1.8652e-04, 8.4999e-07, 4.8064e-06,\n",
            "         2.2606e-04, 2.1330e-06, 4.2762e-08, 5.6971e-08, 4.6522e-08, 3.9973e-07,\n",
            "         7.9971e-07, 5.4412e-08, 4.9301e-08, 2.3683e-06, 3.7639e-07, 3.2186e-07,\n",
            "         7.6808e-08, 1.8325e-04, 1.0000e+00, 4.9223e-06, 5.5155e-08, 2.2703e-04,\n",
            "         5.4558e-08, 6.4428e-07, 9.9925e-08, 6.2867e-08, 9.9083e-07, 8.8763e-07,\n",
            "         3.8280e-08, 2.2910e-06, 6.8983e-07, 2.1357e-04, 2.2078e-04, 3.8027e-06,\n",
            "         1.8206e-04, 3.2928e-06, 1.8416e-04, 2.1027e-04, 4.7264e-08, 1.6064e-07,\n",
            "         6.8310e-07, 5.7650e-07, 5.7598e-06, 2.0729e-04, 2.1042e-04, 1.1634e-07,\n",
            "         7.7754e-08, 1.0254e-06, 1.0000e+00, 1.1128e-06, 7.4186e-06, 2.0388e-04,\n",
            "         4.7056e-08, 7.6131e-08, 1.1401e-06, 5.3116e-07, 2.6505e-08, 4.7410e-06,\n",
            "         8.3482e-07, 2.0092e-04, 5.9127e-08, 4.8074e-08, 5.7259e-08, 5.7400e-06,\n",
            "         6.0874e-07, 8.8501e-07, 2.2272e-04, 1.0000e+00, 4.6468e-07, 3.4525e-06,\n",
            "         4.5213e-08, 1.8598e-04, 1.0000e+00, 6.6248e-07, 2.2545e-04, 4.5794e-06,\n",
            "         9.6369e-07, 3.6919e-06, 1.0000e+00, 1.0000e+00, 3.8459e-06, 2.0506e-06,\n",
            "         5.3013e-08, 2.0815e-04, 1.0000e+00, 1.0630e-06, 5.4492e-08, 2.3915e-07,\n",
            "         1.0000e+00, 1.8832e-04, 2.9228e-06, 5.4943e-08, 2.1975e-04, 1.8566e-04,\n",
            "         9.4779e-07, 2.2112e-04, 1.0000e+00, 3.9205e-08, 4.9276e-08, 1.0994e-06,\n",
            "         3.5922e-08, 2.0242e-04, 8.2977e-08, 5.1031e-08, 2.5874e-07, 7.0888e-07,\n",
            "         5.2156e-08, 1.1334e-06, 2.2902e-07, 2.8073e-07, 1.8814e-04, 2.1804e-07,\n",
            "         1.0000e+00, 1.9599e-04, 6.0223e-08, 2.0623e-04, 6.3393e-08, 7.6679e-07,\n",
            "         1.2102e-06, 6.1606e-08, 1.8275e-04, 5.5149e-08, 2.6913e-08, 5.5190e-08,\n",
            "         2.1510e-04, 1.1237e-06, 1.0000e+00, 6.0746e-08, 5.6089e-08, 1.2803e-06,\n",
            "         2.0317e-04, 6.5841e-08, 2.0570e-07, 6.1033e-08, 5.8279e-07, 1.0000e+00,\n",
            "         4.9977e-07, 4.2763e-07, 1.8606e-04, 2.3128e-04, 1.8196e-04, 5.9755e-08,\n",
            "         2.1227e-04, 6.8729e-08, 5.4754e-08, 1.9271e-04, 9.1970e-08, 1.8602e-07,\n",
            "         6.8608e-06, 5.0796e-08, 2.2481e-04, 4.6788e-08, 1.0094e-06, 4.4296e-08,\n",
            "         2.2041e-04, 2.2600e-04, 1.0324e-07, 2.0339e-04, 2.0280e-04, 7.1768e-07,\n",
            "         1.8539e-04, 1.0703e-06, 8.4279e-08, 6.0356e-08, 1.0000e+00, 2.4116e-07,\n",
            "         2.0390e-06, 6.1422e-08, 2.6720e-07, 2.6159e-07, 2.2501e-04, 3.2279e-06,\n",
            "         1.0000e+00, 3.0550e-07, 8.4903e-06, 5.1261e-08, 2.2705e-04, 6.6518e-08,\n",
            "         1.9647e-04, 5.4797e-06, 9.4165e-07, 2.1073e-04, 5.5585e-08, 2.5364e-06,\n",
            "         1.0000e+00, 1.0000e+00, 5.1450e-08, 2.2275e-07, 3.0810e-07, 2.1420e-07,\n",
            "         1.9520e-08, 5.4082e-07, 3.1218e-06, 5.8035e-07, 1.0000e+00, 1.0000e+00,\n",
            "         4.8610e-06, 2.2339e-04, 5.4251e-07, 8.2007e-08, 8.7718e-07, 3.7653e-08,\n",
            "         2.4339e-07, 2.1244e-04, 1.7534e-06, 1.9817e-06, 1.8551e-04, 2.1346e-04,\n",
            "         7.0368e-08, 3.5318e-08, 9.6764e-08, 4.1279e-06, 8.3330e-08, 3.5773e-06,\n",
            "         2.1585e-08, 7.1437e-08, 1.0000e+00, 7.3496e-08, 1.0395e-06, 3.7224e-06,\n",
            "         6.8922e-07, 2.6459e-07, 2.0543e-06, 2.5974e-07, 1.0000e+00, 2.3123e-06,\n",
            "         1.2667e-06, 6.4554e-08, 4.1751e-07, 1.0000e+00, 5.4876e-06, 4.7819e-06,\n",
            "         2.2516e-04, 2.4795e-07, 1.0000e+00, 4.5724e-06, 1.7529e-06, 1.0000e+00,\n",
            "         1.2973e-06, 5.7703e-07, 4.6902e-07, 3.2629e-08, 1.0000e+00, 2.8173e-08,\n",
            "         1.0000e+00, 6.2408e-08, 8.7207e-08, 2.1497e-07, 6.6212e-07, 1.0000e+00,\n",
            "         6.7226e-08, 1.8201e-04, 2.1312e-04, 8.8932e-07, 3.5515e-08, 3.3202e-08,\n",
            "         2.1146e-04, 3.1244e-07, 2.2288e-08, 4.0646e-08, 2.6235e-08, 4.5831e-08,\n",
            "         1.0000e+00, 6.2757e-08, 1.7819e-06, 4.8162e-08, 4.2595e-06, 9.0982e-08,\n",
            "         8.3157e-08, 4.3139e-08, 8.4264e-08, 1.0000e+00, 4.2079e-08, 1.6467e-06,\n",
            "         2.2961e-04, 1.0000e+00, 5.0401e-07, 5.5336e-06, 6.2981e-08, 5.3722e-07,\n",
            "         7.1339e-08, 1.6009e-06, 5.8609e-07, 2.2728e-04, 4.3783e-06, 3.5717e-08,\n",
            "         2.0711e-07, 2.1240e-04, 1.8073e-04, 1.8818e-04, 7.5237e-08, 3.8874e-06,\n",
            "         5.3258e-08, 1.0000e+00, 1.1438e-07, 2.3017e-04, 5.3473e-07, 5.9084e-08,\n",
            "         2.0858e-04, 4.4149e-06, 2.0826e-04, 2.1186e-04, 2.2276e-04, 1.0000e+00,\n",
            "         5.6966e-07, 2.0090e-07, 6.4060e-08, 2.8162e-07, 3.0322e-07, 4.8782e-07,\n",
            "         1.8949e-04, 1.8253e-06, 2.0793e-04, 5.5511e-08, 4.9409e-07, 4.7365e-08,\n",
            "         5.2150e-06, 7.1823e-08, 6.3692e-08, 5.4099e-06, 2.0018e-07, 1.0000e+00,\n",
            "         3.2949e-07, 6.7873e-08, 2.1836e-04, 7.4133e-07, 4.7379e-08, 4.1781e-06,\n",
            "         2.1147e-04, 6.7796e-08, 5.0596e-07, 6.7346e-07, 7.9024e-07, 6.9983e-08,\n",
            "         3.4201e-06, 2.0935e-04, 1.3431e-06, 7.2831e-08, 1.0000e+00, 3.8480e-08,\n",
            "         5.7229e-08, 6.0562e-07, 1.0000e+00, 1.8231e-04, 2.0423e-04, 2.1389e-04,\n",
            "         2.6409e-07, 2.6969e-07, 3.0926e-07, 3.3743e-06, 2.3154e-04, 1.9896e-08,\n",
            "         5.8394e-08, 2.1803e-04, 5.9806e-08, 3.8758e-06, 7.1111e-08, 3.5785e-06,\n",
            "         1.8544e-04, 1.1438e-06, 6.3140e-08, 7.3686e-08, 3.7833e-07, 3.8202e-08,\n",
            "         2.2221e-04, 3.3099e-06, 1.9023e-04, 3.5960e-07, 1.8765e-04, 1.3172e-06,\n",
            "         6.0973e-08, 3.0007e-08, 1.1094e-07, 6.2280e-08, 4.7835e-07, 7.2307e-08,\n",
            "         1.0000e+00, 2.1236e-04, 3.2432e-08, 2.0043e-06, 7.0595e-08, 5.3307e-08,\n",
            "         9.1114e-07, 7.6879e-06, 9.6308e-08, 5.6690e-08, 2.1577e-04, 7.5447e-08,\n",
            "         2.6105e-07, 5.7654e-08, 6.8980e-08, 1.1406e-06, 1.8459e-04, 1.9448e-06,\n",
            "         1.0000e+00, 1.0329e-06, 6.6336e-08, 2.6944e-07, 5.5821e-08, 3.3787e-06,\n",
            "         8.6998e-07, 1.3013e-07, 4.9002e-08, 2.2837e-04, 2.6619e-07, 3.9809e-06,\n",
            "         2.1326e-06, 5.9894e-07, 2.0188e-07, 5.3876e-07, 4.6785e-08, 2.6908e-07,\n",
            "         7.6076e-08, 1.0000e+00, 1.4119e-06, 4.2207e-08, 8.5464e-07, 4.3096e-08,\n",
            "         7.2539e-07, 2.1476e-07, 6.0191e-08, 1.2217e-06, 2.9550e-07, 1.0070e-06,\n",
            "         8.2432e-06, 1.8706e-04, 1.5443e-06, 7.3285e-07, 4.5628e-08, 2.2156e-04,\n",
            "         5.7775e-06, 1.9115e-04, 2.2624e-04, 7.9777e-07, 1.6226e-07, 4.7518e-08,\n",
            "         1.8332e-07, 1.0000e+00, 2.1540e-04, 7.0419e-07, 1.8290e-04, 1.9788e-04,\n",
            "         6.0441e-08, 6.1933e-08, 8.4536e-08, 2.8623e-06, 2.0299e-04, 7.3125e-08,\n",
            "         7.6902e-08, 2.4206e-08, 2.9209e-07, 2.1301e-04, 2.6046e-07, 1.0000e+00,\n",
            "         2.1886e-04, 5.2339e-07, 1.8586e-04, 3.7895e-06, 1.0371e-07, 7.5319e-08,\n",
            "         1.8406e-04, 1.0000e+00, 1.8443e-04, 6.0892e-07, 2.2996e-04, 2.1123e-04,\n",
            "         5.6092e-07, 1.3610e-07, 2.6892e-07, 2.5463e-06, 4.8509e-08, 7.0049e-07,\n",
            "         1.9007e-04, 1.0000e+00, 1.0000e+00, 2.1924e-04, 5.6600e-07, 1.0000e+00,\n",
            "         1.9583e-07, 1.0000e+00, 5.6587e-06, 1.0368e-06, 1.2444e-07, 1.8189e-04,\n",
            "         4.0228e-07, 7.5789e-08, 4.5725e-06, 7.9859e-08, 2.5236e-07, 2.2008e-07,\n",
            "         1.0000e+00, 1.8987e-04, 1.3991e-07, 6.0419e-06, 6.9543e-08, 1.0000e+00,\n",
            "         1.0000e+00, 1.8590e-04, 5.2007e-06, 2.0665e-04, 7.7534e-08, 4.8406e-06,\n",
            "         2.4464e-07, 8.5192e-07, 1.8319e-04, 3.7682e-06, 7.6429e-07, 2.0921e-04,\n",
            "         8.1750e-08, 2.6671e-07, 1.4703e-06, 2.0595e-04, 3.5970e-08, 1.5369e-06,\n",
            "         4.6001e-06, 1.8850e-04, 9.7272e-07, 6.5110e-07, 6.2006e-08, 3.6486e-08,\n",
            "         2.0800e-04, 2.1757e-04, 4.4327e-08, 4.6830e-07, 4.0546e-08, 6.6245e-08,\n",
            "         9.7522e-08, 2.8192e-07, 4.5265e-08, 8.2733e-08, 5.8327e-07, 5.6689e-08,\n",
            "         3.0017e-07, 1.8528e-04, 1.0652e-06, 1.1887e-07, 4.9985e-08, 2.2257e-04,\n",
            "         1.8407e-04, 1.0000e+00, 2.2765e-04, 2.2226e-07, 1.0000e+00, 1.0000e+00,\n",
            "         1.8186e-04, 6.5776e-08, 8.2627e-07, 4.4507e-08, 2.3450e-07, 1.9930e-04,\n",
            "         3.0053e-08, 1.3323e-06, 1.0000e+00, 7.5609e-08, 2.1456e-04, 5.4440e-07,\n",
            "         7.4316e-07, 4.2030e-08, 1.4475e-07, 2.7642e-07, 3.6531e-07, 6.3446e-08,\n",
            "         1.0000e+00, 2.0648e-04, 3.4314e-08, 3.9815e-08, 1.8495e-04, 5.7113e-08,\n",
            "         7.7184e-08, 1.0000e+00, 5.0129e-08, 2.6044e-07, 6.2611e-06, 6.1139e-06,\n",
            "         4.7576e-08, 5.2352e-06, 6.4736e-08, 1.9144e-04, 1.3518e-06, 2.3291e-08,\n",
            "         1.0000e+00, 2.2606e-07, 1.0105e-06, 7.4754e-07, 2.0057e-07, 6.0003e-07,\n",
            "         1.9428e-07, 5.6097e-06, 2.0144e-04, 5.9005e-08, 3.2725e-06, 5.9039e-08,\n",
            "         8.2766e-08, 9.5560e-07, 1.0000e+00, 2.5629e-06, 2.0223e-04, 1.8319e-04,\n",
            "         2.7172e-07, 2.3609e-07, 4.1960e-06, 5.9940e-06, 2.2363e-04, 1.0000e+00,\n",
            "         1.4094e-06, 6.9588e-08, 4.8583e-06, 3.1758e-07, 2.4183e-07, 5.7802e-08,\n",
            "         3.2627e-08, 9.8131e-07, 3.5569e-08, 6.7161e-08, 4.1913e-08, 6.2444e-06,\n",
            "         2.4634e-07, 7.1920e-08, 6.1429e-07, 4.2916e-08, 2.9998e-07, 1.0000e+00,\n",
            "         1.3416e-07, 8.0757e-08, 2.1955e-06, 2.0310e-04, 1.8799e-04, 2.2027e-04,\n",
            "         7.0943e-08, 2.1127e-04, 4.8674e-08, 1.0000e+00, 5.0759e-06, 6.7463e-08,\n",
            "         2.3314e-04, 2.4101e-06, 1.0000e+00, 1.9405e-04, 8.5077e-08, 6.0314e-07,\n",
            "         3.5497e-08, 1.7691e-07, 4.7938e-06, 6.0813e-06, 2.4403e-07, 6.1792e-06,\n",
            "         4.0310e-07, 1.0000e+00, 1.9056e-04, 2.9882e-06, 5.6277e-07, 1.5506e-06,\n",
            "         1.1647e-06, 5.6825e-06, 4.2083e-08, 1.0000e+00, 1.8433e-04, 2.9597e-08,\n",
            "         9.4272e-08, 1.8623e-04, 1.0000e+00, 1.8567e-04, 1.0000e+00, 4.0585e-08,\n",
            "         2.1365e-04, 2.6395e-07, 6.0871e-06, 3.0441e-07, 4.1646e-06, 1.1060e-06,\n",
            "         2.1291e-04, 1.8339e-04, 3.3911e-07, 2.0886e-04, 2.0630e-04, 6.7695e-07,\n",
            "         1.8125e-04, 2.8318e-07, 1.0707e-07, 5.2864e-06, 5.4855e-07, 1.0000e+00,\n",
            "         5.3638e-06, 1.5952e-06, 9.2783e-07, 1.5329e-06, 1.7359e-08, 5.9673e-07,\n",
            "         1.8616e-04, 1.9690e-04, 1.1661e-06, 6.8540e-08, 1.0000e+00, 5.9331e-08,\n",
            "         2.1687e-07, 6.4649e-08, 5.7833e-08, 1.0000e+00, 3.9637e-06, 8.3056e-07,\n",
            "         1.0000e+00, 1.8334e-04, 6.7411e-08, 5.2060e-08, 2.7459e-07, 2.4341e-07,\n",
            "         2.3121e-08, 2.0445e-04, 3.0010e-07, 1.8995e-04, 9.0574e-07, 5.5581e-08,\n",
            "         7.3243e-08, 8.6876e-07, 1.9710e-06, 2.3278e-06, 7.6742e-08, 3.4756e-07,\n",
            "         2.1686e-04, 1.0553e-06, 1.2469e-07, 2.2432e-04, 6.0527e-08, 2.1002e-04,\n",
            "         3.3110e-07, 4.0370e-06, 6.5472e-08, 2.8947e-07, 2.2636e-04, 5.7268e-07,\n",
            "         1.0000e+00, 2.2276e-04, 1.9816e-04, 5.4291e-08, 3.7070e-07, 1.0000e+00,\n",
            "         5.7228e-07, 1.0000e+00, 6.1329e-08, 1.1246e-06, 8.7532e-08, 4.5325e-06,\n",
            "         5.1777e-08, 2.1667e-07, 4.8755e-08, 5.7039e-08, 1.8407e-07, 7.2873e-08,\n",
            "         1.1016e-06, 1.1071e-07, 7.3598e-08, 7.4429e-07, 1.9232e-04, 1.0000e+00,\n",
            "         4.7614e-08, 2.0608e-04, 4.5519e-08, 3.6206e-06, 7.2917e-06, 1.9207e-04,\n",
            "         1.0000e+00, 1.0000e+00, 8.2396e-07, 2.5038e-07, 3.1042e-06, 1.0000e+00,\n",
            "         3.5885e-06, 4.7315e-06, 1.0000e+00, 8.4801e-08, 9.4538e-08, 2.4523e-07,\n",
            "         9.6802e-08, 2.1769e-04, 4.5843e-07, 5.2700e-08, 2.2603e-04, 7.9230e-08,\n",
            "         2.4876e-07, 3.8778e-06, 1.0000e+00, 2.1869e-04, 7.9008e-07, 2.1680e-08,\n",
            "         7.1634e-08, 4.9193e-08, 8.1885e-08, 1.0000e+00, 2.4002e-06, 1.8279e-04,\n",
            "         3.6158e-06, 2.5378e-08, 2.0968e-04, 3.1418e-06, 4.7065e-06, 2.0042e-04,\n",
            "         2.5860e-07, 2.1625e-04, 6.1073e-08, 2.1519e-04, 6.1545e-07, 5.9725e-07,\n",
            "         5.7318e-08, 2.0882e-04, 2.5891e-07, 2.1500e-04, 1.9954e-06, 3.7758e-08,\n",
            "         1.7550e-07, 1.0819e-06, 1.0000e+00, 5.8173e-07, 5.7837e-07, 2.1511e-04,\n",
            "         5.8806e-08, 2.0394e-06, 4.9538e-06, 6.8501e-08, 5.6153e-08, 5.4257e-08,\n",
            "         2.2916e-04, 1.0000e+00, 5.7605e-08, 4.3730e-06, 6.3314e-08, 1.0000e+00,\n",
            "         6.1052e-08, 3.7868e-06, 6.3746e-07, 1.9847e-04, 7.8253e-08, 2.2705e-04,\n",
            "         1.8411e-07, 5.6536e-06, 1.8250e-04, 2.3020e-04, 2.0847e-04, 5.6213e-06,\n",
            "         3.7100e-08, 1.0000e+00, 5.8107e-08, 2.3084e-08, 1.0000e+00, 1.8643e-04,\n",
            "         8.3391e-07, 1.9356e-04, 1.1779e-07, 7.6030e-08, 8.1244e-07, 5.0588e-08,\n",
            "         1.5250e-07, 7.4179e-08, 5.6481e-07, 5.2107e-07, 2.1582e-04, 7.8178e-08,\n",
            "         2.3327e-07, 1.3801e-06, 3.3687e-06, 1.0000e+00, 1.0000e+00, 9.4829e-08,\n",
            "         1.8193e-04, 3.5251e-07, 5.0578e-08, 2.0771e-07, 5.4286e-08, 1.9755e-04,\n",
            "         1.9124e-07, 2.5856e-07, 2.4928e-07, 6.4314e-08, 2.1216e-04, 7.0115e-07,\n",
            "         4.8301e-08, 8.2115e-07, 3.9788e-08, 1.8675e-04, 2.0598e-04, 1.0000e+00,\n",
            "         3.1350e-07, 2.1538e-04, 3.4408e-07, 9.8318e-08, 1.0745e-06, 1.0021e-07,\n",
            "         1.1312e-06, 1.0592e-06, 1.8332e-04, 2.2719e-04, 6.8563e-07, 4.3370e-08,\n",
            "         3.6926e-06, 4.1197e-08, 5.2610e-06, 3.3267e-06, 5.3205e-06, 4.7785e-08,\n",
            "         1.8365e-04, 1.0000e+00, 6.0279e-08, 5.5665e-08, 1.8408e-04, 2.5624e-07,\n",
            "         3.4624e-06, 6.0633e-08, 1.0000e+00, 6.8680e-08, 2.2499e-06, 7.0785e-07,\n",
            "         3.7024e-07, 5.8727e-08, 6.0916e-07, 6.2882e-08, 1.0000e+00, 6.1606e-08,\n",
            "         7.7620e-08, 1.1910e-06, 9.4244e-07, 3.3261e-07, 3.2841e-08, 1.0000e+00,\n",
            "         1.8933e-04, 1.8754e-04, 9.0270e-08, 5.7755e-07, 4.1605e-07, 1.8746e-04,\n",
            "         1.0000e+00, 1.8523e-04, 4.6309e-07, 2.5923e-07, 2.4338e-07, 2.1319e-04,\n",
            "         1.8973e-04, 4.8130e-08, 3.7346e-07, 4.0960e-08, 2.1010e-04, 5.8568e-08,\n",
            "         1.1416e-06, 1.9675e-04, 1.0000e+00, 6.4924e-07, 3.4963e-06, 1.0000e+00,\n",
            "         4.4418e-08, 4.5669e-06, 2.2980e-04, 1.0000e+00, 6.5514e-07, 2.2014e-04,\n",
            "         1.0000e+00, 8.5921e-07, 1.0000e+00, 6.5364e-07, 5.7036e-08, 7.0120e-08,\n",
            "         2.7118e-06, 2.2765e-08, 2.1251e-04, 6.4391e-08, 1.0000e+00, 5.7872e-08,\n",
            "         5.0311e-08, 4.2899e-06, 5.0605e-06, 1.0000e+00]], device='cuda:0',\n",
            "       grad_fn=<SigmoidBackward>)\n",
            "0.18879971\n",
            "[*] Getting the embeddings and visualizing t-SNE...\n",
            "'dngr_pipeline'  234551.02 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vF5J3xU0RjTB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c2b1b88-26d9-4a9c-e6bc-dd261e4e0cf7"
      },
      "source": [
        "pos_train, neg_train, pos_test, neg_test = train_test_split_pu(dp_net)\r\n",
        "S, H, W =pu_learning(embeddings_drugs, embeddings_targets, dp_net, pos_train, neg_train, k=30, maxiter=100000, alpha=1, gamma=0.6)\r\n",
        "S"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of variables: 6000\n",
            "Finding positive and negative examples...\n",
            "Number of positive examples: 125242\n",
            "Number of negative/unlabelled examples: 125234\n",
            "Going to minimize... Maximum number of iterations: 100000\n",
            "Objective:  tensor(69876.6406, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(64939.7383, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(63269.1016, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62871.4062, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62721.6992, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62651.5547, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62613.9766, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62592.4609, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62574.3203, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62562.9727, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(63237.1016, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62552.1914, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(4313139., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(351931.6562, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(176227.2031, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(126583.4062, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(101315.2734, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(87611.6562, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(78592.0859, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(72372.1641, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(68420.8906, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(65786.9453, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(64109.3281, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(63323.8398, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62917.5156, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(76566.7734, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62630.7227, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62595.1016, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(73626.5234, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62564.0664, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62557.4609, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62545.7461, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62538.7773, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62532.3477, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62523.9102, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(570831.3750, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(175666.3438, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(106967.3203, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(87269.5781, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(75940.6328, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(69733.6172, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(66817.6875, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(65225.0469, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(64248.2266, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(63634.2500, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(63200.8789, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62857.2227, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62729.9062, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62654.5586, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62597.1055, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62566.1367, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62550.3281, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62543.1445, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62534.3008, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62528.7773, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62525.3672, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62594.8516, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62516.7188, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62511.7070, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(729950.6250, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(211910.3750, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(124612.4375, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(94069.9922, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(78848.3594, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(69642.1562, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(65470.3672, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(64132.1797, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(63571.2656, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(63235.0703, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(63060.8555, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62811.8047, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62835.6484, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62630.3164, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62586.0625, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62545.7305, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62534.6992, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62523.9141, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62518.7500, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62514.7070, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62511.8359, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62507.8633, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62504.5273, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62499.4258, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(103391.7656, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(80294.2656, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(72845.0703, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(68853.1172, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(64932.9375, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(73678.8281, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(63267.1641, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(66218.7031, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62882.0664, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62698.0312, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62629.6367, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62595.7734, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62573.0195, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(64094.1992, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62531.0938, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62522.3203, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Objective:  tensor(62519.6797, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "\n",
            "\n",
            "Solved.\n",
            "Now computing Z=HW^T, then will compute S...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.4975, 0.4962, 0.5179,  ..., 0.5141, 0.5132, 0.4975],\n",
              "         [0.5112, 0.5180, 0.5163,  ..., 0.5188, 0.5176, 0.5112],\n",
              "         [0.5120, 0.4984, 0.4671,  ..., 0.4976, 0.4967, 0.5120],\n",
              "         ...,\n",
              "         [0.5049, 0.4919, 0.5091,  ..., 0.5159, 0.5150, 0.5049],\n",
              "         [0.5307, 0.4736, 0.4854,  ..., 0.5387, 0.5376, 0.5307],\n",
              "         [0.4913, 0.4901, 0.5073,  ..., 0.4924, 0.4914, 0.4913]],\n",
              "        device='cuda:0', grad_fn=<MmBackward>),\n",
              " tensor([[ 0.0103, -0.0097, -0.0040,  ..., -0.0288, -0.0077, -0.0143],\n",
              "         [ 0.0003, -0.0077,  0.0003,  ..., -0.0205, -0.0397, -0.0330],\n",
              "         [ 0.0058, -0.0217, -0.0168,  ...,  0.0040,  0.0094,  0.0140],\n",
              "         ...,\n",
              "         [-0.0226, -0.0037, -0.0160,  ..., -0.0101, -0.0056,  0.0158],\n",
              "         [ 0.0030,  0.0089, -0.0051,  ..., -0.0051, -0.0101, -0.0190],\n",
              "         [ 0.0297, -0.0012,  0.0153,  ...,  0.0242,  0.0399,  0.0281]],\n",
              "        device='cuda:0', requires_grad=True),\n",
              " tensor([[ 0.0738,  0.0767,  0.0040,  ...,  0.0953,  0.0444,  0.0956],\n",
              "         [ 0.0299, -0.0024, -0.1807,  ..., -0.0243,  0.0641, -0.0242],\n",
              "         [ 0.0228, -0.0114,  0.0857,  ..., -0.0254,  0.0251,  0.0347],\n",
              "         ...,\n",
              "         [-0.0187, -0.0447, -0.0578,  ...,  0.0395, -0.0324,  0.0158],\n",
              "         [ 0.0043,  0.0393, -0.0041,  ...,  0.0194,  0.0138,  0.0102],\n",
              "         [-0.0963, -0.0944, -0.0779,  ..., -0.1027, -0.1065, -0.0829]],\n",
              "        device='cuda:0', requires_grad=True))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40nSSBPgrq7J",
        "outputId": "5ea77911-dd97-40d9-d968-1301672e8762"
      },
      "source": [
        "test = torch.logical_or(pos_test, neg_test)\r\n",
        "train = torch.logical_or(pos_train, neg_train)\r\n",
        "\r\n",
        "print(torch.sum(test), torch.sum(train))\r\n",
        "print(torch.std(S), torch.std(S[test]), torch.std(S[train]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(533524) tensor(250476)\n",
            "tensor(0.0193, device='cuda:0', grad_fn=<StdBackward0>) tensor(0.0192, device='cuda:0', grad_fn=<StdBackward0>) tensor(0.0193, device='cuda:0', grad_fn=<StdBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86PTYvrRouQO"
      },
      "source": [
        "def binarize(S, threshold=0.5):\r\n",
        "    P = (S > threshold).float()\r\n",
        "    return(P)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RhnpOm_pbw1",
        "outputId": "54e22a58-4ba1-4837-abc2-6f328946e837"
      },
      "source": [
        "P=binarize(S[0])\r\n",
        "dp_net=torch.Tensor(dp_net)\r\n",
        "\r\n",
        "TP=dp_net==1\r\n",
        "print(TP)\r\n",
        "\r\n",
        "print(P[TP].size())\r\n",
        "print(torch.sum(TP))\r\n",
        "\r\n",
        "torch.sum(abs(P[TP]-1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[False, False,  True,  ...,  True,  True,  True],\n",
            "        [ True, False, False,  ..., False, False,  True],\n",
            "        [False, False, False,  ..., False, False, False],\n",
            "        ...,\n",
            "        [ True, False, False,  ..., False, False, False],\n",
            "        [False, False, False,  ...,  True, False,  True],\n",
            "        [ True,  True,  True,  ..., False, False, False]])\n",
            "torch.Size([156577])\n",
            "tensor(156577)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(73420., device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "wzqLVr4HuMXY",
        "outputId": "b3922707-767b-4c63-e536-5a2fc5f6d046"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\r\n",
        "\r\n",
        "y_pred = S.reshape((-1,))\r\n",
        "y_true = dd_net.reshape((-1,))\r\n",
        "\r\n",
        "roc_auc_score(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1568000, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-146-039b6b191c3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdd_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    393\u001b[0m                                              max_fpr=max_fpr),\n\u001b[1;32m    394\u001b[0m                                      \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m                                      sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multilabel-indicator\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: continuous format is not supported"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjJWiAc3RjTB"
      },
      "source": [
        "# [References]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xhpyoOpRjTB"
      },
      "source": [
        "[1] X. Zeng, S. Zhu, W. Lu, Z. Liu, J. Huang, Y. Zhou, J. Fang, Y. Huang, H. Guo, L. Li, B. D. Trapp, R. Nussinov, C. Eng, J. Loscalzo, F. Cheng, Target identification among known drugs by deep learning from heterogeneous networks. Chem. Sci.11, 1775–1797 (2020).\n",
        "\n",
        "[2] Shaosheng Cao, Wei Lu, and Qiongkai Xu. 2016. Deep neural networks for learning graph representations. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI'16). AAAI Press, 1145–1152."
      ]
    }
  ]
}