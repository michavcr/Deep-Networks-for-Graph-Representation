{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implémentation en diverses étapes de l'algorithme deepDTnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing co-occurence matrices (PCO) with random surfing with return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(M):\n",
    "    #Put diagonal elements to 0\n",
    "    M  = M - np.diag(np.diag(M))\n",
    "    \n",
    "    #Normalizing by row\n",
    "    D_inv = np.diag(np.reciprocal(np.sum(M,axis=0)))\n",
    "    M = np.dot(D_inv,  M)\n",
    "\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCO(A, K, alpha):\n",
    "    \"\"\"\n",
    "    For a graph represented by its adjacency matrix *A*, computes the co-occurence matrix by random \n",
    "    surfing on the graph with returns. 1-alpha is the probability to make, at each step, a return \n",
    "    to the original step.\n",
    "    \"\"\"\n",
    "    A=np.array(A, dtype=float)\n",
    "    \n",
    "    #The adjacency matrix A is first normalized\n",
    "    A=normalize(A) \n",
    "    \n",
    "    n=A.shape[0]\n",
    "    \n",
    "    I=np.eye(n)\n",
    "    \n",
    "    P=I\n",
    "    M=np.zeros((n, n))\n",
    "    \n",
    "    for i in range(K):\n",
    "        P = alpha*np.dot(P,A) + (1-alpha)*I\n",
    "        M = M+P\n",
    "    \n",
    "    return(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  0.5 0.5]\n",
      " [0.5 0.  0.5]\n",
      " [0.5 0.5 0. ]]\n",
      "[[0.4 0.3 0.3]\n",
      " [0.3 0.4 0.3]\n",
      " [0.3 0.3 0.4]]\n",
      "[[0.58 0.21 0.21]\n",
      " [0.21 0.58 0.21]\n",
      " [0.21 0.21 0.58]]\n",
      "[[0.526 0.237 0.237]\n",
      " [0.237 0.526 0.237]\n",
      " [0.237 0.237 0.526]]\n",
      "[[0.5422 0.2289 0.2289]\n",
      " [0.2289 0.5422 0.2289]\n",
      " [0.2289 0.2289 0.5422]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.0482, 0.9759, 0.9759],\n",
       "       [0.9759, 2.0482, 0.9759],\n",
       "       [0.9759, 0.9759, 2.0482]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PCO([[0,1,1],[1,0,1],[1,1,0]], 4, 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From co-occurence matrices (PCO) to shifted positive pointwise mutual information (PPMI) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PPMI(M):\n",
    "    \"\"\"Computes the shifted positive pointwise mutual information (PPMI) matrix\n",
    "    from the co-occurence matrix (PCO) of a graph.\"\"\"\n",
    "    \n",
    "    M=normalize(M)\n",
    "    cols = np.sum(M, axis=0)\n",
    "    rows = np.sum(M, axis=1).reshape((-1,1))\n",
    "    s = np.sum(rows)\n",
    "    \n",
    "    P = s*M\n",
    "    P /= cols\n",
    "    P /= rows\n",
    "    \n",
    "    #P[np.where(P<0)] = 1.0\n",
    "    P = np.log(P)\n",
    "\n",
    "    #To avoid NaN when applying log\n",
    "    P[np.isnan(P)] = 0.0\n",
    "    P[np.isinf(P)] = 0.0\n",
    "    P[np.isneginf(P)] = 0.0\n",
    "    P[np.where(P<0)] = 0.0\n",
    "    \n",
    "    return(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  0.5 0.5]\n",
      " [0.5 0.  0.5]\n",
      " [0.5 0.5 0. ]]\n",
      "[[0.6 0.2 0.2]\n",
      " [0.2 0.6 0.2]\n",
      " [0.2 0.2 0.6]]\n",
      "[[0.68 0.16 0.16]\n",
      " [0.16 0.68 0.16]\n",
      " [0.16 0.16 0.68]]\n",
      "[[0.664 0.168 0.168]\n",
      " [0.168 0.664 0.168]\n",
      " [0.168 0.168 0.664]]\n",
      "[[0.6672 0.1664 0.1664]\n",
      " [0.1664 0.6672 0.1664]\n",
      " [0.1664 0.1664 0.6672]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:15: RuntimeWarning: divide by zero encountered in log\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.40546511, 0.40546511],\n",
       "       [0.40546511, 0.        , 0.40546511],\n",
       "       [0.40546511, 0.40546511, 0.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PPMI(PCO([[0,1,1],[1,0,1],[1,1,0]], 4, 0.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding with stacked denoising autoencoders (SDAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNoise(nn.Module):\n",
    "    def __init__(self, stddev):\n",
    "        super().__init__()\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def forward(self, din):\n",
    "        if self.training:\n",
    "            return din + torch.randn(din.size()) * self.stddev\n",
    "        return din"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutNoise(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.size = tuple(x.size())\n",
    "        \n",
    "        a = np.random.random(self.size) > self.p\n",
    "        a = np.array(a, dtype=float)\n",
    "        \n",
    "        return(x*a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor([[1,2,3],[4,5,6],[7,8,9]])\n",
    "DropoutNoise(0.9)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, input_shape, n_neurons, activation='relu', noise=None, noise_arg=None):\n",
    "        super().__init__()\n",
    "        self.n_neurons = n_neurons\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        self.has_noise = False\n",
    "        \n",
    "        if noise=='gaussian':\n",
    "            self.has_noise = True\n",
    "            self.noise = GaussianNoise(noise_arg)\n",
    "        elif noise=='dropout':\n",
    "            self.has_noise = True\n",
    "            self.noise = DropoutNoise(noise_arg)\n",
    "            \n",
    "        self.dense_layer = nn.Linear(self.input_shape, self.n_neurons)\n",
    "        \n",
    "        activations_map = {'relu':nn.ReLU, 'tanh':nn.Tanh, 'sigmoid':nn.Sigmoid}\n",
    "        self.activation = activations_map[activation]()\n",
    "\n",
    "    def forward(self, features):\n",
    "        x=features\n",
    "        \n",
    "        if self.has_noise:\n",
    "            x = self.noise(x)\n",
    "\n",
    "        x = self.dense_layer(features)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDAE(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_layers, activation='relu', last_activation='relu', noise_type='dropout', noise_arg=0.2):\n",
    "        super().__init__()\n",
    "        self.inputs = [input_shape] + hidden_layers\n",
    "        \n",
    "        n = len(self.inputs)\n",
    "        encoder_units = [BasicBlock(self.inputs[0], self.inputs[1], activation=activation, noise=noise_type, noise_arg=noise_arg)]\n",
    "        encoder_units.extend([BasicBlock(self.inputs[i], self.inputs[i+1], activation=activation) for i in range(1, n-1)])\n",
    "        \n",
    "        self.encoder = nn.Sequential(*encoder_units)\n",
    "        \n",
    "        decoder_units = [BasicBlock(self.inputs[i], self.inputs[i-1], activation=activation) for i in range(n-1,1,-1)]\n",
    "        decoder_units.append(BasicBlock(self.inputs[1], self.inputs[0], activation=last_activation))\n",
    "        \n",
    "        self.decoder = nn.Sequential(*decoder_units)\n",
    "        \n",
    "    def forward(self, features):\n",
    "        encoded = self.encoder(features)\n",
    "        \n",
    "        decoded = self.decoder(encoded)\n",
    "        \n",
    "        return(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the SDAE on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SDAE(784, [372, 186, 93]).to(device)\n",
    "\n",
    "# create an optimizer object\n",
    "# Adam optimizer with learning rate 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# mean-squared error loss\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 372]         292,020\n",
      "              ReLU-2                  [-1, 372]               0\n",
      "     GaussianNoise-3                  [-1, 372]               0\n",
      "        BasicBlock-4                  [-1, 372]               0\n",
      "            Linear-5                  [-1, 186]          69,378\n",
      "              ReLU-6                  [-1, 186]               0\n",
      "     GaussianNoise-7                  [-1, 186]               0\n",
      "        BasicBlock-8                  [-1, 186]               0\n",
      "            Linear-9                   [-1, 93]          17,391\n",
      "             ReLU-10                   [-1, 93]               0\n",
      "    GaussianNoise-11                   [-1, 93]               0\n",
      "       BasicBlock-12                   [-1, 93]               0\n",
      "           Linear-13                  [-1, 186]          17,484\n",
      "             ReLU-14                  [-1, 186]               0\n",
      "       BasicBlock-15                  [-1, 186]               0\n",
      "           Linear-16                  [-1, 372]          69,564\n",
      "             ReLU-17                  [-1, 372]               0\n",
      "       BasicBlock-18                  [-1, 372]               0\n",
      "           Linear-19                  [-1, 784]         292,432\n",
      "          Sigmoid-20                  [-1, 784]               0\n",
      "================================================================\n",
      "Total params: 758,269\n",
      "Trainable params: 758,269\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.04\n",
      "Params size (MB): 2.89\n",
      "Estimated Total Size (MB): 2.94\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (784,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()])\n",
    "\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=1,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    npimg = img     # unnormalize\n",
    "    #npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.059\n",
      "[1,  4000] loss: 0.044\n",
      "[1,  6000] loss: 0.038\n",
      "[1,  8000] loss: 0.035\n",
      "[1, 10000] loss: 0.034\n",
      "[1, 12000] loss: 0.033\n",
      "[1, 14000] loss: 0.031\n",
      "[1, 16000] loss: 0.031\n",
      "[1, 18000] loss: 0.030\n",
      "[1, 20000] loss: 0.028\n",
      "[1, 22000] loss: 0.029\n",
      "[1, 24000] loss: 0.028\n",
      "[1, 26000] loss: 0.028\n",
      "[1, 28000] loss: 0.027\n",
      "[1, 30000] loss: 0.027\n",
      "[1, 32000] loss: 0.026\n",
      "[1, 34000] loss: 0.026\n",
      "[1, 36000] loss: 0.026\n",
      "[1, 38000] loss: 0.026\n",
      "[1, 40000] loss: 0.026\n",
      "[1, 42000] loss: 0.025\n",
      "[1, 44000] loss: 0.025\n",
      "[1, 46000] loss: 0.025\n",
      "[1, 48000] loss: 0.024\n",
      "[1, 50000] loss: 0.025\n",
      "[1, 52000] loss: 0.024\n",
      "[1, 54000] loss: 0.024\n",
      "[1, 56000] loss: 0.024\n",
      "[1, 58000] loss: 0.024\n",
      "[1, 60000] loss: 0.024\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        \n",
    "        inputs=torch.flatten(inputs)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, data in enumerate(trainloader, 0):\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    inputs, labels = data\n",
    "    print(inputs.shape)\n",
    "    imshow(torchvision.utils.make_grid(inputs).numpy())     \n",
    "    \n",
    "    inputs=torch.flatten(inputs)\n",
    "        # zero the parameter gradients\n",
    "        # forward + backward + optimize\n",
    "    \n",
    "    inputs_noise=GaussianNoise(0.2)(inputs).reshape((1,1,28,28))\n",
    "    imshow(torchvision.utils.make_grid(inputs_noise).detach().numpy())\n",
    "    \n",
    "    outputs = model(inputs).reshape((1, 1, 28, 28))\n",
    "    \n",
    "    imshow(torchvision.utils.make_grid(outputs).detach().numpy())     \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PU-Learning via matrix completion and convex optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PU Learning\n",
    "\n",
    "k = 7\n",
    "Fd = 20\n",
    "Ft = 20\n",
    "\n",
    "#Number of variables\n",
    "N_variables = Fd * k + Ft * k\n",
    "\n",
    "#Because scipy's minimize needs a (n,) array of variables\n",
    "mat_compo = {}\n",
    "compo_mat = []\n",
    "p=0\n",
    "\n",
    "for c in ['H', 'W']:\n",
    "    for i in range(Fd if c=='H' else Ft):\n",
    "        for j in range(k):\n",
    "            mat_compo[(c, i, j)] = p\n",
    "            compo_mat.append((c, i, j))\n",
    "            p += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nd = 20\n",
    "Nt = 20\n",
    "\n",
    "p = np.eye(20, dtype=float)\n",
    "x = np.random.randn(Nd,Fd)\n",
    "y = np.random.randn(Nt,Ft)\n",
    "\n",
    "alpha = 0.2\n",
    "gamma = 0.3\n",
    "\n",
    "Ipos = [(i,i) for i in range(min(Nd,Nt))]\n",
    "Ineg = [(i,j) for i in range(Nd) for j in range(Nt) if (i,j) not in Ipos]\n",
    "I = Ipos + Ineg\n",
    "\n",
    "def objective(z):\n",
    "    res=0\n",
    "    \n",
    "    H = np.zeros((Fd,k), dtype=float)\n",
    "    W = np.zeros((Ft,k), dtype=float)\n",
    "    \n",
    "    for i in range(Fd):\n",
    "        for j in range(k):\n",
    "            H[i,j]=z[mat_compo[('H', i, j)]]\n",
    "            \n",
    "    for i in range(Ft):\n",
    "        for j in range(k):\n",
    "            W[i,j]=z[mat_compo[('W', i, j)]]\n",
    "         \n",
    "    for (i,j) in Ipos:\n",
    "        res += (p[i,j]- np.dot(np.dot(x[i,:], (H @ np.transpose(W))), y[:,j]))**2\n",
    "    for (i,j) in Ineg:\n",
    "        res += alpha*(p[i,j]- np.dot(np.dot(x[i,:], (H @ np.transpose(W))), y[:,j]))**2\n",
    "    \n",
    "    Sreg = gamma/2 * (np.linalg.norm(H) + np.linalg.norm(W))\n",
    "    \n",
    "    res += Sreg\n",
    "    \n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 6.829665\n",
      "         Iterations: 1000\n",
      "         Function evaluations: 284538\n",
      "         Gradient evaluations: 1009\n"
     ]
    }
   ],
   "source": [
    "res = minimize(objective,x0 = np.random.randn(N_variables), options={'maxiter':1000, 'disp':'True'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2156256971064746\n",
      "0.32101247399922694\n",
      "-0.08885838184022438\n",
      "0.08233132094469015\n",
      "0.062120094941989355\n",
      "-0.3769398966190506\n",
      "-0.16007070675968857\n",
      "-0.1175141503312301\n",
      "0.3132671197931596\n",
      "0.12212999909888365\n",
      "0.18886774771029252\n",
      "0.1406524958251719\n",
      "0.34237874607068397\n",
      "-0.2840154424597365\n",
      "0.04800438015653148\n",
      "-0.11207551752224493\n",
      "0.03384893586862922\n",
      "0.27059663352276064\n",
      "-0.48687662827390343\n",
      "0.19229498628662084\n",
      "-0.2630780666249034\n",
      "0.11850204407581913\n",
      "0.1061514311398693\n",
      "-0.1488636535906391\n",
      "0.2101892946283354\n",
      "-0.08611401275830777\n",
      "0.3248914544289978\n",
      "0.22064895828783526\n",
      "-0.21372587896585798\n",
      "0.28920854715348643\n",
      "0.2303915437191929\n",
      "0.43439703814341285\n",
      "-0.1169274019061948\n",
      "0.3577067468410794\n",
      "0.18783391568892388\n",
      "0.045509432547769624\n",
      "-0.2095365695838818\n",
      "-0.14464879335870995\n",
      "0.32321428325492335\n",
      "-0.24325847533612158\n",
      "-0.1266791676258012\n",
      "0.4080895645803141\n",
      "0.039041491129598496\n",
      "0.009295704832545992\n",
      "-0.34240381593296176\n",
      "-0.059335308473098064\n",
      "-0.042737677874143565\n",
      "0.2350307312628195\n",
      "0.020343417516329065\n",
      "0.08741020561189941\n",
      "-0.20714371932758407\n",
      "0.06017643476629825\n",
      "0.12146295965254293\n",
      "-0.21721329901272735\n",
      "-0.39402268954614283\n",
      "0.40856098545937314\n",
      "-0.1397487803420088\n",
      "-0.0943856116299812\n",
      "0.08962819803646627\n",
      "-0.13941016581082954\n",
      "-0.25712302294731143\n",
      "0.6068882519759633\n",
      "0.13940391442309927\n",
      "0.00978498176056436\n",
      "0.017476620834715698\n",
      "-0.39193123189533\n",
      "0.05599220495116169\n",
      "-0.13934435014926005\n",
      "0.34727211861355284\n",
      "0.2703174430903042\n",
      "-0.17670747319705232\n",
      "0.04494868324274062\n",
      "0.1513182578408119\n",
      "-0.2614322148203796\n",
      "0.004160865965042504\n",
      "0.07934092495863748\n",
      "0.08358124674334837\n",
      "0.11750505819648614\n",
      "0.3921577474954172\n",
      "-0.24177659465368945\n",
      "-0.2805306537099777\n",
      "0.35833859442937727\n",
      "-0.42930938466753354\n",
      "0.12143806899808586\n",
      "0.014038607979863268\n",
      "-0.2276867665679331\n",
      "-0.012798702535273349\n",
      "-0.2107038537708888\n",
      "0.06310262559897968\n",
      "-0.14400745671626872\n",
      "0.225085768815943\n",
      "-0.32014522709334514\n",
      "-0.2620225437400775\n",
      "0.005213717303562043\n",
      "0.16250957336138247\n",
      "0.06331896309049487\n",
      "-0.1582144626572694\n",
      "0.03139373138862231\n",
      "0.623996365847286\n",
      "-0.2960814932530168\n",
      "-0.13682852587302616\n",
      "0.0797227714888547\n",
      "-0.15912874159357604\n",
      "-0.4148031243719279\n",
      "0.1282024936380529\n",
      "-0.23998991544754072\n",
      "0.20662424688679268\n",
      "0.2392616057872741\n",
      "-0.2520104941072222\n",
      "-0.08648562380629338\n",
      "-0.04370167181007828\n",
      "-0.21588690785862014\n",
      "0.17246886087458826\n",
      "-0.057763914384075965\n",
      "-0.3558440777817268\n",
      "0.19627440069370666\n",
      "0.005859764802829848\n",
      "0.15749440509269286\n",
      "0.35593169994935736\n",
      "-0.11310238447958493\n",
      "-0.33890771114372087\n",
      "0.08278882560691403\n",
      "-0.22032181040486534\n",
      "-0.2854933708958344\n",
      "0.1852701796285125\n",
      "0.22989581878754703\n",
      "-0.03237624631339636\n",
      "0.1284048835568754\n",
      "0.1534494057708935\n",
      "0.08785461819671311\n",
      "-0.1670092610604318\n",
      "-0.02854383661825194\n",
      "0.5071262436795254\n",
      "-0.16983984656071738\n",
      "-0.08223009426661355\n",
      "-0.35275452885184055\n",
      "0.009247167007078005\n",
      "-0.42188838582189253\n",
      "0.07188960428143948\n",
      "-0.3195989511125301\n",
      "0.18557237836377555\n",
      "-0.2722449976514071\n",
      "-0.1489999773432108\n",
      "-0.04739275851427904\n",
      "-0.19744653356129435\n",
      "-0.08033434791496197\n",
      "0.295427049571264\n",
      "0.20376622308916592\n",
      "-0.15228878978462956\n",
      "0.2750948375850163\n",
      "0.029646380413555655\n",
      "-0.3386084052171674\n",
      "-0.09949696114970111\n",
      "0.31583329600244314\n",
      "-0.09982727953794669\n",
      "0.03558127325156643\n",
      "0.11147481960563849\n",
      "-0.1356272898003691\n",
      "0.096637360467501\n",
      "0.20800834671392202\n",
      "0.009413826063298214\n",
      "0.10131400348300279\n",
      "-0.15773718193625216\n",
      "-0.12173907053317858\n",
      "-0.06084618905147682\n",
      "0.19424944273869288\n",
      "0.5941659905295124\n",
      "0.08955338278738433\n",
      "-0.3602397986504336\n",
      "-0.3579514889996744\n",
      "-0.29588911925795214\n",
      "0.3048392661323286\n",
      "0.2521828599067393\n",
      "-0.05464149510538652\n",
      "-0.36373186585508455\n",
      "0.003752614761238046\n",
      "-0.3885665576444708\n",
      "-0.1878380608196771\n",
      "-0.27661149327942536\n",
      "0.02049427061271729\n",
      "0.3044705433655105\n",
      "0.12026889484961567\n",
      "0.02043301743626499\n",
      "0.027778710554051213\n",
      "-0.14094716990970896\n",
      "-0.18761951470678898\n",
      "0.36849041760329493\n",
      "0.11609600176244975\n",
      "-0.30177231316173136\n",
      "-0.11726451037989766\n",
      "0.12247564488527297\n",
      "-0.2563282258125359\n",
      "0.24745934862994995\n",
      "0.03969417151630705\n",
      "-0.20730920365018923\n",
      "0.13455610783378535\n",
      "-0.26574813633882266\n",
      "-0.14689826123413813\n",
      "0.08753369968197167\n",
      "0.29476819649488994\n",
      "-0.24577498444599719\n",
      "0.136399351213393\n",
      "-0.33822548144125497\n",
      "0.05810949922673109\n",
      "-0.14371473873246068\n",
      "0.39366401922068395\n",
      "-0.3966730063998505\n",
      "-0.12657413161091643\n",
      "-0.17681723883996842\n",
      "0.19411878718990097\n",
      "0.3353345432823114\n",
      "0.014294239372396197\n",
      "-0.11535260249137062\n",
      "-0.007303652203645074\n",
      "-0.23253269778900054\n",
      "0.1896183551402384\n",
      "-0.06436482663524504\n",
      "-0.14511312241586688\n",
      "0.11860516635906816\n",
      "-0.23735651398338348\n",
      "0.5137022277496277\n",
      "0.03355226189787286\n",
      "0.3196951405187839\n",
      "0.11304601496539818\n",
      "0.1499063921418184\n",
      "0.359236887434552\n",
      "0.3974115589820771\n",
      "-0.05228627567552788\n",
      "0.13602420957228695\n",
      "0.2062851165448577\n",
      "0.08569536273132831\n",
      "0.3359459582883499\n",
      "-0.19731062291008003\n",
      "0.19624375842253147\n",
      "0.20933407008102692\n",
      "-0.20152321977075044\n",
      "-0.12237507983091073\n",
      "-0.002591277509947809\n",
      "0.1551371854731254\n",
      "0.4615795005241847\n",
      "0.0025693498316562563\n",
      "0.21269824627851888\n",
      "0.02487559391624261\n",
      "0.4107691359982403\n",
      "-0.03679552691184748\n",
      "0.31712159808452345\n",
      "0.011970688705064042\n",
      "0.33308194041288824\n",
      "0.4460866107858103\n",
      "-0.1980396745795652\n",
      "0.27674332957249187\n",
      "0.05715199424254752\n",
      "0.22313608645846483\n",
      "-0.05315190951675816\n",
      "-0.08265692412006254\n",
      "-0.07159080700168832\n",
      "0.0070178109370160335\n",
      "-0.3285089543016538\n",
      "0.012693598220443386\n",
      "0.004838798550088515\n",
      "-0.5723851533074108\n",
      "0.03283461387355968\n",
      "-0.1716146832919876\n",
      "0.1303851899280749\n",
      "-0.06669526117481878\n",
      "-0.09803005352367944\n",
      "-0.24520816111378027\n",
      "-0.10510446974604695\n",
      "0.0464632828773586\n",
      "-0.16459568686984227\n",
      "-0.1300259398676554\n",
      "-0.22282333302768825\n",
      "0.1321698028449564\n",
      "0.2921929189191512\n",
      "0.1893340374261501\n",
      "-0.1332729668102819\n",
      "0.047877136671904234\n",
      "0.3290762770086457\n",
      "0.22993063686478976\n",
      "-0.5334835633733672\n",
      "0.2695979623600122\n",
      "0.14767182499636436\n",
      "0.055951806964040035\n",
      "0.24246464572173132\n",
      "-0.09169850566527471\n",
      "0.03294410987652904\n",
      "0.3378138068935207\n",
      "0.06814902604413892\n",
      "0.07682109450115931\n",
      "0.20200502281689886\n",
      "-0.05849478775982131\n",
      "0.1550112350328653\n",
      "0.23785028436433742\n",
      "0.589559830824979\n",
      "-0.21356454623506213\n",
      "-0.12913187838230206\n",
      "0.08163059049175633\n",
      "0.02803228679267882\n",
      "0.023371527953223503\n",
      "0.27426680378644014\n",
      "-0.1298840265477451\n",
      "-0.2674990458916525\n",
      "0.019455440120841272\n",
      "-0.364913971178368\n",
      "-0.10849756539600434\n",
      "0.17998224354220008\n",
      "-0.24198529418328443\n",
      "0.1504136677413892\n",
      "-0.3437769310301396\n",
      "-0.04359441615979387\n",
      "0.10149320048722818\n",
      "-0.16333178216816957\n",
      "0.08502639202924539\n",
      "0.3743457028224558\n",
      "0.431882904726105\n",
      "0.1425764461504919\n",
      "0.44663337123577235\n",
      "-0.16662064081404965\n",
      "0.15951035921486167\n",
      "-0.15306766145862105\n",
      "-0.06011100265874614\n",
      "-0.2782684272597868\n",
      "0.11069446316926518\n",
      "0.0003503254187689486\n",
      "0.09168009197297951\n",
      "-0.38995582785950245\n",
      "-0.25784092758840327\n",
      "-0.2621402664746654\n",
      "-0.2341916136944358\n",
      "-0.15063170977091078\n",
      "0.26982319293529655\n",
      "-0.3373482786296025\n",
      "0.1216055173014827\n",
      "-0.3975820531623352\n",
      "0.37442891572162906\n",
      "-0.018167226883474177\n",
      "0.12811429179753275\n",
      "0.06644191744893249\n",
      "-0.24941979599267888\n",
      "-0.04897630795847666\n",
      "0.12835472505885026\n",
      "0.035506524678362586\n",
      "0.23159412627862763\n",
      "-0.20098260503289778\n",
      "0.41113259973013955\n",
      "0.00757752906028436\n",
      "0.02199936868493456\n",
      "0.1578600781749694\n",
      "-0.02977991955203635\n",
      "0.03674444864664142\n",
      "-0.3318057346835045\n",
      "-0.3073572615307818\n",
      "-0.13456578466894797\n",
      "0.38764814633360123\n",
      "0.3216564463447149\n",
      "-0.09607734537770063\n",
      "0.2581059908521283\n",
      "0.04447636202187774\n",
      "-0.25743496773704316\n",
      "0.1262099607356721\n",
      "-0.1342079218438997\n",
      "-0.4738275075794275\n",
      "-0.1257611859212696\n",
      "-0.148916471595884\n",
      "0.06790057334155705\n",
      "0.14056884216207077\n",
      "-0.07243652562564587\n",
      "0.4982514230970061\n",
      "-0.34655855897658583\n",
      "-0.27765307614105056\n",
      "-0.10512014875542708\n",
      "-0.19905141327397557\n",
      "-0.024006150642597868\n",
      "0.022247967188228218\n",
      "-0.08221069182278498\n",
      "-0.07312933516945158\n",
      "-0.363523445255585\n",
      "0.11935978742865916\n",
      "0.05093536394034853\n",
      "-0.15290462093986734\n"
     ]
    }
   ],
   "source": [
    "z=res['x']\n",
    "H = np.zeros((Fd,k), dtype=float)\n",
    "W = np.zeros((Ft,k), dtype=float)\n",
    "    \n",
    "for i in range(Fd):\n",
    "    for j in range(k):\n",
    "        H[i,j]=z[mat_compo[('H', i, j)]]\n",
    "            \n",
    "for i in range(Ft):\n",
    "    for j in range(k):\n",
    "        W[i,j]=z[mat_compo[('W', i, j)]]\n",
    "\n",
    "H @ np.transpose(W)\n",
    "\n",
    "for i,j in Ineg:\n",
    "    print(np.dot(np.dot(x[i,:], (H @ np.transpose(W))), y[:,j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate random networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_graph(p, size=(100,100)):\n",
    "    return(np.array([[int(random.random() < p) for i in range(size[1])] for j in range(size[0])]))\n",
    "\n",
    "def random_undirected_graph(p, size=(100,100)):\n",
    "    graph = random_graph(p, size=size)\n",
    "    graph[np.arange(size[0]),np.arange(size[1])]=0 #nullify the diagonal\n",
    "    graph = np.maximum(graph, graph.T) #make it symmetric\n",
    "    \n",
    "    return(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_graph_with_fixed_components(p, nodes_per_component=[50,50]):\n",
    "    nodes_per_component = np.array(nodes_per_component)\n",
    "    n_nodes = nodes_per_component.sum()\n",
    "    n_cmp = nodes_per_component.shape[0]\n",
    "    \n",
    "    graph = np.zeros((n_nodes, n_nodes))\n",
    "    nodes = np.arange(n_nodes)\n",
    "    np.random.shuffle(nodes)\n",
    "    \n",
    "    cmp_nodes = []\n",
    "    acc=0\n",
    "    \n",
    "    for i in range(n_cmp):\n",
    "        cmp = nodes[acc:(acc+nodes_per_component[i])]\n",
    "        cmp_nodes.append(cmp)\n",
    "        acc += nodes_per_component[i]\n",
    "        \n",
    "        size = cmp.shape[0]\n",
    "        submatrix=np.ix_(cmp,cmp)\n",
    "\n",
    "        graph[submatrix] = random_undirected_graph(p, (size,size))\n",
    "        \n",
    "    return(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbors(adj, i):\n",
    "    return (np.where(adj[i,:]==1)[0])\n",
    "\n",
    "def dfs(adj, i):\n",
    "    n = adj.shape[0] #number of nodes in the graph\n",
    "    visited = [False for k in range(n)]\n",
    "    \n",
    "    stack = [i]\n",
    "    \n",
    "    while(len(stack)>0):\n",
    "        k = stack.pop()\n",
    "        neighborhood = neighbors(adj, k)\n",
    "        visited[k] = True\n",
    "        \n",
    "        for n in neighborhood:\n",
    "            if not visited[n]:\n",
    "                stack.append(n)\n",
    "    \n",
    "    return(np.where(visited))\n",
    "\n",
    "def connected_components(adj):\n",
    "    n = adj.shape[0]\n",
    "    \n",
    "    visited = np.array([0 for k in range(n)])\n",
    "    s = np.sum(visited)\n",
    "    \n",
    "    comp=[]\n",
    "    \n",
    "    while s<n:\n",
    "        i = np.where(1-visited)[0][0]\n",
    "        \n",
    "        cmp = dfs(adj, i)\n",
    "        visited[cmp] = 1\n",
    "        s = np.sum(visited)\n",
    "        \n",
    "        comp.append(list(cmp[0]))\n",
    "    \n",
    "    return(np.array(comp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5001339285714286"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_rand_drugs = 784\n",
    "N_rand_targets = 1000\n",
    "\n",
    "density=0.5\n",
    "#generate random matrices in M({0,1})\n",
    "#1. a drug-drug network (drug similarities)\n",
    "#2. a protein-protein network (protein similarities)\n",
    "#3. a drug-protein network (drug-target known relationships)\n",
    "dd_net = random_graph_with_fixed_components(density, [196,196,196,196])\n",
    "pp_net = random_graph_with_fixed_components(density, [100 for i in range(10)])\n",
    "dp_net = random_graph(density,size=(N_rand_drugs,N_rand_targets))\n",
    "\n",
    "np.sum(dp_net)/(N_rand_drugs*N_rand_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18621147438567265"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.sum(dd_net))/(dd_net[0].shape[0])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "       0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.,\n",
       "       1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
       "       1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
       "       1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
       "       0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0.])"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd_net[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the drug-drug network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:15: RuntimeWarning: divide by zero encountered in log\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(784, 784)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 10\n",
    "alpha = 0.2\n",
    "\n",
    "ppmi_dd_net = PPMI(PCO(dd_net, K, alpha))\n",
    "\n",
    "ppmi_dd_net.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SDAE(784, [500, 200, 100], activation='sigmoid', last_activation='sigmoid').to(device)\n",
    "\n",
    "# create an optimizer object\n",
    "# Adam optimizer with learning rate 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# mean-squared error loss\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "      DropoutNoise-1                  [-1, 784]               0\n",
      "            Linear-2                  [-1, 500]         392,500\n",
      "           Sigmoid-3                  [-1, 500]               0\n",
      "        BasicBlock-4                  [-1, 500]               0\n",
      "            Linear-5                  [-1, 200]         100,200\n",
      "           Sigmoid-6                  [-1, 200]               0\n",
      "        BasicBlock-7                  [-1, 200]               0\n",
      "            Linear-8                  [-1, 100]          20,100\n",
      "           Sigmoid-9                  [-1, 100]               0\n",
      "       BasicBlock-10                  [-1, 100]               0\n",
      "           Linear-11                  [-1, 200]          20,200\n",
      "          Sigmoid-12                  [-1, 200]               0\n",
      "       BasicBlock-13                  [-1, 200]               0\n",
      "           Linear-14                  [-1, 500]         100,500\n",
      "          Sigmoid-15                  [-1, 500]               0\n",
      "       BasicBlock-16                  [-1, 500]               0\n",
      "           Linear-17                  [-1, 784]         392,784\n",
      "             ReLU-18                  [-1, 784]               0\n",
      "       BasicBlock-19                  [-1, 784]               0\n",
      "================================================================\n",
      "Total params: 1,026,284\n",
      "Trainable params: 1,026,284\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.06\n",
      "Params size (MB): 3.91\n",
      "Estimated Total Size (MB): 3.98\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (784,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 1.60408146, 0.        , 0.        ,\n",
       "       0.        , 1.56300076, 0.        , 0.        , 1.58436426,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.58634296, 1.59339621, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 1.5592621 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.61541055, 0.        , 1.62067346, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.61363193, 0.        , 1.57987602, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.56780089, 0.        , 1.56404455, 1.67590289, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.56646421, 1.58175967, 0.        , 0.        , 1.56374424,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 1.57555983, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.55501368, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 1.62367985, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 1.61346692, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.56804039, 0.        , 1.64091477, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 1.62152438,\n",
       "       1.64537169, 0.        , 0.        , 1.59167141, 0.        ,\n",
       "       1.61227814, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 1.58721102, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 1.57455533, 0.        ,\n",
       "       1.64370788, 0.        , 0.        , 0.        , 1.63560698,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.61959645, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 1.62654394, 1.59847435, 1.58265516, 0.        ,\n",
       "       1.58376374, 0.        , 1.59225987, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 1.57641683,\n",
       "       0.        , 1.55487979, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 1.58752884, 0.        ,\n",
       "       0.        , 0.        , 1.56885655, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 1.56219489, 1.5942498 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 1.58827304,\n",
       "       1.61129856, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 1.64281726, 0.        , 1.57930889,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.60719244, 0.        , 0.        , 0.        , 1.64536446,\n",
       "       1.65348152, 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.64457932, 0.        , 1.669813  , 1.60078799, 0.        ,\n",
       "       0.        , 0.        , 1.61754284, 0.        , 1.65098812,\n",
       "       1.66701181, 0.        , 1.62971128, 0.        , 0.        ,\n",
       "       1.61844612, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.64538994, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 1.60151734, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 1.69993768, 0.        ,\n",
       "       1.61684124, 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.65471232, 0.        , 1.63161391, 0.        , 0.        ,\n",
       "       1.57658964, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.60072631, 1.56689751, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 1.65216277, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 1.65448602, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.60203159, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.63107726, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 1.5666654 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 1.58567431, 0.        , 0.        , 0.        ,\n",
       "       1.58763955, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 1.56103948,\n",
       "       0.        , 0.        , 0.        , 1.60788982, 0.        ,\n",
       "       1.55597695, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.5884851 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 1.66027107, 1.58876191, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 1.57522261,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 1.62563906, 0.        ,\n",
       "       0.        , 1.57022388, 0.        , 0.        , 0.        ,\n",
       "       0.        , 1.61723902, 0.        , 0.        , 0.        ,\n",
       "       1.57262433, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 1.5859873 , 0.        , 0.        ,\n",
       "       0.        , 1.60046545, 0.        , 0.        , 0.        ,\n",
       "       1.63871364, 1.63122511, 0.        , 1.57729599, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 1.62122434, 1.65713645,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.60858708, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.65583856, 0.        , 1.62649965, 0.        , 0.        ,\n",
       "       0.        , 1.5591578 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 1.59710902, 0.        ,\n",
       "       1.57466444, 0.        , 1.57741183, 0.        , 1.60001349,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.60259748, 1.62202501, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 1.5601752 ,\n",
       "       0.        , 0.        , 1.62652163, 0.        , 0.        ,\n",
       "       0.        , 1.66720194, 0.        , 1.56921479, 0.        ,\n",
       "       1.56455535, 0.        , 0.        , 1.6139395 , 1.56345265,\n",
       "       0.        , 0.        , 0.        , 0.        , 1.6151922 ,\n",
       "       1.58917089, 1.61257193, 0.        , 0.        , 0.        ,\n",
       "       1.58990103, 0.        , 1.64955979, 1.58433675, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.58617417, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 1.57936295, 0.        , 1.60021512,\n",
       "       0.        , 1.58819002, 0.        , 1.60728902, 1.6190357 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 1.58185107, 0.        , 1.58898871,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.60704305, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 1.61044636, 1.56652416,\n",
       "       0.        , 0.        , 0.        , 0.        , 1.63948101,\n",
       "       0.        , 1.60103982, 0.        , 1.57055646, 0.        ,\n",
       "       1.6054618 , 0.        , 1.62408767, 1.62947731, 0.        ,\n",
       "       0.        , 0.        , 0.        , 1.60158348, 0.        ,\n",
       "       0.        , 0.        , 0.        , 1.57131936, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 1.56887598,\n",
       "       0.        , 1.5850386 , 0.        , 0.        , 1.68283198,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.5840642 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.57006798, 1.62154167, 0.        , 1.63255942, 0.        ,\n",
       "       0.        , 1.58005226, 0.        , 0.        , 0.        ,\n",
       "       1.61049156, 0.        , 1.58668785, 1.6249244 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 1.55528265, 1.64342012,\n",
       "       0.        , 0.        , 1.6520896 , 0.        , 1.65294257,\n",
       "       0.        , 0.        , 0.        , 1.57944202, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 1.66008892, 0.        ,\n",
       "       0.        , 1.63412262, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ppmi_dd_net = (ppmi_dd_net - ppmi_dd_net.mean(axis=0)) / (ppmi_dd_net.std(axis=0))\n",
    "ppmi_dd_net[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.utils.data.TensorDataset(torch.Tensor(ppmi_dd_net), torch.Tensor(ppmi_dd_net))\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   784] loss: 0.481\n",
      "[2,   784] loss: 0.461\n",
      "[3,   784] loss: 0.452\n",
      "[4,   784] loss: 0.448\n",
      "[5,   784] loss: 0.447\n",
      "[6,   784] loss: 0.446\n",
      "[7,   784] loss: 0.444\n",
      "[8,   784] loss: 0.443\n",
      "[9,   784] loss: 0.442\n",
      "[10,   784] loss: 0.441\n",
      "[11,   784] loss: 0.441\n",
      "[12,   784] loss: 0.440\n",
      "[13,   784] loss: 0.440\n",
      "[14,   784] loss: 0.440\n",
      "[15,   784] loss: 0.439\n",
      "[16,   784] loss: 0.439\n",
      "[17,   784] loss: 0.439\n",
      "[18,   784] loss: 0.438\n",
      "[19,   784] loss: 0.438\n",
      "[20,   784] loss: 0.438\n",
      "[21,   784] loss: 0.437\n",
      "[22,   784] loss: 0.437\n",
      "[23,   784] loss: 0.437\n",
      "[24,   784] loss: 0.436\n",
      "[25,   784] loss: 0.436\n",
      "[26,   784] loss: 0.435\n",
      "[27,   784] loss: 0.435\n",
      "[28,   784] loss: 0.435\n",
      "[29,   784] loss: 0.435\n",
      "[30,   784] loss: 0.435\n",
      "[31,   784] loss: 0.435\n",
      "[32,   784] loss: 0.435\n",
      "[33,   784] loss: 0.435\n",
      "[34,   784] loss: 0.435\n",
      "[35,   784] loss: 0.435\n",
      "[36,   784] loss: 0.434\n",
      "[37,   784] loss: 0.434\n",
      "[38,   784] loss: 0.434\n",
      "[39,   784] loss: 0.434\n",
      "[40,   784] loss: 0.434\n",
      "[41,   784] loss: 0.433\n",
      "[42,   784] loss: 0.433\n",
      "[43,   784] loss: 0.432\n",
      "[44,   784] loss: 0.432\n",
      "[45,   784] loss: 0.432\n",
      "[46,   784] loss: 0.432\n",
      "[47,   784] loss: 0.432\n",
      "[48,   784] loss: 0.432\n",
      "[49,   784] loss: 0.432\n",
      "[50,   784] loss: 0.432\n",
      "[51,   784] loss: 0.432\n",
      "[52,   784] loss: 0.432\n",
      "[53,   784] loss: 0.432\n",
      "[54,   784] loss: 0.432\n",
      "[55,   784] loss: 0.432\n",
      "[56,   784] loss: 0.432\n",
      "[57,   784] loss: 0.432\n",
      "[58,   784] loss: 0.432\n",
      "[59,   784] loss: 0.431\n",
      "[60,   784] loss: 0.431\n",
      "[61,   784] loss: 0.431\n",
      "[62,   784] loss: 0.430\n",
      "[63,   784] loss: 0.430\n",
      "[64,   784] loss: 0.430\n",
      "[65,   784] loss: 0.430\n",
      "[66,   784] loss: 0.430\n",
      "[67,   784] loss: 0.430\n",
      "[68,   784] loss: 0.430\n",
      "[69,   784] loss: 0.430\n",
      "[70,   784] loss: 0.430\n",
      "[71,   784] loss: 0.430\n",
      "[72,   784] loss: 0.430\n",
      "[73,   784] loss: 0.430\n",
      "[74,   784] loss: 0.430\n",
      "[75,   784] loss: 0.429\n",
      "[76,   784] loss: 0.429\n",
      "[77,   784] loss: 0.429\n",
      "[78,   784] loss: 0.429\n",
      "[79,   784] loss: 0.429\n",
      "[80,   784] loss: 0.429\n",
      "[81,   784] loss: 0.429\n",
      "[82,   784] loss: 0.429\n",
      "[83,   784] loss: 0.429\n",
      "[84,   784] loss: 0.429\n",
      "[85,   784] loss: 0.429\n",
      "[86,   784] loss: 0.429\n",
      "[87,   784] loss: 0.429\n",
      "[88,   784] loss: 0.428\n",
      "[89,   784] loss: 0.428\n",
      "[90,   784] loss: 0.428\n",
      "[91,   784] loss: 0.428\n",
      "[92,   784] loss: 0.428\n",
      "[93,   784] loss: 0.428\n",
      "[94,   784] loss: 0.428\n",
      "[95,   784] loss: 0.428\n",
      "[96,   784] loss: 0.428\n",
      "[97,   784] loss: 0.427\n",
      "[98,   784] loss: 0.427\n",
      "[99,   784] loss: 0.427\n",
      "[100,   784] loss: 0.427\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        \n",
    "        inputs=torch.flatten(inputs)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 784 == 783: \n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 784))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.6041, 0.0000,\n",
      "         0.0000, 0.0000, 1.5630, 0.0000, 0.0000, 1.5844, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.5863, 1.5934, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.5593, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.6154, 0.0000, 1.6207, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.6136, 0.0000, 1.5799, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.5678, 0.0000, 1.5640,\n",
      "         1.6759, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.5665, 1.5818, 0.0000, 0.0000, 1.5637,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.5756, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.5550, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.6237, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.6135, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.5680, 0.0000, 1.6409, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.6215, 1.6454, 0.0000, 0.0000, 1.5917, 0.0000, 1.6123, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.5872, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 1.5746, 0.0000, 1.6437, 0.0000, 0.0000, 0.0000, 1.6356, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.6196, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 1.6265, 1.5985, 1.5827, 0.0000, 1.5838, 0.0000, 1.5923, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 1.5764, 0.0000, 1.5549, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.5875, 0.0000, 0.0000,\n",
      "         0.0000, 1.5689, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.5622,\n",
      "         1.5942, 0.0000, 0.0000, 0.0000, 0.0000, 1.5883, 1.6113, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.6428, 0.0000, 1.5793, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 1.6072, 0.0000, 0.0000, 0.0000, 1.6454, 1.6535,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.6446, 0.0000, 1.6698, 1.6008, 0.0000,\n",
      "         0.0000, 0.0000, 1.6175, 0.0000, 1.6510, 1.6670, 0.0000, 1.6297, 0.0000,\n",
      "         0.0000, 1.6184, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.6454, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.6015, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 1.6999, 0.0000, 1.6168, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 1.6547, 0.0000, 1.6316, 0.0000, 0.0000, 1.5766, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.6007, 1.5669,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.6522, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 1.6545, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.6020,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.6311, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.5667, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.5857, 0.0000, 0.0000, 0.0000, 1.5876, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.5610, 0.0000, 0.0000,\n",
      "         0.0000, 1.6079, 0.0000, 1.5560, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.5885, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 1.6603, 1.5888, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.5752, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.6256, 0.0000, 0.0000, 1.5702,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.6172, 0.0000, 0.0000, 0.0000, 1.5726,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.5860, 0.0000, 0.0000, 0.0000, 1.6005, 0.0000, 0.0000,\n",
      "         0.0000, 1.6387, 1.6312, 0.0000, 1.5773, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.6212, 1.6571, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.6086,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.6558, 0.0000, 1.6265, 0.0000, 0.0000, 0.0000, 1.5592, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.5971, 0.0000, 1.5747, 0.0000, 1.5774,\n",
      "         0.0000, 1.6000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.6026, 1.6220,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.5602, 0.0000,\n",
      "         0.0000, 1.6265, 0.0000, 0.0000, 0.0000, 1.6672, 0.0000, 1.5692, 0.0000,\n",
      "         1.5646, 0.0000, 0.0000, 1.6139, 1.5635, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.6152, 1.5892, 1.6126, 0.0000, 0.0000, 0.0000, 1.5899, 0.0000, 1.6496,\n",
      "         1.5843, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.5862, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.5794, 0.0000, 1.6002, 0.0000,\n",
      "         1.5882, 0.0000, 1.6073, 1.6190, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.5819, 0.0000, 1.5890, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.6070, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.6104, 1.5665, 0.0000, 0.0000, 0.0000, 0.0000, 1.6395,\n",
      "         0.0000, 1.6010, 0.0000, 1.5706, 0.0000, 1.6055, 0.0000, 1.6241, 1.6295,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.6016, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.5713, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.5689, 0.0000, 1.5850, 0.0000, 0.0000, 1.6828, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.5841, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.5701, 1.6215, 0.0000, 1.6326, 0.0000, 0.0000, 1.5801, 0.0000, 0.0000,\n",
      "         0.0000, 1.6105, 0.0000, 1.5867, 1.6249, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.5553, 1.6434, 0.0000, 0.0000, 1.6521, 0.0000, 1.6529, 0.0000, 0.0000,\n",
      "         0.0000, 1.5794, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 1.6601, 0.0000, 0.0000, 1.6341, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 1.2025, 1.2392, 0.0000, 0.0000, 1.2274, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 1.2474, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.2742, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.1365, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.2186, 0.0000, 1.3029, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.2231, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.2099, 0.0000, 0.0000, 0.0000, 0.0000, 1.2357, 0.0000, 0.0000, 1.2666,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.2076, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.2051, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.1988, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 1.2246, 0.0000, 0.0000, 1.2131, 0.0000, 1.1766, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.2415, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 1.2262, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.2326, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 1.1252, 1.2292, 1.2731, 0.0000, 0.0000, 0.0000, 1.1935, 1.2264,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.2941, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 1.2379, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.2454, 0.0000, 0.0000, 0.0000, 0.0000, 1.2162, 1.2078, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.2285, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 1.2135, 0.0000, 0.0000, 0.0000, 1.1707, 1.1297,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.1493, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.2187, 1.2113, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.2347, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.2451, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 1.0889, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.2818, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.1438, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 1.2769, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.2549,\n",
      "         0.0000, 0.0000, 1.2343, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.1727, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.2401, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.2852, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.3249, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.2314,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.1861, 0.0000, 0.0000, 0.0000, 1.2854,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.2623, 0.0000, 0.0000,\n",
      "         1.1570, 1.2126, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.3142,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.1804, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.2764, 1.2643,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 1.2465, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.2805, 0.0000,\n",
      "         1.2294, 0.0000, 0.0000, 0.0000, 1.3002, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 1.2270, 0.0000, 0.0000, 0.0000, 0.0000, 1.1168, 0.0000, 1.2088,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.2265, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.1457, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.2007, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.1360,\n",
      "         0.0000, 0.0000, 0.0000, 1.1781, 0.0000, 0.0000, 0.0000, 0.0000, 1.2037,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.2304, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.2521, 0.0000, 0.0000, 0.0000, 0.0000, 1.2193, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.2656, 0.0000, 1.2820, 1.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.2853, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0962, 0.0000, 0.0000,\n",
      "         0.0000, 1.2620, 1.2230, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 1.1802, 0.0000, 0.0000, 1.2092, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.3115, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]], grad_fn=<ReluBackward0>)\n",
      "0.3003582\n"
     ]
    }
   ],
   "source": [
    "trainiter = iter(train_loader)\n",
    "inputs, _ = trainiter.next()\n",
    "\n",
    "print(inputs)\n",
    "print(model(inputs))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(mean_squared_error(inputs.detach().numpy(), model(inputs).detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connected components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = random_undirected_graph(0.02, size=(100, 100))\n",
    "graph[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  3,  6,  8, 16, 20, 21, 31, 32, 38, 39, 41, 45, 55, 56,\n",
       "        60, 64, 65, 67, 68, 76, 81, 87, 95],\n",
       "       [ 2,  9, 18, 24, 27, 34, 35, 40, 48, 52, 59, 71, 75, 77, 78, 79,\n",
       "        80, 82, 83, 84, 85, 89, 93, 96, 97],\n",
       "       [ 4,  5,  7, 10, 22, 23, 28, 29, 44, 46, 47, 53, 54, 58, 62, 63,\n",
       "        66, 70, 73, 74, 86, 88, 90, 92, 99],\n",
       "       [11, 12, 13, 14, 15, 17, 19, 25, 26, 30, 33, 36, 37, 42, 43, 49,\n",
       "        50, 51, 57, 61, 69, 72, 91, 94, 98]])"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connected_components(random_graph_with_fixed_components(0.2, [25,25,25,25]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding evaluation with t-SNE visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize_TSNE(embeddings,target):\n",
    "    tsne = TSNE(n_components=2, init='pca',\n",
    "                         random_state=0, perplexity=30)\n",
    "    data = tsne.fit_transform(embeddings)\n",
    "    #plt.figure(figsize=(12, 6))\n",
    "    plt.title(\"TSNE visualization of the embeddings\")\n",
    "    plt.scatter(data[:,0],data[:,1],c=target)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(train_loader, N, model, size_encoded=100):\n",
    "    trainiter = iter(train_loader)\n",
    "    embeddings = np.zeros((N, size_encoded))\n",
    "\n",
    "    for i,q in enumerate(trainiter):\n",
    "        embedded = model.encoder(q[0]).detach().numpy()\n",
    "        embeddings[i,:] = embedded.reshape((size_encoded,))\n",
    "    \n",
    "    return(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=get_embeddings(train_loader, N_rand_drugs, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmps = connected_components(dd_net)\n",
    "targets = [0 for i in range(N_rand_drugs)]\n",
    "\n",
    "for i, cmp in enumerate(cmps):\n",
    "    for n in cmp:\n",
    "        targets[n] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_TSNE(embeddings, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [References]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] X. Zeng, S. Zhu, W. Lu, Z. Liu, J. Huang, Y. Zhou, J. Fang, Y. Huang, H. Guo, L. Li, B. D. Trapp, R. Nussinov, C. Eng, J. Loscalzo, F. Cheng, Target identification among known drugs by deep learning from heterogeneous networks. Chem. Sci.11, 1775–1797 (2020).\n",
    "\n",
    "[2] Shaosheng Cao, Wei Lu, and Qiongkai Xu. 2016. Deep neural networks for learning graph representations. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI'16). AAAI Press, 1145–1152."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
