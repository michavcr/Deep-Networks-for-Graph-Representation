{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../src/\")\n",
    "\n",
    "from utils import *\n",
    "from pu_learning import *\n",
    "from autoencoder import *\n",
    "from dgnr import *\n",
    "from random_graphs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.496125"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_rand_drugs = 80\n",
    "N_rand_targets = 100\n",
    "\n",
    "density=0.2\n",
    "density_dp = 0.5\n",
    "#generate random matrices in M({0,1})\n",
    "#1. a drug-drug network (drug similarities)\n",
    "#2. a protein-protein network (protein similarities)\n",
    "#3. a drug-protein network (drug-target known relationships)\n",
    "dd_net = random_graph_with_fixed_components(density, [40,40])\n",
    "pp_net = random_graph_with_fixed_components(0.4, [50,50])\n",
    "dp_net = random_graph(density_dp, size=(N_rand_drugs,N_rand_targets))\n",
    "\n",
    "np.sum(dp_net)/(N_rand_drugs*N_rand_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../src/dgnr.py:92: RuntimeWarning: divide by zero encountered in log\n",
      "  P = np.log(P)\n",
      "/home/michael/.local/lib/python3.6/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([80])) that is different to the input size (torch.Size([1, 80])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    80] loss: 0.108\n",
      "[2,    80] loss: 0.067\n",
      "[3,    80] loss: 0.067\n",
      "[4,    80] loss: 0.067\n",
      "[5,    80] loss: 0.067\n",
      "[6,    80] loss: 0.067\n",
      "[7,    80] loss: 0.067\n",
      "[8,    80] loss: 0.067\n",
      "[9,    80] loss: 0.067\n",
      "[10,    80] loss: 0.067\n",
      "[11,    80] loss: 0.067\n",
      "[12,    80] loss: 0.067\n",
      "[13,    80] loss: 0.067\n",
      "[14,    80] loss: 0.067\n",
      "[15,    80] loss: 0.067\n",
      "[16,    80] loss: 0.067\n",
      "[17,    80] loss: 0.067\n",
      "[18,    80] loss: 0.067\n",
      "[19,    80] loss: 0.067\n",
      "[20,    80] loss: 0.067\n",
      "[21,    80] loss: 0.067\n",
      "[22,    80] loss: 0.067\n",
      "[23,    80] loss: 0.067\n",
      "[24,    80] loss: 0.067\n",
      "[25,    80] loss: 0.067\n",
      "[26,    80] loss: 0.067\n",
      "[27,    80] loss: 0.067\n",
      "[28,    80] loss: 0.067\n",
      "[29,    80] loss: 0.067\n",
      "[30,    80] loss: 0.066\n",
      "[31,    80] loss: 0.065\n",
      "[32,    80] loss: 0.061\n",
      "[33,    80] loss: 0.058\n",
      "[34,    80] loss: 0.056\n",
      "[35,    80] loss: 0.055\n",
      "[36,    80] loss: 0.055\n",
      "[37,    80] loss: 0.055\n",
      "[38,    80] loss: 0.054\n",
      "[39,    80] loss: 0.054\n",
      "[40,    80] loss: 0.054\n",
      "[41,    80] loss: 0.054\n",
      "[42,    80] loss: 0.054\n",
      "[43,    80] loss: 0.054\n",
      "[44,    80] loss: 0.054\n",
      "[45,    80] loss: 0.054\n",
      "[46,    80] loss: 0.054\n",
      "[47,    80] loss: 0.054\n",
      "[48,    80] loss: 0.054\n",
      "[49,    80] loss: 0.054\n",
      "[50,    80] loss: 0.054\n",
      "[51,    80] loss: 0.054\n",
      "[52,    80] loss: 0.054\n",
      "[53,    80] loss: 0.053\n",
      "[54,    80] loss: 0.053\n",
      "[55,    80] loss: 0.053\n",
      "[56,    80] loss: 0.053\n",
      "[57,    80] loss: 0.053\n",
      "[58,    80] loss: 0.053\n",
      "[59,    80] loss: 0.053\n",
      "[60,    80] loss: 0.054\n",
      "[61,    80] loss: 0.053\n",
      "[62,    80] loss: 0.053\n",
      "[63,    80] loss: 0.053\n",
      "[64,    80] loss: 0.053\n",
      "[65,    80] loss: 0.053\n",
      "[66,    80] loss: 0.053\n",
      "[67,    80] loss: 0.053\n",
      "[68,    80] loss: 0.053\n",
      "[69,    80] loss: 0.053\n",
      "[70,    80] loss: 0.053\n",
      "[71,    80] loss: 0.053\n",
      "[72,    80] loss: 0.053\n",
      "[73,    80] loss: 0.054\n",
      "[74,    80] loss: 0.053\n",
      "[75,    80] loss: 0.053\n",
      "[76,    80] loss: 0.053\n",
      "[77,    80] loss: 0.053\n",
      "[78,    80] loss: 0.053\n",
      "[79,    80] loss: 0.053\n",
      "[80,    80] loss: 0.053\n",
      "[81,    80] loss: 0.053\n",
      "[82,    80] loss: 0.053\n",
      "[83,    80] loss: 0.053\n",
      "[84,    80] loss: 0.053\n",
      "[85,    80] loss: 0.053\n",
      "[86,    80] loss: 0.053\n",
      "[87,    80] loss: 0.053\n",
      "[88,    80] loss: 0.053\n",
      "[89,    80] loss: 0.053\n",
      "[90,    80] loss: 0.053\n",
      "[91,    80] loss: 0.053\n",
      "[92,    80] loss: 0.053\n",
      "[93,    80] loss: 0.053\n",
      "[94,    80] loss: 0.053\n",
      "[95,    80] loss: 0.053\n",
      "[96,    80] loss: 0.053\n",
      "[97,    80] loss: 0.053\n",
      "[98,    80] loss: 0.053\n",
      "[99,    80] loss: 0.053\n",
      "[100,    80] loss: 0.053\n",
      "Finished Training\n",
      "[*] Visualizing an example's output...\n",
      "tensor([[0.0000, 0.0000, 0.8017, 0.0000, 0.0000, 0.7311, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.8345, 0.6362, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.6263, 0.6513, 0.6508, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.7716, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7916,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6765, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6593,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6230, 0.0000]])\n",
      "tensor([[0.1841, 0.0102, 0.2148, 0.1999, 0.1805, 0.2075, 0.2017, 0.0151, 0.2225,\n",
      "         0.2011, 0.0168, 0.0100, 0.2001, 0.2038, 0.2100, 0.2584, 0.0096, 0.1920,\n",
      "         0.0171, 0.0179, 0.0243, 0.0248, 0.0215, 0.2109, 0.0154, 0.1887, 0.0144,\n",
      "         0.0159, 0.0206, 0.2146, 0.2237, 0.2157, 0.0218, 0.2573, 0.1956, 0.1910,\n",
      "         0.1869, 0.0272, 0.2200, 0.2241, 0.0213, 0.1854, 0.0148, 0.2236, 0.2115,\n",
      "         0.0116, 0.0116, 0.0175, 0.0228, 0.0122, 0.2053, 0.0096, 0.2092, 0.0168,\n",
      "         0.0226, 0.2325, 0.1982, 0.0227, 0.1904, 0.0181, 0.0145, 0.2280, 0.2257,\n",
      "         0.1755, 0.2090, 0.0181, 0.0123, 0.0241, 0.0110, 0.0088, 0.0286, 0.2109,\n",
      "         0.0078, 0.0169, 0.0100, 0.0113, 0.2335, 0.0214, 0.2428, 0.1634]],\n",
      "       grad_fn=<SigmoidBackward>)\n",
      "0.051980935\n",
      "[*] Getting the embeddings and visualizing t-SNE...\n",
      "[tensor([0]), tensor([0])]\n",
      "[tensor([1]), tensor([1])]\n",
      "[tensor([2]), tensor([2])]\n",
      "[tensor([3]), tensor([3])]\n",
      "[tensor([4]), tensor([4])]\n",
      "[tensor([5]), tensor([5])]\n",
      "[tensor([6]), tensor([6])]\n",
      "[tensor([7]), tensor([7])]\n",
      "[tensor([8]), tensor([8])]\n",
      "[tensor([9]), tensor([9])]\n",
      "[tensor([10]), tensor([10])]\n",
      "[tensor([11]), tensor([11])]\n",
      "[tensor([12]), tensor([12])]\n",
      "[tensor([13]), tensor([13])]\n",
      "[tensor([14]), tensor([14])]\n",
      "[tensor([15]), tensor([15])]\n",
      "[tensor([16]), tensor([16])]\n",
      "[tensor([17]), tensor([17])]\n",
      "[tensor([18]), tensor([18])]\n",
      "[tensor([19]), tensor([19])]\n",
      "[tensor([20]), tensor([20])]\n",
      "[tensor([21]), tensor([21])]\n",
      "[tensor([22]), tensor([22])]\n",
      "[tensor([23]), tensor([23])]\n",
      "[tensor([24]), tensor([24])]\n",
      "[tensor([25]), tensor([25])]\n",
      "[tensor([26]), tensor([26])]\n",
      "[tensor([27]), tensor([27])]\n",
      "[tensor([28]), tensor([28])]\n",
      "[tensor([29]), tensor([29])]\n",
      "[tensor([30]), tensor([30])]\n",
      "[tensor([31]), tensor([31])]\n",
      "[tensor([32]), tensor([32])]\n",
      "[tensor([33]), tensor([33])]\n",
      "[tensor([34]), tensor([34])]\n",
      "[tensor([35]), tensor([35])]\n",
      "[tensor([36]), tensor([36])]\n",
      "[tensor([37]), tensor([37])]\n",
      "[tensor([38]), tensor([38])]\n",
      "[tensor([39]), tensor([39])]\n",
      "[tensor([40]), tensor([40])]\n",
      "[tensor([41]), tensor([41])]\n",
      "[tensor([42]), tensor([42])]\n",
      "[tensor([43]), tensor([43])]\n",
      "[tensor([44]), tensor([44])]\n",
      "[tensor([45]), tensor([45])]\n",
      "[tensor([46]), tensor([46])]\n",
      "[tensor([47]), tensor([47])]\n",
      "[tensor([48]), tensor([48])]\n",
      "[tensor([49]), tensor([49])]\n",
      "[tensor([50]), tensor([50])]\n",
      "[tensor([51]), tensor([51])]\n",
      "[tensor([52]), tensor([52])]\n",
      "[tensor([53]), tensor([53])]\n",
      "[tensor([54]), tensor([54])]\n",
      "[tensor([55]), tensor([55])]\n",
      "[tensor([56]), tensor([56])]\n",
      "[tensor([57]), tensor([57])]\n",
      "[tensor([58]), tensor([58])]\n",
      "[tensor([59]), tensor([59])]\n",
      "[tensor([60]), tensor([60])]\n",
      "[tensor([61]), tensor([61])]\n",
      "[tensor([62]), tensor([62])]\n",
      "[tensor([63]), tensor([63])]\n",
      "[tensor([64]), tensor([64])]\n",
      "[tensor([65]), tensor([65])]\n",
      "[tensor([66]), tensor([66])]\n",
      "[tensor([67]), tensor([67])]\n",
      "[tensor([68]), tensor([68])]\n",
      "[tensor([69]), tensor([69])]\n",
      "[tensor([70]), tensor([70])]\n",
      "[tensor([71]), tensor([71])]\n",
      "[tensor([72]), tensor([72])]\n",
      "[tensor([73]), tensor([73])]\n",
      "[tensor([74]), tensor([74])]\n",
      "[tensor([75]), tensor([75])]\n",
      "[tensor([76]), tensor([76])]\n",
      "[tensor([77]), tensor([77])]\n",
      "[tensor([78]), tensor([78])]\n",
      "[tensor([79]), tensor([79])]\n",
      "'dngr_pipeline'  8601.91 ms\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings_drugs, _, _ = dngr_pipeline(dd_net, N_rand_drugs, [60, 40, 20], n_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../src/dgnr.py:92: RuntimeWarning: divide by zero encountered in log\n",
      "  P = np.log(P)\n",
      "/home/michael/.local/lib/python3.6/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([1, 100])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.134\n",
      "[2,   100] loss: 0.116\n",
      "[3,   100] loss: 0.116\n",
      "[4,   100] loss: 0.116\n",
      "[5,   100] loss: 0.116\n",
      "[6,   100] loss: 0.113\n",
      "[7,   100] loss: 0.097\n",
      "[8,   100] loss: 0.076\n",
      "[9,   100] loss: 0.070\n",
      "[10,   100] loss: 0.068\n",
      "[11,   100] loss: 0.067\n",
      "[12,   100] loss: 0.067\n",
      "[13,   100] loss: 0.066\n",
      "[14,   100] loss: 0.066\n",
      "[15,   100] loss: 0.066\n",
      "[16,   100] loss: 0.066\n",
      "[17,   100] loss: 0.066\n",
      "[18,   100] loss: 0.066\n",
      "[19,   100] loss: 0.066\n",
      "[20,   100] loss: 0.066\n",
      "[21,   100] loss: 0.066\n",
      "[22,   100] loss: 0.065\n",
      "[23,   100] loss: 0.065\n",
      "[24,   100] loss: 0.065\n",
      "[25,   100] loss: 0.065\n",
      "[26,   100] loss: 0.065\n",
      "[27,   100] loss: 0.065\n",
      "[28,   100] loss: 0.065\n",
      "[29,   100] loss: 0.065\n",
      "[30,   100] loss: 0.065\n",
      "[31,   100] loss: 0.065\n",
      "[32,   100] loss: 0.065\n",
      "[33,   100] loss: 0.065\n",
      "[34,   100] loss: 0.065\n",
      "[35,   100] loss: 0.065\n",
      "[36,   100] loss: 0.065\n",
      "[37,   100] loss: 0.065\n",
      "[38,   100] loss: 0.065\n",
      "[39,   100] loss: 0.065\n",
      "[40,   100] loss: 0.065\n",
      "[41,   100] loss: 0.065\n",
      "[42,   100] loss: 0.065\n",
      "[43,   100] loss: 0.065\n",
      "[44,   100] loss: 0.065\n",
      "[45,   100] loss: 0.065\n",
      "[46,   100] loss: 0.065\n",
      "[47,   100] loss: 0.065\n",
      "[48,   100] loss: 0.065\n",
      "[49,   100] loss: 0.065\n",
      "[50,   100] loss: 0.065\n",
      "[51,   100] loss: 0.065\n",
      "[52,   100] loss: 0.065\n",
      "[53,   100] loss: 0.065\n",
      "[54,   100] loss: 0.065\n",
      "[55,   100] loss: 0.065\n",
      "[56,   100] loss: 0.065\n",
      "[57,   100] loss: 0.065\n",
      "[58,   100] loss: 0.065\n",
      "[59,   100] loss: 0.065\n",
      "[60,   100] loss: 0.065\n",
      "[61,   100] loss: 0.065\n",
      "[62,   100] loss: 0.065\n",
      "[63,   100] loss: 0.065\n",
      "[64,   100] loss: 0.065\n",
      "[65,   100] loss: 0.065\n",
      "[66,   100] loss: 0.065\n",
      "[67,   100] loss: 0.065\n",
      "[68,   100] loss: 0.065\n",
      "[69,   100] loss: 0.065\n",
      "[70,   100] loss: 0.065\n",
      "[71,   100] loss: 0.065\n",
      "[72,   100] loss: 0.065\n",
      "[73,   100] loss: 0.065\n",
      "[74,   100] loss: 0.065\n",
      "[75,   100] loss: 0.065\n",
      "[76,   100] loss: 0.065\n",
      "[77,   100] loss: 0.065\n",
      "[78,   100] loss: 0.065\n",
      "[79,   100] loss: 0.065\n",
      "[80,   100] loss: 0.065\n",
      "[81,   100] loss: 0.065\n",
      "[82,   100] loss: 0.065\n",
      "[83,   100] loss: 0.065\n",
      "[84,   100] loss: 0.065\n",
      "[85,   100] loss: 0.065\n",
      "[86,   100] loss: 0.064\n",
      "[87,   100] loss: 0.064\n",
      "[88,   100] loss: 0.064\n",
      "[89,   100] loss: 0.064\n",
      "[90,   100] loss: 0.064\n",
      "[91,   100] loss: 0.064\n",
      "[92,   100] loss: 0.064\n",
      "[93,   100] loss: 0.064\n",
      "[94,   100] loss: 0.064\n",
      "[95,   100] loss: 0.064\n",
      "[96,   100] loss: 0.064\n",
      "[97,   100] loss: 0.064\n",
      "[98,   100] loss: 0.064\n",
      "[99,   100] loss: 0.064\n",
      "[100,   100] loss: 0.064\n",
      "[101,   100] loss: 0.064\n",
      "[102,   100] loss: 0.064\n",
      "[103,   100] loss: 0.064\n",
      "[104,   100] loss: 0.064\n",
      "[105,   100] loss: 0.064\n",
      "[106,   100] loss: 0.064\n",
      "[107,   100] loss: 0.064\n",
      "[108,   100] loss: 0.064\n",
      "[109,   100] loss: 0.063\n",
      "[110,   100] loss: 0.063\n",
      "[111,   100] loss: 0.063\n",
      "[112,   100] loss: 0.063\n",
      "[113,   100] loss: 0.063\n",
      "[114,   100] loss: 0.063\n",
      "[115,   100] loss: 0.062\n",
      "[116,   100] loss: 0.062\n",
      "[117,   100] loss: 0.062\n",
      "[118,   100] loss: 0.062\n",
      "[119,   100] loss: 0.061\n",
      "[120,   100] loss: 0.061\n",
      "[121,   100] loss: 0.061\n",
      "[122,   100] loss: 0.061\n",
      "[123,   100] loss: 0.061\n",
      "[124,   100] loss: 0.061\n",
      "[125,   100] loss: 0.060\n",
      "[126,   100] loss: 0.060\n",
      "[127,   100] loss: 0.060\n",
      "[128,   100] loss: 0.060\n",
      "[129,   100] loss: 0.060\n",
      "[130,   100] loss: 0.060\n",
      "[131,   100] loss: 0.060\n",
      "[132,   100] loss: 0.060\n",
      "[133,   100] loss: 0.060\n",
      "[134,   100] loss: 0.060\n",
      "[135,   100] loss: 0.059\n",
      "[136,   100] loss: 0.059\n",
      "[137,   100] loss: 0.059\n",
      "[138,   100] loss: 0.059\n",
      "[139,   100] loss: 0.059\n",
      "[140,   100] loss: 0.059\n",
      "[141,   100] loss: 0.059\n",
      "[142,   100] loss: 0.059\n",
      "[143,   100] loss: 0.059\n",
      "[144,   100] loss: 0.059\n",
      "[145,   100] loss: 0.059\n",
      "[146,   100] loss: 0.060\n",
      "[147,   100] loss: 0.059\n",
      "[148,   100] loss: 0.059\n",
      "[149,   100] loss: 0.059\n",
      "[150,   100] loss: 0.059\n",
      "[151,   100] loss: 0.059\n",
      "[152,   100] loss: 0.059\n",
      "[153,   100] loss: 0.059\n",
      "[154,   100] loss: 0.058\n",
      "[155,   100] loss: 0.058\n",
      "[156,   100] loss: 0.058\n",
      "[157,   100] loss: 0.058\n",
      "[158,   100] loss: 0.058\n",
      "[159,   100] loss: 0.058\n",
      "[160,   100] loss: 0.058\n",
      "[161,   100] loss: 0.058\n",
      "[162,   100] loss: 0.058\n",
      "[163,   100] loss: 0.058\n",
      "[164,   100] loss: 0.058\n",
      "[165,   100] loss: 0.058\n",
      "[166,   100] loss: 0.058\n",
      "[167,   100] loss: 0.058\n",
      "[168,   100] loss: 0.058\n",
      "[169,   100] loss: 0.058\n",
      "[170,   100] loss: 0.058\n",
      "[171,   100] loss: 0.058\n",
      "[172,   100] loss: 0.058\n",
      "[173,   100] loss: 0.057\n",
      "[174,   100] loss: 0.057\n",
      "[175,   100] loss: 0.057\n",
      "[176,   100] loss: 0.057\n",
      "[177,   100] loss: 0.057\n",
      "[178,   100] loss: 0.057\n",
      "[179,   100] loss: 0.057\n",
      "[180,   100] loss: 0.057\n",
      "[181,   100] loss: 0.057\n",
      "[182,   100] loss: 0.057\n",
      "[183,   100] loss: 0.057\n",
      "[184,   100] loss: 0.057\n",
      "[185,   100] loss: 0.057\n",
      "[186,   100] loss: 0.057\n",
      "[187,   100] loss: 0.057\n",
      "[188,   100] loss: 0.057\n",
      "[189,   100] loss: 0.057\n",
      "[190,   100] loss: 0.057\n",
      "[191,   100] loss: 0.057\n",
      "[192,   100] loss: 0.057\n",
      "[193,   100] loss: 0.057\n",
      "[194,   100] loss: 0.057\n",
      "[195,   100] loss: 0.057\n",
      "[196,   100] loss: 0.057\n",
      "[197,   100] loss: 0.057\n",
      "[198,   100] loss: 0.057\n",
      "[199,   100] loss: 0.056\n",
      "[200,   100] loss: 0.056\n",
      "[201,   100] loss: 0.056\n",
      "[202,   100] loss: 0.056\n",
      "[203,   100] loss: 0.056\n",
      "[204,   100] loss: 0.056\n",
      "[205,   100] loss: 0.056\n",
      "[206,   100] loss: 0.056\n",
      "[207,   100] loss: 0.056\n",
      "[208,   100] loss: 0.056\n",
      "[209,   100] loss: 0.056\n",
      "[210,   100] loss: 0.056\n",
      "[211,   100] loss: 0.056\n",
      "[212,   100] loss: 0.056\n",
      "[213,   100] loss: 0.056\n",
      "[214,   100] loss: 0.056\n",
      "[215,   100] loss: 0.056\n",
      "[216,   100] loss: 0.056\n",
      "[217,   100] loss: 0.056\n",
      "[218,   100] loss: 0.056\n",
      "[219,   100] loss: 0.056\n",
      "[220,   100] loss: 0.056\n",
      "[221,   100] loss: 0.056\n",
      "[222,   100] loss: 0.055\n",
      "[223,   100] loss: 0.055\n",
      "[224,   100] loss: 0.056\n",
      "[225,   100] loss: 0.056\n",
      "[226,   100] loss: 0.056\n",
      "[227,   100] loss: 0.055\n",
      "[228,   100] loss: 0.055\n",
      "[229,   100] loss: 0.055\n",
      "[230,   100] loss: 0.055\n",
      "[231,   100] loss: 0.055\n",
      "[232,   100] loss: 0.055\n",
      "[233,   100] loss: 0.055\n",
      "[234,   100] loss: 0.055\n",
      "[235,   100] loss: 0.055\n",
      "[236,   100] loss: 0.055\n",
      "[237,   100] loss: 0.055\n",
      "[238,   100] loss: 0.055\n",
      "[239,   100] loss: 0.055\n",
      "[240,   100] loss: 0.055\n",
      "[241,   100] loss: 0.055\n",
      "[242,   100] loss: 0.055\n",
      "[243,   100] loss: 0.055\n",
      "[244,   100] loss: 0.055\n",
      "[245,   100] loss: 0.055\n",
      "[246,   100] loss: 0.055\n",
      "[247,   100] loss: 0.055\n",
      "[248,   100] loss: 0.055\n",
      "[249,   100] loss: 0.055\n",
      "[250,   100] loss: 0.055\n",
      "[251,   100] loss: 0.055\n",
      "[252,   100] loss: 0.054\n",
      "[253,   100] loss: 0.054\n",
      "[254,   100] loss: 0.054\n",
      "[255,   100] loss: 0.054\n",
      "[256,   100] loss: 0.054\n",
      "[257,   100] loss: 0.054\n",
      "[258,   100] loss: 0.054\n",
      "[259,   100] loss: 0.054\n",
      "[260,   100] loss: 0.054\n",
      "[261,   100] loss: 0.054\n",
      "[262,   100] loss: 0.054\n",
      "[263,   100] loss: 0.054\n",
      "[264,   100] loss: 0.054\n",
      "[265,   100] loss: 0.054\n",
      "[266,   100] loss: 0.054\n",
      "[267,   100] loss: 0.054\n",
      "[268,   100] loss: 0.054\n",
      "[269,   100] loss: 0.054\n",
      "[270,   100] loss: 0.054\n",
      "[271,   100] loss: 0.054\n",
      "[272,   100] loss: 0.054\n",
      "[273,   100] loss: 0.054\n",
      "[274,   100] loss: 0.054\n",
      "[275,   100] loss: 0.054\n",
      "[276,   100] loss: 0.054\n",
      "[277,   100] loss: 0.054\n",
      "[278,   100] loss: 0.054\n",
      "[279,   100] loss: 0.054\n",
      "[280,   100] loss: 0.054\n",
      "[281,   100] loss: 0.054\n",
      "[282,   100] loss: 0.054\n",
      "[283,   100] loss: 0.054\n",
      "[284,   100] loss: 0.054\n",
      "[285,   100] loss: 0.054\n",
      "[286,   100] loss: 0.054\n",
      "[287,   100] loss: 0.053\n",
      "[288,   100] loss: 0.053\n",
      "[289,   100] loss: 0.053\n",
      "[290,   100] loss: 0.053\n",
      "[291,   100] loss: 0.053\n",
      "[292,   100] loss: 0.053\n",
      "[293,   100] loss: 0.053\n",
      "[294,   100] loss: 0.053\n",
      "[295,   100] loss: 0.053\n",
      "[296,   100] loss: 0.053\n",
      "[297,   100] loss: 0.053\n",
      "[298,   100] loss: 0.053\n",
      "[299,   100] loss: 0.053\n",
      "[300,   100] loss: 0.053\n",
      "[301,   100] loss: 0.053\n",
      "[302,   100] loss: 0.054\n",
      "[303,   100] loss: 0.054\n",
      "[304,   100] loss: 0.054\n",
      "[305,   100] loss: 0.053\n",
      "[306,   100] loss: 0.053\n",
      "[307,   100] loss: 0.053\n",
      "[308,   100] loss: 0.053\n",
      "[309,   100] loss: 0.053\n",
      "[310,   100] loss: 0.053\n",
      "[311,   100] loss: 0.053\n",
      "[312,   100] loss: 0.053\n",
      "[313,   100] loss: 0.053\n",
      "[314,   100] loss: 0.052\n",
      "[315,   100] loss: 0.052\n",
      "[316,   100] loss: 0.052\n",
      "[317,   100] loss: 0.052\n",
      "[318,   100] loss: 0.052\n",
      "[319,   100] loss: 0.052\n",
      "[320,   100] loss: 0.052\n",
      "[321,   100] loss: 0.052\n",
      "[322,   100] loss: 0.052\n",
      "[323,   100] loss: 0.052\n",
      "[324,   100] loss: 0.052\n",
      "[325,   100] loss: 0.052\n",
      "[326,   100] loss: 0.052\n",
      "[327,   100] loss: 0.052\n",
      "[328,   100] loss: 0.052\n",
      "[329,   100] loss: 0.052\n",
      "[330,   100] loss: 0.053\n",
      "[331,   100] loss: 0.052\n",
      "[332,   100] loss: 0.053\n",
      "[333,   100] loss: 0.053\n",
      "[334,   100] loss: 0.053\n",
      "[335,   100] loss: 0.052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[336,   100] loss: 0.052\n",
      "[337,   100] loss: 0.052\n",
      "[338,   100] loss: 0.052\n",
      "[339,   100] loss: 0.052\n",
      "[340,   100] loss: 0.052\n",
      "[341,   100] loss: 0.052\n",
      "[342,   100] loss: 0.052\n",
      "[343,   100] loss: 0.052\n",
      "[344,   100] loss: 0.052\n",
      "[345,   100] loss: 0.052\n",
      "[346,   100] loss: 0.052\n",
      "[347,   100] loss: 0.052\n",
      "[348,   100] loss: 0.051\n",
      "[349,   100] loss: 0.051\n",
      "[350,   100] loss: 0.051\n",
      "[351,   100] loss: 0.052\n",
      "[352,   100] loss: 0.052\n",
      "[353,   100] loss: 0.052\n",
      "[354,   100] loss: 0.051\n",
      "[355,   100] loss: 0.051\n",
      "[356,   100] loss: 0.052\n",
      "[357,   100] loss: 0.052\n",
      "[358,   100] loss: 0.052\n",
      "[359,   100] loss: 0.052\n",
      "[360,   100] loss: 0.052\n",
      "[361,   100] loss: 0.052\n",
      "[362,   100] loss: 0.051\n",
      "[363,   100] loss: 0.051\n",
      "[364,   100] loss: 0.051\n",
      "[365,   100] loss: 0.051\n",
      "[366,   100] loss: 0.051\n",
      "[367,   100] loss: 0.051\n",
      "[368,   100] loss: 0.051\n",
      "[369,   100] loss: 0.051\n",
      "[370,   100] loss: 0.051\n",
      "[371,   100] loss: 0.051\n",
      "[372,   100] loss: 0.051\n",
      "[373,   100] loss: 0.051\n",
      "[374,   100] loss: 0.051\n",
      "[375,   100] loss: 0.051\n",
      "[376,   100] loss: 0.051\n",
      "[377,   100] loss: 0.051\n",
      "[378,   100] loss: 0.051\n",
      "[379,   100] loss: 0.051\n",
      "[380,   100] loss: 0.051\n",
      "[381,   100] loss: 0.051\n",
      "[382,   100] loss: 0.052\n",
      "[383,   100] loss: 0.051\n",
      "[384,   100] loss: 0.051\n",
      "[385,   100] loss: 0.051\n",
      "[386,   100] loss: 0.051\n",
      "[387,   100] loss: 0.051\n",
      "[388,   100] loss: 0.051\n",
      "[389,   100] loss: 0.051\n",
      "[390,   100] loss: 0.051\n",
      "[391,   100] loss: 0.051\n",
      "[392,   100] loss: 0.051\n",
      "[393,   100] loss: 0.051\n",
      "[394,   100] loss: 0.051\n",
      "[395,   100] loss: 0.051\n",
      "[396,   100] loss: 0.051\n",
      "[397,   100] loss: 0.051\n",
      "[398,   100] loss: 0.050\n",
      "[399,   100] loss: 0.050\n",
      "[400,   100] loss: 0.051\n",
      "[401,   100] loss: 0.051\n",
      "[402,   100] loss: 0.050\n",
      "[403,   100] loss: 0.050\n",
      "[404,   100] loss: 0.050\n",
      "[405,   100] loss: 0.050\n",
      "[406,   100] loss: 0.050\n",
      "[407,   100] loss: 0.050\n",
      "[408,   100] loss: 0.050\n",
      "[409,   100] loss: 0.051\n",
      "[410,   100] loss: 0.051\n",
      "[411,   100] loss: 0.051\n",
      "[412,   100] loss: 0.050\n",
      "[413,   100] loss: 0.050\n",
      "[414,   100] loss: 0.050\n",
      "[415,   100] loss: 0.050\n",
      "[416,   100] loss: 0.050\n",
      "[417,   100] loss: 0.050\n",
      "[418,   100] loss: 0.050\n",
      "[419,   100] loss: 0.050\n",
      "[420,   100] loss: 0.050\n",
      "[421,   100] loss: 0.050\n",
      "[422,   100] loss: 0.050\n",
      "[423,   100] loss: 0.050\n",
      "[424,   100] loss: 0.050\n",
      "[425,   100] loss: 0.050\n",
      "[426,   100] loss: 0.050\n",
      "[427,   100] loss: 0.050\n",
      "[428,   100] loss: 0.050\n",
      "[429,   100] loss: 0.050\n",
      "[430,   100] loss: 0.050\n",
      "[431,   100] loss: 0.050\n",
      "[432,   100] loss: 0.050\n",
      "[433,   100] loss: 0.050\n",
      "[434,   100] loss: 0.050\n",
      "[435,   100] loss: 0.050\n",
      "[436,   100] loss: 0.050\n",
      "[437,   100] loss: 0.050\n",
      "[438,   100] loss: 0.050\n",
      "[439,   100] loss: 0.050\n",
      "[440,   100] loss: 0.050\n",
      "[441,   100] loss: 0.050\n",
      "[442,   100] loss: 0.050\n",
      "[443,   100] loss: 0.050\n",
      "[444,   100] loss: 0.050\n",
      "[445,   100] loss: 0.050\n",
      "[446,   100] loss: 0.050\n",
      "[447,   100] loss: 0.050\n",
      "[448,   100] loss: 0.050\n",
      "[449,   100] loss: 0.049\n",
      "[450,   100] loss: 0.049\n",
      "[451,   100] loss: 0.049\n",
      "[452,   100] loss: 0.049\n",
      "[453,   100] loss: 0.049\n",
      "[454,   100] loss: 0.049\n",
      "[455,   100] loss: 0.050\n",
      "[456,   100] loss: 0.050\n",
      "[457,   100] loss: 0.049\n",
      "[458,   100] loss: 0.049\n",
      "[459,   100] loss: 0.049\n",
      "[460,   100] loss: 0.049\n",
      "[461,   100] loss: 0.049\n",
      "[462,   100] loss: 0.049\n",
      "[463,   100] loss: 0.049\n",
      "[464,   100] loss: 0.049\n",
      "[465,   100] loss: 0.049\n",
      "[466,   100] loss: 0.049\n",
      "[467,   100] loss: 0.049\n",
      "[468,   100] loss: 0.049\n",
      "[469,   100] loss: 0.049\n",
      "[470,   100] loss: 0.049\n",
      "[471,   100] loss: 0.049\n",
      "[472,   100] loss: 0.049\n",
      "[473,   100] loss: 0.049\n",
      "[474,   100] loss: 0.049\n",
      "[475,   100] loss: 0.049\n",
      "[476,   100] loss: 0.049\n",
      "[477,   100] loss: 0.049\n",
      "[478,   100] loss: 0.049\n",
      "[479,   100] loss: 0.049\n",
      "[480,   100] loss: 0.049\n",
      "[481,   100] loss: 0.049\n",
      "[482,   100] loss: 0.049\n",
      "[483,   100] loss: 0.049\n",
      "[484,   100] loss: 0.049\n",
      "[485,   100] loss: 0.049\n",
      "[486,   100] loss: 0.050\n",
      "[487,   100] loss: 0.049\n",
      "[488,   100] loss: 0.049\n",
      "[489,   100] loss: 0.049\n",
      "[490,   100] loss: 0.049\n",
      "[491,   100] loss: 0.049\n",
      "[492,   100] loss: 0.049\n",
      "[493,   100] loss: 0.049\n",
      "[494,   100] loss: 0.048\n",
      "[495,   100] loss: 0.048\n",
      "[496,   100] loss: 0.048\n",
      "[497,   100] loss: 0.049\n",
      "[498,   100] loss: 0.048\n",
      "[499,   100] loss: 0.049\n",
      "[500,   100] loss: 0.048\n",
      "[501,   100] loss: 0.048\n",
      "[502,   100] loss: 0.048\n",
      "[503,   100] loss: 0.049\n",
      "[504,   100] loss: 0.048\n",
      "[505,   100] loss: 0.049\n",
      "[506,   100] loss: 0.048\n",
      "[507,   100] loss: 0.048\n",
      "[508,   100] loss: 0.048\n",
      "[509,   100] loss: 0.048\n",
      "[510,   100] loss: 0.048\n",
      "[511,   100] loss: 0.048\n",
      "[512,   100] loss: 0.049\n",
      "[513,   100] loss: 0.049\n",
      "[514,   100] loss: 0.049\n",
      "[515,   100] loss: 0.049\n",
      "[516,   100] loss: 0.049\n",
      "[517,   100] loss: 0.048\n",
      "[518,   100] loss: 0.048\n",
      "[519,   100] loss: 0.048\n",
      "[520,   100] loss: 0.048\n",
      "[521,   100] loss: 0.048\n",
      "[522,   100] loss: 0.048\n",
      "[523,   100] loss: 0.048\n",
      "[524,   100] loss: 0.048\n",
      "[525,   100] loss: 0.048\n",
      "[526,   100] loss: 0.048\n",
      "[527,   100] loss: 0.048\n",
      "[528,   100] loss: 0.048\n",
      "[529,   100] loss: 0.048\n",
      "[530,   100] loss: 0.048\n",
      "[531,   100] loss: 0.048\n",
      "[532,   100] loss: 0.048\n",
      "[533,   100] loss: 0.049\n",
      "[534,   100] loss: 0.048\n",
      "[535,   100] loss: 0.048\n",
      "[536,   100] loss: 0.048\n",
      "[537,   100] loss: 0.048\n",
      "[538,   100] loss: 0.048\n",
      "[539,   100] loss: 0.048\n",
      "[540,   100] loss: 0.048\n",
      "[541,   100] loss: 0.048\n",
      "[542,   100] loss: 0.048\n",
      "[543,   100] loss: 0.048\n",
      "[544,   100] loss: 0.048\n",
      "[545,   100] loss: 0.047\n",
      "[546,   100] loss: 0.048\n",
      "[547,   100] loss: 0.048\n",
      "[548,   100] loss: 0.048\n",
      "[549,   100] loss: 0.048\n",
      "[550,   100] loss: 0.048\n",
      "[551,   100] loss: 0.048\n",
      "[552,   100] loss: 0.048\n",
      "[553,   100] loss: 0.048\n",
      "[554,   100] loss: 0.048\n",
      "[555,   100] loss: 0.048\n",
      "[556,   100] loss: 0.048\n",
      "[557,   100] loss: 0.048\n",
      "[558,   100] loss: 0.048\n",
      "[559,   100] loss: 0.047\n",
      "[560,   100] loss: 0.047\n",
      "[561,   100] loss: 0.047\n",
      "[562,   100] loss: 0.047\n",
      "[563,   100] loss: 0.047\n",
      "[564,   100] loss: 0.047\n",
      "[565,   100] loss: 0.047\n",
      "[566,   100] loss: 0.047\n",
      "[567,   100] loss: 0.047\n",
      "[568,   100] loss: 0.047\n",
      "[569,   100] loss: 0.047\n",
      "[570,   100] loss: 0.048\n",
      "[571,   100] loss: 0.048\n",
      "[572,   100] loss: 0.047\n",
      "[573,   100] loss: 0.047\n",
      "[574,   100] loss: 0.047\n",
      "[575,   100] loss: 0.047\n",
      "[576,   100] loss: 0.047\n",
      "[577,   100] loss: 0.047\n",
      "[578,   100] loss: 0.047\n",
      "[579,   100] loss: 0.048\n",
      "[580,   100] loss: 0.047\n",
      "[581,   100] loss: 0.047\n",
      "[582,   100] loss: 0.047\n",
      "[583,   100] loss: 0.047\n",
      "[584,   100] loss: 0.047\n",
      "[585,   100] loss: 0.047\n",
      "[586,   100] loss: 0.047\n",
      "[587,   100] loss: 0.047\n",
      "[588,   100] loss: 0.047\n",
      "[589,   100] loss: 0.047\n",
      "[590,   100] loss: 0.047\n",
      "[591,   100] loss: 0.048\n",
      "[592,   100] loss: 0.047\n",
      "[593,   100] loss: 0.047\n",
      "[594,   100] loss: 0.047\n",
      "[595,   100] loss: 0.047\n",
      "[596,   100] loss: 0.047\n",
      "[597,   100] loss: 0.047\n",
      "[598,   100] loss: 0.047\n",
      "[599,   100] loss: 0.048\n",
      "[600,   100] loss: 0.047\n",
      "[601,   100] loss: 0.047\n",
      "[602,   100] loss: 0.047\n",
      "[603,   100] loss: 0.047\n",
      "[604,   100] loss: 0.046\n",
      "[605,   100] loss: 0.046\n",
      "[606,   100] loss: 0.047\n",
      "[607,   100] loss: 0.047\n",
      "[608,   100] loss: 0.046\n",
      "[609,   100] loss: 0.046\n",
      "[610,   100] loss: 0.046\n",
      "[611,   100] loss: 0.047\n",
      "[612,   100] loss: 0.047\n",
      "[613,   100] loss: 0.047\n",
      "[614,   100] loss: 0.046\n",
      "[615,   100] loss: 0.046\n",
      "[616,   100] loss: 0.047\n",
      "[617,   100] loss: 0.046\n",
      "[618,   100] loss: 0.046\n",
      "[619,   100] loss: 0.047\n",
      "[620,   100] loss: 0.046\n",
      "[621,   100] loss: 0.046\n",
      "[622,   100] loss: 0.046\n",
      "[623,   100] loss: 0.046\n",
      "[624,   100] loss: 0.046\n",
      "[625,   100] loss: 0.046\n",
      "[626,   100] loss: 0.046\n",
      "[627,   100] loss: 0.046\n",
      "[628,   100] loss: 0.046\n",
      "[629,   100] loss: 0.046\n",
      "[630,   100] loss: 0.046\n",
      "[631,   100] loss: 0.046\n",
      "[632,   100] loss: 0.046\n",
      "[633,   100] loss: 0.046\n",
      "[634,   100] loss: 0.046\n",
      "[635,   100] loss: 0.046\n",
      "[636,   100] loss: 0.046\n",
      "[637,   100] loss: 0.046\n",
      "[638,   100] loss: 0.046\n",
      "[639,   100] loss: 0.047\n",
      "[640,   100] loss: 0.047\n",
      "[641,   100] loss: 0.046\n",
      "[642,   100] loss: 0.046\n",
      "[643,   100] loss: 0.046\n",
      "[644,   100] loss: 0.046\n",
      "[645,   100] loss: 0.046\n",
      "[646,   100] loss: 0.046\n",
      "[647,   100] loss: 0.046\n",
      "[648,   100] loss: 0.046\n",
      "[649,   100] loss: 0.046\n",
      "[650,   100] loss: 0.046\n",
      "[651,   100] loss: 0.046\n",
      "[652,   100] loss: 0.046\n",
      "[653,   100] loss: 0.046\n",
      "[654,   100] loss: 0.046\n",
      "[655,   100] loss: 0.046\n",
      "[656,   100] loss: 0.046\n",
      "[657,   100] loss: 0.046\n",
      "[658,   100] loss: 0.046\n",
      "[659,   100] loss: 0.046\n",
      "[660,   100] loss: 0.046\n",
      "[661,   100] loss: 0.046\n",
      "[662,   100] loss: 0.046\n",
      "[663,   100] loss: 0.046\n",
      "[664,   100] loss: 0.046\n",
      "[665,   100] loss: 0.046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[666,   100] loss: 0.046\n",
      "[667,   100] loss: 0.046\n",
      "[668,   100] loss: 0.046\n",
      "[669,   100] loss: 0.045\n",
      "[670,   100] loss: 0.045\n",
      "[671,   100] loss: 0.045\n",
      "[672,   100] loss: 0.045\n",
      "[673,   100] loss: 0.045\n",
      "[674,   100] loss: 0.045\n",
      "[675,   100] loss: 0.045\n",
      "[676,   100] loss: 0.045\n",
      "[677,   100] loss: 0.045\n",
      "[678,   100] loss: 0.045\n",
      "[679,   100] loss: 0.045\n",
      "[680,   100] loss: 0.045\n",
      "[681,   100] loss: 0.045\n",
      "[682,   100] loss: 0.045\n",
      "[683,   100] loss: 0.045\n",
      "[684,   100] loss: 0.046\n",
      "[685,   100] loss: 0.046\n",
      "[686,   100] loss: 0.045\n",
      "[687,   100] loss: 0.045\n",
      "[688,   100] loss: 0.045\n",
      "[689,   100] loss: 0.045\n",
      "[690,   100] loss: 0.045\n",
      "[691,   100] loss: 0.045\n",
      "[692,   100] loss: 0.045\n",
      "[693,   100] loss: 0.045\n",
      "[694,   100] loss: 0.045\n",
      "[695,   100] loss: 0.045\n",
      "[696,   100] loss: 0.045\n",
      "[697,   100] loss: 0.045\n",
      "[698,   100] loss: 0.045\n",
      "[699,   100] loss: 0.045\n",
      "[700,   100] loss: 0.045\n",
      "[701,   100] loss: 0.045\n",
      "[702,   100] loss: 0.045\n",
      "[703,   100] loss: 0.045\n",
      "[704,   100] loss: 0.045\n",
      "[705,   100] loss: 0.045\n",
      "[706,   100] loss: 0.045\n",
      "[707,   100] loss: 0.045\n",
      "[708,   100] loss: 0.045\n",
      "[709,   100] loss: 0.045\n",
      "[710,   100] loss: 0.045\n",
      "[711,   100] loss: 0.045\n",
      "[712,   100] loss: 0.045\n",
      "[713,   100] loss: 0.045\n",
      "[714,   100] loss: 0.045\n",
      "[715,   100] loss: 0.045\n",
      "[716,   100] loss: 0.045\n",
      "[717,   100] loss: 0.045\n",
      "[718,   100] loss: 0.045\n",
      "[719,   100] loss: 0.045\n",
      "[720,   100] loss: 0.045\n",
      "[721,   100] loss: 0.045\n",
      "[722,   100] loss: 0.045\n",
      "[723,   100] loss: 0.045\n",
      "[724,   100] loss: 0.045\n",
      "[725,   100] loss: 0.045\n",
      "[726,   100] loss: 0.045\n",
      "[727,   100] loss: 0.045\n",
      "[728,   100] loss: 0.045\n",
      "[729,   100] loss: 0.045\n",
      "[730,   100] loss: 0.045\n",
      "[731,   100] loss: 0.045\n",
      "[732,   100] loss: 0.046\n",
      "[733,   100] loss: 0.045\n",
      "[734,   100] loss: 0.045\n",
      "[735,   100] loss: 0.045\n",
      "[736,   100] loss: 0.045\n",
      "[737,   100] loss: 0.045\n",
      "[738,   100] loss: 0.045\n",
      "[739,   100] loss: 0.045\n",
      "[740,   100] loss: 0.044\n",
      "[741,   100] loss: 0.044\n",
      "[742,   100] loss: 0.044\n",
      "[743,   100] loss: 0.044\n",
      "[744,   100] loss: 0.044\n",
      "[745,   100] loss: 0.044\n",
      "[746,   100] loss: 0.044\n",
      "[747,   100] loss: 0.044\n",
      "[748,   100] loss: 0.044\n",
      "[749,   100] loss: 0.044\n",
      "[750,   100] loss: 0.044\n",
      "[751,   100] loss: 0.044\n",
      "[752,   100] loss: 0.044\n",
      "[753,   100] loss: 0.044\n",
      "[754,   100] loss: 0.044\n",
      "[755,   100] loss: 0.045\n",
      "[756,   100] loss: 0.044\n",
      "[757,   100] loss: 0.044\n",
      "[758,   100] loss: 0.044\n",
      "[759,   100] loss: 0.044\n",
      "[760,   100] loss: 0.044\n",
      "[761,   100] loss: 0.044\n",
      "[762,   100] loss: 0.044\n",
      "[763,   100] loss: 0.045\n",
      "[764,   100] loss: 0.044\n",
      "[765,   100] loss: 0.044\n",
      "[766,   100] loss: 0.044\n",
      "[767,   100] loss: 0.044\n",
      "[768,   100] loss: 0.044\n",
      "[769,   100] loss: 0.044\n",
      "[770,   100] loss: 0.044\n",
      "[771,   100] loss: 0.044\n",
      "[772,   100] loss: 0.044\n",
      "[773,   100] loss: 0.044\n",
      "[774,   100] loss: 0.044\n",
      "[775,   100] loss: 0.044\n",
      "[776,   100] loss: 0.044\n",
      "[777,   100] loss: 0.044\n",
      "[778,   100] loss: 0.044\n",
      "[779,   100] loss: 0.044\n",
      "[780,   100] loss: 0.044\n",
      "[781,   100] loss: 0.044\n",
      "[782,   100] loss: 0.044\n",
      "[783,   100] loss: 0.044\n",
      "[784,   100] loss: 0.044\n",
      "[785,   100] loss: 0.044\n",
      "[786,   100] loss: 0.044\n",
      "[787,   100] loss: 0.044\n",
      "[788,   100] loss: 0.044\n",
      "[789,   100] loss: 0.044\n",
      "[790,   100] loss: 0.044\n",
      "[791,   100] loss: 0.045\n",
      "[792,   100] loss: 0.045\n",
      "[793,   100] loss: 0.045\n",
      "[794,   100] loss: 0.044\n",
      "[795,   100] loss: 0.044\n",
      "[796,   100] loss: 0.044\n",
      "[797,   100] loss: 0.044\n",
      "[798,   100] loss: 0.044\n",
      "[799,   100] loss: 0.044\n",
      "[800,   100] loss: 0.044\n",
      "[801,   100] loss: 0.044\n",
      "[802,   100] loss: 0.044\n",
      "[803,   100] loss: 0.043\n",
      "[804,   100] loss: 0.043\n",
      "[805,   100] loss: 0.043\n",
      "[806,   100] loss: 0.043\n",
      "[807,   100] loss: 0.043\n",
      "[808,   100] loss: 0.043\n",
      "[809,   100] loss: 0.044\n",
      "[810,   100] loss: 0.044\n",
      "[811,   100] loss: 0.044\n",
      "[812,   100] loss: 0.044\n",
      "[813,   100] loss: 0.043\n",
      "[814,   100] loss: 0.043\n",
      "[815,   100] loss: 0.043\n",
      "[816,   100] loss: 0.043\n",
      "[817,   100] loss: 0.043\n",
      "[818,   100] loss: 0.044\n",
      "[819,   100] loss: 0.043\n",
      "[820,   100] loss: 0.044\n",
      "[821,   100] loss: 0.044\n",
      "[822,   100] loss: 0.044\n",
      "[823,   100] loss: 0.044\n",
      "[824,   100] loss: 0.043\n",
      "[825,   100] loss: 0.043\n",
      "[826,   100] loss: 0.043\n",
      "[827,   100] loss: 0.045\n",
      "[828,   100] loss: 0.044\n",
      "[829,   100] loss: 0.043\n",
      "[830,   100] loss: 0.043\n",
      "[831,   100] loss: 0.043\n",
      "[832,   100] loss: 0.043\n",
      "[833,   100] loss: 0.043\n",
      "[834,   100] loss: 0.043\n",
      "[835,   100] loss: 0.043\n",
      "[836,   100] loss: 0.043\n",
      "[837,   100] loss: 0.043\n",
      "[838,   100] loss: 0.043\n",
      "[839,   100] loss: 0.043\n",
      "[840,   100] loss: 0.043\n",
      "[841,   100] loss: 0.043\n",
      "[842,   100] loss: 0.043\n",
      "[843,   100] loss: 0.044\n",
      "[844,   100] loss: 0.044\n",
      "[845,   100] loss: 0.044\n",
      "[846,   100] loss: 0.043\n",
      "[847,   100] loss: 0.044\n",
      "[848,   100] loss: 0.043\n",
      "[849,   100] loss: 0.043\n",
      "[850,   100] loss: 0.043\n",
      "[851,   100] loss: 0.043\n",
      "[852,   100] loss: 0.043\n",
      "[853,   100] loss: 0.043\n",
      "[854,   100] loss: 0.043\n",
      "[855,   100] loss: 0.043\n",
      "[856,   100] loss: 0.043\n",
      "[857,   100] loss: 0.043\n",
      "[858,   100] loss: 0.043\n",
      "[859,   100] loss: 0.043\n",
      "[860,   100] loss: 0.043\n",
      "[861,   100] loss: 0.043\n",
      "[862,   100] loss: 0.043\n",
      "[863,   100] loss: 0.043\n",
      "[864,   100] loss: 0.044\n",
      "[865,   100] loss: 0.044\n",
      "[866,   100] loss: 0.043\n",
      "[867,   100] loss: 0.044\n",
      "[868,   100] loss: 0.043\n",
      "[869,   100] loss: 0.043\n",
      "[870,   100] loss: 0.043\n",
      "[871,   100] loss: 0.043\n",
      "[872,   100] loss: 0.043\n",
      "[873,   100] loss: 0.043\n",
      "[874,   100] loss: 0.043\n",
      "[875,   100] loss: 0.043\n",
      "[876,   100] loss: 0.043\n",
      "[877,   100] loss: 0.043\n",
      "[878,   100] loss: 0.043\n",
      "[879,   100] loss: 0.043\n",
      "[880,   100] loss: 0.043\n",
      "[881,   100] loss: 0.043\n",
      "[882,   100] loss: 0.043\n",
      "[883,   100] loss: 0.043\n",
      "[884,   100] loss: 0.043\n",
      "[885,   100] loss: 0.043\n",
      "[886,   100] loss: 0.043\n",
      "[887,   100] loss: 0.042\n",
      "[888,   100] loss: 0.042\n",
      "[889,   100] loss: 0.042\n",
      "[890,   100] loss: 0.043\n",
      "[891,   100] loss: 0.043\n",
      "[892,   100] loss: 0.043\n",
      "[893,   100] loss: 0.043\n",
      "[894,   100] loss: 0.042\n",
      "[895,   100] loss: 0.042\n",
      "[896,   100] loss: 0.042\n",
      "[897,   100] loss: 0.042\n",
      "[898,   100] loss: 0.042\n",
      "[899,   100] loss: 0.043\n",
      "[900,   100] loss: 0.043\n",
      "[901,   100] loss: 0.042\n",
      "[902,   100] loss: 0.043\n",
      "[903,   100] loss: 0.042\n",
      "[904,   100] loss: 0.042\n",
      "[905,   100] loss: 0.042\n",
      "[906,   100] loss: 0.042\n",
      "[907,   100] loss: 0.042\n",
      "[908,   100] loss: 0.042\n",
      "[909,   100] loss: 0.042\n",
      "[910,   100] loss: 0.043\n",
      "[911,   100] loss: 0.043\n",
      "[912,   100] loss: 0.043\n",
      "[913,   100] loss: 0.043\n",
      "[914,   100] loss: 0.042\n",
      "[915,   100] loss: 0.042\n",
      "[916,   100] loss: 0.042\n",
      "[917,   100] loss: 0.042\n",
      "[918,   100] loss: 0.042\n",
      "[919,   100] loss: 0.042\n",
      "[920,   100] loss: 0.042\n",
      "[921,   100] loss: 0.042\n",
      "[922,   100] loss: 0.042\n",
      "[923,   100] loss: 0.042\n",
      "[924,   100] loss: 0.042\n",
      "[925,   100] loss: 0.042\n",
      "[926,   100] loss: 0.042\n",
      "[927,   100] loss: 0.043\n",
      "[928,   100] loss: 0.043\n",
      "[929,   100] loss: 0.043\n",
      "[930,   100] loss: 0.042\n",
      "[931,   100] loss: 0.042\n",
      "[932,   100] loss: 0.042\n",
      "[933,   100] loss: 0.042\n",
      "[934,   100] loss: 0.042\n",
      "[935,   100] loss: 0.042\n",
      "[936,   100] loss: 0.042\n",
      "[937,   100] loss: 0.042\n",
      "[938,   100] loss: 0.042\n",
      "[939,   100] loss: 0.042\n",
      "[940,   100] loss: 0.042\n",
      "[941,   100] loss: 0.042\n",
      "[942,   100] loss: 0.042\n",
      "[943,   100] loss: 0.042\n",
      "[944,   100] loss: 0.043\n",
      "[945,   100] loss: 0.043\n",
      "[946,   100] loss: 0.043\n",
      "[947,   100] loss: 0.042\n",
      "[948,   100] loss: 0.042\n",
      "[949,   100] loss: 0.041\n",
      "[950,   100] loss: 0.041\n",
      "[951,   100] loss: 0.041\n",
      "[952,   100] loss: 0.041\n",
      "[953,   100] loss: 0.041\n",
      "[954,   100] loss: 0.041\n",
      "[955,   100] loss: 0.041\n",
      "[956,   100] loss: 0.041\n",
      "[957,   100] loss: 0.041\n",
      "[958,   100] loss: 0.041\n",
      "[959,   100] loss: 0.042\n",
      "[960,   100] loss: 0.042\n",
      "[961,   100] loss: 0.042\n",
      "[962,   100] loss: 0.043\n",
      "[963,   100] loss: 0.044\n",
      "[964,   100] loss: 0.042\n",
      "[965,   100] loss: 0.041\n",
      "[966,   100] loss: 0.042\n",
      "[967,   100] loss: 0.041\n",
      "[968,   100] loss: 0.041\n",
      "[969,   100] loss: 0.041\n",
      "[970,   100] loss: 0.041\n",
      "[971,   100] loss: 0.041\n",
      "[972,   100] loss: 0.041\n",
      "[973,   100] loss: 0.041\n",
      "[974,   100] loss: 0.041\n",
      "[975,   100] loss: 0.041\n",
      "[976,   100] loss: 0.041\n",
      "[977,   100] loss: 0.041\n",
      "[978,   100] loss: 0.041\n",
      "[979,   100] loss: 0.042\n",
      "[980,   100] loss: 0.043\n",
      "[981,   100] loss: 0.042\n",
      "[982,   100] loss: 0.042\n",
      "[983,   100] loss: 0.041\n",
      "[984,   100] loss: 0.041\n",
      "[985,   100] loss: 0.041\n",
      "[986,   100] loss: 0.041\n",
      "[987,   100] loss: 0.041\n",
      "[988,   100] loss: 0.041\n",
      "[989,   100] loss: 0.041\n",
      "[990,   100] loss: 0.041\n",
      "[991,   100] loss: 0.041\n",
      "[992,   100] loss: 0.041\n",
      "[993,   100] loss: 0.041\n",
      "[994,   100] loss: 0.041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[995,   100] loss: 0.041\n",
      "[996,   100] loss: 0.042\n",
      "[997,   100] loss: 0.042\n",
      "[998,   100] loss: 0.041\n",
      "[999,   100] loss: 0.041\n",
      "[1000,   100] loss: 0.041\n",
      "Finished Training\n",
      "[*] Visualizing an example's output...\n",
      "tensor([[0.0000, 0.0000, 0.8550, 0.0000, 0.7975, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.7655, 0.8190, 0.0000, 0.0000, 0.7698, 0.0000, 0.7802, 0.0000,\n",
      "         0.8305, 0.0000, 0.0000, 0.0000, 0.0000, 0.8463, 0.0000, 0.0000, 0.7310,\n",
      "         0.0000, 0.0000, 0.0000, 0.8255, 0.0000, 0.6962, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.6634, 0.7243, 0.0000, 0.0000, 0.7627, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.8543, 0.0000, 0.0000, 0.0000, 0.7219,\n",
      "         0.7176, 0.0000, 0.0000, 0.0000, 0.6515, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7031, 0.8823, 0.8090,\n",
      "         0.0000, 0.0000, 0.8826, 0.0000, 0.0000, 0.7951, 0.6790, 0.0000, 0.7347,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8207, 0.0000, 0.0000, 0.7415,\n",
      "         0.0000, 0.0000, 0.0000, 0.8878, 0.0000, 0.0000, 0.0000, 0.0000, 0.7295,\n",
      "         0.0000]])\n",
      "tensor([[6.9340e-02, 1.1877e-05, 7.5751e-01, 5.0369e-07, 5.8751e-01, 2.8221e-01,\n",
      "         1.5177e-04, 2.8995e-03, 1.6452e-03, 3.7367e-01, 7.3449e-01, 6.0679e-01,\n",
      "         6.8917e-04, 1.2206e-01, 8.4260e-01, 4.2000e-05, 4.5056e-01, 3.9179e-03,\n",
      "         3.4854e-01, 5.7494e-06, 7.5444e-13, 2.3475e-01, 2.0512e-03, 6.1751e-01,\n",
      "         1.4090e-07, 7.3750e-05, 6.6229e-01, 2.7945e-01, 7.7949e-05, 1.1089e-10,\n",
      "         3.8572e-01, 3.3703e-08, 5.0516e-01, 7.0503e-08, 6.5025e-01, 4.1544e-01,\n",
      "         1.0133e-08, 6.1330e-01, 4.5658e-01, 6.4705e-01, 5.7241e-05, 1.7637e-01,\n",
      "         7.5764e-01, 5.4120e-08, 1.0362e-03, 4.5789e-01, 1.9029e-04, 1.4332e-03,\n",
      "         1.4267e-01, 1.3087e-01, 3.2524e-03, 3.0931e-01, 4.4799e-04, 7.0907e-01,\n",
      "         6.2395e-01, 7.0718e-08, 3.8901e-04, 9.6022e-05, 5.4602e-01, 8.8856e-08,\n",
      "         2.2006e-08, 4.2999e-01, 3.6306e-01, 3.2634e-15, 2.8482e-01, 2.3354e-01,\n",
      "         3.8279e-05, 3.4825e-04, 2.6267e-07, 5.8406e-01, 5.7243e-01, 4.2561e-01,\n",
      "         7.7493e-06, 1.7085e-03, 8.4291e-01, 2.0977e-01, 1.3510e-04, 8.1120e-01,\n",
      "         5.6817e-01, 1.9462e-06, 6.7303e-01, 6.8853e-14, 3.3320e-05, 3.2276e-01,\n",
      "         8.1312e-04, 5.2082e-04, 5.4296e-01, 6.5212e-04, 8.7149e-05, 3.5716e-01,\n",
      "         2.9828e-08, 5.6638e-12, 6.4956e-04, 8.3624e-01, 1.5979e-05, 7.5340e-07,\n",
      "         2.7986e-05, 9.2833e-05, 2.5581e-01, 2.7008e-02]],\n",
      "       grad_fn=<SigmoidBackward>)\n",
      "0.043539774\n",
      "[*] Getting the embeddings and visualizing t-SNE...\n",
      "[tensor([0]), tensor([0])]\n",
      "[tensor([1]), tensor([1])]\n",
      "[tensor([2]), tensor([2])]\n",
      "[tensor([3]), tensor([3])]\n",
      "[tensor([4]), tensor([4])]\n",
      "[tensor([5]), tensor([5])]\n",
      "[tensor([6]), tensor([6])]\n",
      "[tensor([7]), tensor([7])]\n",
      "[tensor([8]), tensor([8])]\n",
      "[tensor([9]), tensor([9])]\n",
      "[tensor([10]), tensor([10])]\n",
      "[tensor([11]), tensor([11])]\n",
      "[tensor([12]), tensor([12])]\n",
      "[tensor([13]), tensor([13])]\n",
      "[tensor([14]), tensor([14])]\n",
      "[tensor([15]), tensor([15])]\n",
      "[tensor([16]), tensor([16])]\n",
      "[tensor([17]), tensor([17])]\n",
      "[tensor([18]), tensor([18])]\n",
      "[tensor([19]), tensor([19])]\n",
      "[tensor([20]), tensor([20])]\n",
      "[tensor([21]), tensor([21])]\n",
      "[tensor([22]), tensor([22])]\n",
      "[tensor([23]), tensor([23])]\n",
      "[tensor([24]), tensor([24])]\n",
      "[tensor([25]), tensor([25])]\n",
      "[tensor([26]), tensor([26])]\n",
      "[tensor([27]), tensor([27])]\n",
      "[tensor([28]), tensor([28])]\n",
      "[tensor([29]), tensor([29])]\n",
      "[tensor([30]), tensor([30])]\n",
      "[tensor([31]), tensor([31])]\n",
      "[tensor([32]), tensor([32])]\n",
      "[tensor([33]), tensor([33])]\n",
      "[tensor([34]), tensor([34])]\n",
      "[tensor([35]), tensor([35])]\n",
      "[tensor([36]), tensor([36])]\n",
      "[tensor([37]), tensor([37])]\n",
      "[tensor([38]), tensor([38])]\n",
      "[tensor([39]), tensor([39])]\n",
      "[tensor([40]), tensor([40])]\n",
      "[tensor([41]), tensor([41])]\n",
      "[tensor([42]), tensor([42])]\n",
      "[tensor([43]), tensor([43])]\n",
      "[tensor([44]), tensor([44])]\n",
      "[tensor([45]), tensor([45])]\n",
      "[tensor([46]), tensor([46])]\n",
      "[tensor([47]), tensor([47])]\n",
      "[tensor([48]), tensor([48])]\n",
      "[tensor([49]), tensor([49])]\n",
      "[tensor([50]), tensor([50])]\n",
      "[tensor([51]), tensor([51])]\n",
      "[tensor([52]), tensor([52])]\n",
      "[tensor([53]), tensor([53])]\n",
      "[tensor([54]), tensor([54])]\n",
      "[tensor([55]), tensor([55])]\n",
      "[tensor([56]), tensor([56])]\n",
      "[tensor([57]), tensor([57])]\n",
      "[tensor([58]), tensor([58])]\n",
      "[tensor([59]), tensor([59])]\n",
      "[tensor([60]), tensor([60])]\n",
      "[tensor([61]), tensor([61])]\n",
      "[tensor([62]), tensor([62])]\n",
      "[tensor([63]), tensor([63])]\n",
      "[tensor([64]), tensor([64])]\n",
      "[tensor([65]), tensor([65])]\n",
      "[tensor([66]), tensor([66])]\n",
      "[tensor([67]), tensor([67])]\n",
      "[tensor([68]), tensor([68])]\n",
      "[tensor([69]), tensor([69])]\n",
      "[tensor([70]), tensor([70])]\n",
      "[tensor([71]), tensor([71])]\n",
      "[tensor([72]), tensor([72])]\n",
      "[tensor([73]), tensor([73])]\n",
      "[tensor([74]), tensor([74])]\n",
      "[tensor([75]), tensor([75])]\n",
      "[tensor([76]), tensor([76])]\n",
      "[tensor([77]), tensor([77])]\n",
      "[tensor([78]), tensor([78])]\n",
      "[tensor([79]), tensor([79])]\n",
      "[tensor([80]), tensor([80])]\n",
      "[tensor([81]), tensor([81])]\n",
      "[tensor([82]), tensor([82])]\n",
      "[tensor([83]), tensor([83])]\n",
      "[tensor([84]), tensor([84])]\n",
      "[tensor([85]), tensor([85])]\n",
      "[tensor([86]), tensor([86])]\n",
      "[tensor([87]), tensor([87])]\n",
      "[tensor([88]), tensor([88])]\n",
      "[tensor([89]), tensor([89])]\n",
      "[tensor([90]), tensor([90])]\n",
      "[tensor([91]), tensor([91])]\n",
      "[tensor([92]), tensor([92])]\n",
      "[tensor([93]), tensor([93])]\n",
      "[tensor([94]), tensor([94])]\n",
      "[tensor([95]), tensor([95])]\n",
      "[tensor([96]), tensor([96])]\n",
      "[tensor([97]), tensor([97])]\n",
      "[tensor([98]), tensor([98])]\n",
      "[tensor([99]), tensor([99])]\n",
      "'dngr_pipeline'  100903.47 ms\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings_targets, _, _ = dngr_pipeline(pp_net, N_rand_targets, [60, 40, 20], n_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spliting train and test sets...\n",
      "Building the train loader...\n",
      "Number of variables: 120\n",
      "Finding positive and negative examples...\n",
      "Number of train examples: 4031\n",
      "Number of positive examples in train set: 2015\n",
      "Number of negative/unlabelled examples in train set: 2016\n",
      "[1] loss: 0.381, auc: 0.495, acc: 0.499\n",
      "[2] loss: 0.320, auc: 0.495, acc: 0.500\n",
      "[3] loss: 0.315, auc: 0.496, acc: 0.498\n",
      "[4] loss: 0.312, auc: 0.496, acc: 0.497\n",
      "[5] loss: 0.310, auc: 0.497, acc: 0.499\n",
      "[6] loss: 0.309, auc: 0.498, acc: 0.502\n",
      "[7] loss: 0.308, auc: 0.498, acc: 0.503\n",
      "[8] loss: 0.306, auc: 0.499, acc: 0.503\n",
      "[9] loss: 0.305, auc: 0.500, acc: 0.500\n",
      "[10] loss: 0.303, auc: 0.501, acc: 0.498\n",
      "[11] loss: 0.301, auc: 0.501, acc: 0.502\n",
      "[12] loss: 0.301, auc: 0.502, acc: 0.501\n",
      "[13] loss: 0.299, auc: 0.503, acc: 0.503\n",
      "[14] loss: 0.298, auc: 0.504, acc: 0.505\n",
      "[15] loss: 0.296, auc: 0.504, acc: 0.503\n",
      "[16] loss: 0.295, auc: 0.505, acc: 0.500\n",
      "[17] loss: 0.294, auc: 0.506, acc: 0.499\n",
      "[18] loss: 0.292, auc: 0.506, acc: 0.500\n",
      "[19] loss: 0.292, auc: 0.507, acc: 0.499\n",
      "[20] loss: 0.290, auc: 0.508, acc: 0.501\n",
      "[21] loss: 0.289, auc: 0.508, acc: 0.502\n",
      "[22] loss: 0.288, auc: 0.509, acc: 0.502\n",
      "[23] loss: 0.287, auc: 0.509, acc: 0.504\n",
      "[24] loss: 0.285, auc: 0.510, acc: 0.503\n",
      "[25] loss: 0.285, auc: 0.511, acc: 0.504\n",
      "[26] loss: 0.283, auc: 0.512, acc: 0.504\n",
      "[27] loss: 0.283, auc: 0.512, acc: 0.505\n",
      "[28] loss: 0.281, auc: 0.513, acc: 0.505\n",
      "[29] loss: 0.280, auc: 0.513, acc: 0.506\n",
      "[30] loss: 0.279, auc: 0.513, acc: 0.506\n",
      "[31] loss: 0.278, auc: 0.515, acc: 0.506\n",
      "[32] loss: 0.277, auc: 0.515, acc: 0.506\n",
      "[33] loss: 0.276, auc: 0.516, acc: 0.506\n",
      "[34] loss: 0.275, auc: 0.516, acc: 0.507\n",
      "[35] loss: 0.274, auc: 0.517, acc: 0.507\n",
      "[36] loss: 0.273, auc: 0.518, acc: 0.508\n",
      "[37] loss: 0.272, auc: 0.518, acc: 0.507\n",
      "[38] loss: 0.272, auc: 0.519, acc: 0.507\n",
      "[39] loss: 0.270, auc: 0.519, acc: 0.507\n",
      "[40] loss: 0.270, auc: 0.520, acc: 0.508\n",
      "[41] loss: 0.268, auc: 0.521, acc: 0.507\n",
      "[42] loss: 0.268, auc: 0.522, acc: 0.509\n",
      "[43] loss: 0.267, auc: 0.523, acc: 0.509\n",
      "[44] loss: 0.266, auc: 0.523, acc: 0.507\n",
      "[45] loss: 0.266, auc: 0.524, acc: 0.508\n",
      "[46] loss: 0.265, auc: 0.525, acc: 0.509\n",
      "[47] loss: 0.264, auc: 0.525, acc: 0.509\n",
      "[48] loss: 0.263, auc: 0.525, acc: 0.509\n",
      "[49] loss: 0.263, auc: 0.526, acc: 0.509\n",
      "[50] loss: 0.262, auc: 0.526, acc: 0.511\n",
      "[51] loss: 0.261, auc: 0.527, acc: 0.508\n",
      "[52] loss: 0.260, auc: 0.528, acc: 0.508\n",
      "[53] loss: 0.260, auc: 0.529, acc: 0.508\n",
      "[54] loss: 0.259, auc: 0.530, acc: 0.508\n",
      "[55] loss: 0.259, auc: 0.530, acc: 0.511\n",
      "[56] loss: 0.258, auc: 0.530, acc: 0.511\n",
      "[57] loss: 0.257, auc: 0.530, acc: 0.510\n",
      "[58] loss: 0.257, auc: 0.531, acc: 0.511\n",
      "[59] loss: 0.256, auc: 0.532, acc: 0.510\n",
      "[60] loss: 0.256, auc: 0.533, acc: 0.512\n",
      "[61] loss: 0.255, auc: 0.533, acc: 0.511\n",
      "[62] loss: 0.255, auc: 0.533, acc: 0.511\n",
      "[63] loss: 0.254, auc: 0.534, acc: 0.511\n",
      "[64] loss: 0.254, auc: 0.535, acc: 0.510\n",
      "[65] loss: 0.253, auc: 0.535, acc: 0.510\n",
      "[66] loss: 0.253, auc: 0.536, acc: 0.509\n",
      "[67] loss: 0.253, auc: 0.536, acc: 0.511\n",
      "[68] loss: 0.252, auc: 0.537, acc: 0.511\n",
      "[69] loss: 0.252, auc: 0.537, acc: 0.512\n",
      "[70] loss: 0.251, auc: 0.539, acc: 0.510\n",
      "[71] loss: 0.251, auc: 0.539, acc: 0.512\n",
      "[72] loss: 0.250, auc: 0.540, acc: 0.511\n",
      "[73] loss: 0.250, auc: 0.540, acc: 0.510\n",
      "[74] loss: 0.250, auc: 0.541, acc: 0.510\n",
      "[75] loss: 0.249, auc: 0.540, acc: 0.510\n",
      "[76] loss: 0.249, auc: 0.541, acc: 0.509\n",
      "[77] loss: 0.249, auc: 0.541, acc: 0.511\n",
      "[78] loss: 0.249, auc: 0.541, acc: 0.511\n",
      "[79] loss: 0.248, auc: 0.541, acc: 0.511\n",
      "[80] loss: 0.248, auc: 0.540, acc: 0.511\n",
      "[81] loss: 0.247, auc: 0.542, acc: 0.510\n",
      "[82] loss: 0.247, auc: 0.543, acc: 0.510\n",
      "[83] loss: 0.247, auc: 0.547, acc: 0.511\n",
      "[84] loss: 0.247, auc: 0.545, acc: 0.512\n",
      "[85] loss: 0.247, auc: 0.546, acc: 0.511\n",
      "[86] loss: 0.246, auc: 0.545, acc: 0.510\n",
      "[87] loss: 0.246, auc: 0.545, acc: 0.510\n",
      "[88] loss: 0.246, auc: 0.546, acc: 0.510\n",
      "[89] loss: 0.246, auc: 0.547, acc: 0.510\n",
      "[90] loss: 0.245, auc: 0.549, acc: 0.510\n",
      "[91] loss: 0.245, auc: 0.549, acc: 0.510\n",
      "[92] loss: 0.245, auc: 0.548, acc: 0.509\n",
      "[93] loss: 0.245, auc: 0.548, acc: 0.508\n",
      "[94] loss: 0.245, auc: 0.548, acc: 0.509\n",
      "[95] loss: 0.245, auc: 0.548, acc: 0.510\n",
      "[96] loss: 0.244, auc: 0.549, acc: 0.510\n",
      "[97] loss: 0.244, auc: 0.549, acc: 0.509\n",
      "[98] loss: 0.244, auc: 0.549, acc: 0.509\n",
      "[99] loss: 0.244, auc: 0.551, acc: 0.509\n",
      "[100] loss: 0.244, auc: 0.552, acc: 0.510\n",
      "[101] loss: 0.244, auc: 0.550, acc: 0.510\n",
      "[102] loss: 0.244, auc: 0.551, acc: 0.511\n",
      "[103] loss: 0.243, auc: 0.552, acc: 0.510\n",
      "[104] loss: 0.243, auc: 0.551, acc: 0.509\n",
      "[105] loss: 0.243, auc: 0.552, acc: 0.509\n",
      "[106] loss: 0.243, auc: 0.553, acc: 0.510\n",
      "[107] loss: 0.243, auc: 0.553, acc: 0.510\n",
      "[108] loss: 0.243, auc: 0.553, acc: 0.509\n",
      "[109] loss: 0.243, auc: 0.553, acc: 0.509\n",
      "[110] loss: 0.242, auc: 0.553, acc: 0.509\n",
      "[111] loss: 0.243, auc: 0.554, acc: 0.509\n",
      "[112] loss: 0.242, auc: 0.554, acc: 0.508\n",
      "[113] loss: 0.242, auc: 0.554, acc: 0.509\n",
      "[114] loss: 0.242, auc: 0.555, acc: 0.508\n",
      "[115] loss: 0.242, auc: 0.554, acc: 0.508\n",
      "[116] loss: 0.242, auc: 0.556, acc: 0.508\n",
      "[117] loss: 0.242, auc: 0.555, acc: 0.509\n",
      "[118] loss: 0.242, auc: 0.555, acc: 0.508\n",
      "[119] loss: 0.242, auc: 0.556, acc: 0.508\n",
      "[120] loss: 0.242, auc: 0.556, acc: 0.508\n",
      "[121] loss: 0.242, auc: 0.557, acc: 0.509\n",
      "[122] loss: 0.242, auc: 0.557, acc: 0.509\n",
      "[123] loss: 0.242, auc: 0.557, acc: 0.509\n",
      "[124] loss: 0.241, auc: 0.557, acc: 0.509\n",
      "[125] loss: 0.241, auc: 0.558, acc: 0.510\n",
      "[126] loss: 0.241, auc: 0.558, acc: 0.509\n",
      "[127] loss: 0.241, auc: 0.558, acc: 0.507\n",
      "[128] loss: 0.241, auc: 0.556, acc: 0.508\n",
      "[129] loss: 0.241, auc: 0.556, acc: 0.508\n",
      "[130] loss: 0.241, auc: 0.558, acc: 0.507\n",
      "[131] loss: 0.241, auc: 0.558, acc: 0.507\n",
      "[132] loss: 0.241, auc: 0.557, acc: 0.509\n",
      "[133] loss: 0.241, auc: 0.558, acc: 0.508\n",
      "[134] loss: 0.241, auc: 0.559, acc: 0.507\n",
      "[135] loss: 0.241, auc: 0.559, acc: 0.507\n",
      "[136] loss: 0.241, auc: 0.559, acc: 0.507\n",
      "[137] loss: 0.241, auc: 0.558, acc: 0.507\n",
      "[138] loss: 0.241, auc: 0.559, acc: 0.507\n",
      "[139] loss: 0.241, auc: 0.559, acc: 0.507\n",
      "[140] loss: 0.241, auc: 0.561, acc: 0.507\n",
      "[141] loss: 0.241, auc: 0.560, acc: 0.507\n",
      "[142] loss: 0.241, auc: 0.560, acc: 0.508\n",
      "[143] loss: 0.241, auc: 0.561, acc: 0.507\n",
      "[144] loss: 0.241, auc: 0.562, acc: 0.508\n",
      "[145] loss: 0.241, auc: 0.561, acc: 0.506\n",
      "[146] loss: 0.241, auc: 0.560, acc: 0.506\n",
      "[147] loss: 0.241, auc: 0.561, acc: 0.506\n",
      "[148] loss: 0.241, auc: 0.561, acc: 0.506\n",
      "[149] loss: 0.241, auc: 0.561, acc: 0.507\n",
      "[150] loss: 0.241, auc: 0.562, acc: 0.506\n",
      "[151] loss: 0.240, auc: 0.564, acc: 0.506\n",
      "[152] loss: 0.241, auc: 0.562, acc: 0.506\n",
      "[153] loss: 0.241, auc: 0.562, acc: 0.505\n",
      "[154] loss: 0.240, auc: 0.560, acc: 0.506\n",
      "[155] loss: 0.241, auc: 0.562, acc: 0.505\n",
      "[156] loss: 0.241, auc: 0.561, acc: 0.506\n",
      "[157] loss: 0.241, auc: 0.561, acc: 0.507\n",
      "[158] loss: 0.241, auc: 0.563, acc: 0.507\n",
      "[159] loss: 0.241, auc: 0.562, acc: 0.507\n",
      "[160] loss: 0.241, auc: 0.561, acc: 0.505\n",
      "[161] loss: 0.240, auc: 0.563, acc: 0.505\n",
      "[162] loss: 0.240, auc: 0.563, acc: 0.505\n",
      "[163] loss: 0.240, auc: 0.563, acc: 0.504\n",
      "[164] loss: 0.240, auc: 0.563, acc: 0.505\n",
      "[165] loss: 0.241, auc: 0.564, acc: 0.505\n",
      "[166] loss: 0.240, auc: 0.563, acc: 0.506\n",
      "[167] loss: 0.240, auc: 0.563, acc: 0.506\n",
      "[168] loss: 0.240, auc: 0.562, acc: 0.505\n",
      "[169] loss: 0.240, auc: 0.565, acc: 0.505\n",
      "[170] loss: 0.240, auc: 0.565, acc: 0.505\n",
      "[171] loss: 0.240, auc: 0.564, acc: 0.504\n",
      "[172] loss: 0.241, auc: 0.565, acc: 0.505\n",
      "[173] loss: 0.240, auc: 0.566, acc: 0.506\n",
      "[174] loss: 0.241, auc: 0.566, acc: 0.505\n",
      "[175] loss: 0.240, auc: 0.567, acc: 0.505\n",
      "[176] loss: 0.240, auc: 0.564, acc: 0.504\n",
      "[177] loss: 0.240, auc: 0.566, acc: 0.505\n",
      "[178] loss: 0.240, auc: 0.567, acc: 0.505\n",
      "[179] loss: 0.240, auc: 0.567, acc: 0.504\n",
      "[180] loss: 0.240, auc: 0.567, acc: 0.505\n",
      "[181] loss: 0.240, auc: 0.567, acc: 0.504\n",
      "[182] loss: 0.240, auc: 0.568, acc: 0.505\n",
      "[183] loss: 0.240, auc: 0.567, acc: 0.504\n",
      "[184] loss: 0.240, auc: 0.568, acc: 0.504\n",
      "[185] loss: 0.240, auc: 0.569, acc: 0.504\n",
      "[186] loss: 0.240, auc: 0.567, acc: 0.504\n",
      "[187] loss: 0.240, auc: 0.566, acc: 0.504\n",
      "[188] loss: 0.240, auc: 0.570, acc: 0.503\n",
      "[189] loss: 0.240, auc: 0.569, acc: 0.503\n",
      "[190] loss: 0.240, auc: 0.569, acc: 0.504\n",
      "[191] loss: 0.240, auc: 0.570, acc: 0.503\n",
      "[192] loss: 0.240, auc: 0.571, acc: 0.503\n",
      "[193] loss: 0.240, auc: 0.571, acc: 0.504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[194] loss: 0.240, auc: 0.569, acc: 0.503\n",
      "[195] loss: 0.240, auc: 0.571, acc: 0.504\n",
      "[196] loss: 0.240, auc: 0.571, acc: 0.504\n",
      "[197] loss: 0.240, auc: 0.573, acc: 0.503\n",
      "[198] loss: 0.240, auc: 0.572, acc: 0.504\n",
      "[199] loss: 0.240, auc: 0.573, acc: 0.503\n",
      "[200] loss: 0.240, auc: 0.574, acc: 0.504\n",
      "[201] loss: 0.240, auc: 0.575, acc: 0.504\n",
      "[202] loss: 0.240, auc: 0.574, acc: 0.503\n",
      "[203] loss: 0.240, auc: 0.573, acc: 0.503\n",
      "[204] loss: 0.240, auc: 0.573, acc: 0.503\n",
      "[205] loss: 0.240, auc: 0.572, acc: 0.503\n",
      "[206] loss: 0.240, auc: 0.574, acc: 0.504\n",
      "[207] loss: 0.240, auc: 0.573, acc: 0.503\n",
      "[208] loss: 0.240, auc: 0.572, acc: 0.504\n",
      "[209] loss: 0.240, auc: 0.573, acc: 0.504\n",
      "[210] loss: 0.240, auc: 0.573, acc: 0.504\n",
      "[211] loss: 0.240, auc: 0.573, acc: 0.503\n",
      "[212] loss: 0.240, auc: 0.573, acc: 0.503\n",
      "[213] loss: 0.240, auc: 0.572, acc: 0.502\n",
      "[214] loss: 0.240, auc: 0.573, acc: 0.503\n",
      "[215] loss: 0.240, auc: 0.573, acc: 0.502\n",
      "[216] loss: 0.240, auc: 0.572, acc: 0.503\n",
      "[217] loss: 0.240, auc: 0.572, acc: 0.503\n",
      "[218] loss: 0.240, auc: 0.571, acc: 0.503\n",
      "[219] loss: 0.240, auc: 0.572, acc: 0.503\n",
      "[220] loss: 0.240, auc: 0.572, acc: 0.502\n",
      "[221] loss: 0.240, auc: 0.573, acc: 0.503\n",
      "[222] loss: 0.240, auc: 0.574, acc: 0.503\n",
      "[223] loss: 0.240, auc: 0.574, acc: 0.503\n",
      "[224] loss: 0.240, auc: 0.575, acc: 0.502\n",
      "[225] loss: 0.240, auc: 0.575, acc: 0.502\n",
      "[226] loss: 0.240, auc: 0.575, acc: 0.502\n",
      "[227] loss: 0.240, auc: 0.576, acc: 0.502\n",
      "[228] loss: 0.240, auc: 0.574, acc: 0.502\n",
      "[229] loss: 0.240, auc: 0.575, acc: 0.503\n",
      "[230] loss: 0.240, auc: 0.576, acc: 0.503\n",
      "[231] loss: 0.240, auc: 0.575, acc: 0.502\n",
      "[232] loss: 0.240, auc: 0.574, acc: 0.502\n",
      "[233] loss: 0.240, auc: 0.573, acc: 0.503\n",
      "[234] loss: 0.240, auc: 0.573, acc: 0.502\n",
      "[235] loss: 0.240, auc: 0.573, acc: 0.503\n",
      "[236] loss: 0.240, auc: 0.573, acc: 0.503\n",
      "[237] loss: 0.240, auc: 0.573, acc: 0.503\n",
      "[238] loss: 0.240, auc: 0.573, acc: 0.503\n",
      "[239] loss: 0.240, auc: 0.574, acc: 0.502\n",
      "[240] loss: 0.240, auc: 0.573, acc: 0.503\n",
      "[241] loss: 0.240, auc: 0.573, acc: 0.503\n",
      "[242] loss: 0.240, auc: 0.573, acc: 0.503\n",
      "[243] loss: 0.240, auc: 0.574, acc: 0.503\n",
      "[244] loss: 0.240, auc: 0.574, acc: 0.503\n",
      "[245] loss: 0.240, auc: 0.574, acc: 0.502\n",
      "[246] loss: 0.240, auc: 0.575, acc: 0.503\n",
      "[247] loss: 0.240, auc: 0.575, acc: 0.503\n",
      "[248] loss: 0.240, auc: 0.575, acc: 0.502\n",
      "[249] loss: 0.240, auc: 0.576, acc: 0.503\n",
      "[250] loss: 0.240, auc: 0.577, acc: 0.503\n",
      "[251] loss: 0.240, auc: 0.577, acc: 0.503\n",
      "[252] loss: 0.240, auc: 0.576, acc: 0.503\n",
      "[253] loss: 0.240, auc: 0.576, acc: 0.504\n",
      "[254] loss: 0.240, auc: 0.576, acc: 0.503\n",
      "[255] loss: 0.240, auc: 0.577, acc: 0.504\n",
      "[256] loss: 0.240, auc: 0.578, acc: 0.504\n",
      "[257] loss: 0.240, auc: 0.577, acc: 0.503\n",
      "[258] loss: 0.240, auc: 0.578, acc: 0.503\n",
      "[259] loss: 0.240, auc: 0.577, acc: 0.503\n",
      "[260] loss: 0.240, auc: 0.579, acc: 0.503\n",
      "[261] loss: 0.240, auc: 0.579, acc: 0.503\n",
      "[262] loss: 0.240, auc: 0.577, acc: 0.503\n",
      "[263] loss: 0.240, auc: 0.576, acc: 0.503\n",
      "[264] loss: 0.240, auc: 0.578, acc: 0.503\n",
      "[265] loss: 0.240, auc: 0.579, acc: 0.503\n",
      "[266] loss: 0.240, auc: 0.578, acc: 0.503\n",
      "[267] loss: 0.240, auc: 0.578, acc: 0.503\n",
      "[268] loss: 0.240, auc: 0.578, acc: 0.503\n",
      "[269] loss: 0.240, auc: 0.579, acc: 0.503\n",
      "[270] loss: 0.240, auc: 0.580, acc: 0.503\n",
      "[271] loss: 0.240, auc: 0.580, acc: 0.503\n",
      "[272] loss: 0.240, auc: 0.580, acc: 0.503\n",
      "[273] loss: 0.240, auc: 0.579, acc: 0.503\n",
      "[274] loss: 0.240, auc: 0.581, acc: 0.502\n",
      "[275] loss: 0.240, auc: 0.580, acc: 0.503\n",
      "[276] loss: 0.240, auc: 0.581, acc: 0.502\n",
      "[277] loss: 0.240, auc: 0.582, acc: 0.502\n",
      "[278] loss: 0.240, auc: 0.582, acc: 0.502\n",
      "[279] loss: 0.240, auc: 0.582, acc: 0.502\n",
      "[280] loss: 0.240, auc: 0.581, acc: 0.502\n",
      "[281] loss: 0.240, auc: 0.581, acc: 0.502\n",
      "[282] loss: 0.240, auc: 0.583, acc: 0.502\n",
      "[283] loss: 0.240, auc: 0.585, acc: 0.502\n",
      "[284] loss: 0.240, auc: 0.583, acc: 0.501\n",
      "[285] loss: 0.240, auc: 0.582, acc: 0.501\n",
      "[286] loss: 0.240, auc: 0.583, acc: 0.501\n",
      "[287] loss: 0.240, auc: 0.581, acc: 0.502\n",
      "[288] loss: 0.240, auc: 0.583, acc: 0.502\n",
      "[289] loss: 0.240, auc: 0.583, acc: 0.502\n",
      "[290] loss: 0.240, auc: 0.581, acc: 0.502\n",
      "[291] loss: 0.240, auc: 0.582, acc: 0.502\n",
      "[292] loss: 0.240, auc: 0.582, acc: 0.502\n",
      "[293] loss: 0.240, auc: 0.581, acc: 0.502\n",
      "[294] loss: 0.240, auc: 0.582, acc: 0.503\n",
      "[295] loss: 0.240, auc: 0.581, acc: 0.502\n",
      "[296] loss: 0.240, auc: 0.583, acc: 0.502\n",
      "[297] loss: 0.240, auc: 0.583, acc: 0.502\n",
      "[298] loss: 0.240, auc: 0.582, acc: 0.502\n",
      "[299] loss: 0.240, auc: 0.583, acc: 0.502\n",
      "[300] loss: 0.240, auc: 0.583, acc: 0.503\n",
      "[301] loss: 0.240, auc: 0.583, acc: 0.503\n",
      "[302] loss: 0.240, auc: 0.580, acc: 0.503\n",
      "[303] loss: 0.240, auc: 0.582, acc: 0.503\n",
      "[304] loss: 0.240, auc: 0.581, acc: 0.503\n",
      "[305] loss: 0.240, auc: 0.582, acc: 0.504\n",
      "[306] loss: 0.240, auc: 0.581, acc: 0.503\n",
      "[307] loss: 0.240, auc: 0.581, acc: 0.503\n",
      "[308] loss: 0.240, auc: 0.583, acc: 0.504\n",
      "[309] loss: 0.240, auc: 0.583, acc: 0.504\n",
      "[310] loss: 0.240, auc: 0.583, acc: 0.504\n",
      "[311] loss: 0.240, auc: 0.583, acc: 0.503\n",
      "[312] loss: 0.240, auc: 0.583, acc: 0.504\n",
      "[313] loss: 0.240, auc: 0.583, acc: 0.504\n",
      "[314] loss: 0.240, auc: 0.584, acc: 0.504\n",
      "[315] loss: 0.240, auc: 0.584, acc: 0.504\n",
      "[316] loss: 0.240, auc: 0.584, acc: 0.504\n",
      "[317] loss: 0.240, auc: 0.583, acc: 0.504\n",
      "[318] loss: 0.240, auc: 0.583, acc: 0.503\n",
      "[319] loss: 0.240, auc: 0.583, acc: 0.504\n",
      "[320] loss: 0.240, auc: 0.582, acc: 0.503\n",
      "[321] loss: 0.240, auc: 0.583, acc: 0.504\n",
      "[322] loss: 0.240, auc: 0.583, acc: 0.504\n",
      "[323] loss: 0.240, auc: 0.583, acc: 0.504\n",
      "[324] loss: 0.240, auc: 0.583, acc: 0.504\n",
      "[325] loss: 0.240, auc: 0.583, acc: 0.503\n",
      "[326] loss: 0.240, auc: 0.583, acc: 0.504\n",
      "[327] loss: 0.240, auc: 0.584, acc: 0.504\n",
      "[328] loss: 0.240, auc: 0.585, acc: 0.504\n",
      "[329] loss: 0.239, auc: 0.583, acc: 0.503\n",
      "[330] loss: 0.240, auc: 0.584, acc: 0.504\n",
      "[331] loss: 0.240, auc: 0.584, acc: 0.504\n",
      "[332] loss: 0.240, auc: 0.584, acc: 0.504\n",
      "[333] loss: 0.240, auc: 0.584, acc: 0.504\n",
      "[334] loss: 0.240, auc: 0.584, acc: 0.504\n",
      "[335] loss: 0.240, auc: 0.583, acc: 0.504\n",
      "[336] loss: 0.240, auc: 0.583, acc: 0.504\n",
      "[337] loss: 0.240, auc: 0.584, acc: 0.504\n",
      "[338] loss: 0.240, auc: 0.583, acc: 0.504\n",
      "[339] loss: 0.240, auc: 0.584, acc: 0.504\n",
      "[340] loss: 0.240, auc: 0.585, acc: 0.504\n",
      "[341] loss: 0.240, auc: 0.583, acc: 0.504\n",
      "[342] loss: 0.240, auc: 0.583, acc: 0.503\n",
      "[343] loss: 0.240, auc: 0.584, acc: 0.504\n",
      "[344] loss: 0.240, auc: 0.585, acc: 0.504\n",
      "[345] loss: 0.240, auc: 0.585, acc: 0.504\n",
      "[346] loss: 0.240, auc: 0.584, acc: 0.504\n",
      "[347] loss: 0.240, auc: 0.585, acc: 0.505\n",
      "[348] loss: 0.240, auc: 0.585, acc: 0.504\n",
      "[349] loss: 0.240, auc: 0.585, acc: 0.504\n",
      "[350] loss: 0.240, auc: 0.585, acc: 0.504\n",
      "[351] loss: 0.240, auc: 0.585, acc: 0.504\n",
      "[352] loss: 0.240, auc: 0.584, acc: 0.504\n",
      "[353] loss: 0.240, auc: 0.585, acc: 0.504\n",
      "[354] loss: 0.240, auc: 0.584, acc: 0.504\n",
      "[355] loss: 0.240, auc: 0.584, acc: 0.504\n",
      "[356] loss: 0.240, auc: 0.585, acc: 0.504\n",
      "[357] loss: 0.240, auc: 0.585, acc: 0.504\n",
      "[358] loss: 0.240, auc: 0.583, acc: 0.504\n",
      "[359] loss: 0.240, auc: 0.583, acc: 0.504\n",
      "[360] loss: 0.240, auc: 0.584, acc: 0.504\n",
      "[361] loss: 0.240, auc: 0.584, acc: 0.504\n",
      "[362] loss: 0.240, auc: 0.584, acc: 0.504\n",
      "[363] loss: 0.240, auc: 0.584, acc: 0.504\n",
      "[364] loss: 0.240, auc: 0.585, acc: 0.504\n",
      "[365] loss: 0.240, auc: 0.584, acc: 0.504\n",
      "[366] loss: 0.240, auc: 0.585, acc: 0.504\n",
      "[367] loss: 0.240, auc: 0.583, acc: 0.504\n",
      "[368] loss: 0.240, auc: 0.582, acc: 0.504\n",
      "[369] loss: 0.240, auc: 0.583, acc: 0.504\n",
      "[370] loss: 0.240, auc: 0.581, acc: 0.504\n",
      "[371] loss: 0.240, auc: 0.581, acc: 0.504\n",
      "[372] loss: 0.240, auc: 0.581, acc: 0.505\n",
      "[373] loss: 0.240, auc: 0.582, acc: 0.505\n",
      "[374] loss: 0.240, auc: 0.584, acc: 0.505\n",
      "[375] loss: 0.240, auc: 0.583, acc: 0.504\n",
      "[376] loss: 0.240, auc: 0.584, acc: 0.504\n",
      "[377] loss: 0.239, auc: 0.585, acc: 0.505\n",
      "[378] loss: 0.240, auc: 0.584, acc: 0.505\n",
      "[379] loss: 0.240, auc: 0.585, acc: 0.504\n",
      "[380] loss: 0.240, auc: 0.585, acc: 0.504\n",
      "[381] loss: 0.240, auc: 0.586, acc: 0.504\n",
      "[382] loss: 0.240, auc: 0.585, acc: 0.504\n",
      "[383] loss: 0.240, auc: 0.586, acc: 0.504\n",
      "[384] loss: 0.240, auc: 0.583, acc: 0.504\n",
      "[385] loss: 0.240, auc: 0.587, acc: 0.504\n",
      "[386] loss: 0.240, auc: 0.587, acc: 0.504\n",
      "[387] loss: 0.240, auc: 0.587, acc: 0.504\n",
      "[388] loss: 0.240, auc: 0.587, acc: 0.504\n",
      "[389] loss: 0.240, auc: 0.587, acc: 0.504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[390] loss: 0.240, auc: 0.587, acc: 0.504\n",
      "[391] loss: 0.240, auc: 0.587, acc: 0.504\n",
      "[392] loss: 0.240, auc: 0.588, acc: 0.504\n",
      "[393] loss: 0.240, auc: 0.587, acc: 0.506\n",
      "[394] loss: 0.240, auc: 0.587, acc: 0.504\n",
      "[395] loss: 0.240, auc: 0.586, acc: 0.504\n",
      "[396] loss: 0.240, auc: 0.586, acc: 0.505\n",
      "[397] loss: 0.240, auc: 0.584, acc: 0.504\n",
      "[398] loss: 0.240, auc: 0.587, acc: 0.506\n",
      "[399] loss: 0.240, auc: 0.586, acc: 0.506\n",
      "[400] loss: 0.240, auc: 0.587, acc: 0.507\n",
      "[401] loss: 0.240, auc: 0.586, acc: 0.505\n",
      "[402] loss: 0.240, auc: 0.587, acc: 0.506\n",
      "[403] loss: 0.240, auc: 0.587, acc: 0.505\n",
      "[404] loss: 0.240, auc: 0.587, acc: 0.506\n",
      "[405] loss: 0.240, auc: 0.587, acc: 0.506\n",
      "[406] loss: 0.240, auc: 0.586, acc: 0.505\n",
      "[407] loss: 0.240, auc: 0.586, acc: 0.506\n",
      "[408] loss: 0.239, auc: 0.586, acc: 0.505\n",
      "[409] loss: 0.240, auc: 0.586, acc: 0.506\n",
      "[410] loss: 0.240, auc: 0.587, acc: 0.506\n",
      "[411] loss: 0.240, auc: 0.586, acc: 0.506\n",
      "[412] loss: 0.240, auc: 0.586, acc: 0.506\n",
      "[413] loss: 0.239, auc: 0.585, acc: 0.507\n",
      "[414] loss: 0.240, auc: 0.586, acc: 0.506\n",
      "[415] loss: 0.240, auc: 0.584, acc: 0.505\n",
      "[416] loss: 0.240, auc: 0.587, acc: 0.507\n",
      "[417] loss: 0.240, auc: 0.586, acc: 0.506\n",
      "[418] loss: 0.240, auc: 0.586, acc: 0.506\n",
      "[419] loss: 0.240, auc: 0.586, acc: 0.506\n",
      "[420] loss: 0.240, auc: 0.586, acc: 0.506\n",
      "[421] loss: 0.240, auc: 0.586, acc: 0.506\n",
      "[422] loss: 0.240, auc: 0.586, acc: 0.506\n",
      "[423] loss: 0.240, auc: 0.586, acc: 0.506\n",
      "[424] loss: 0.240, auc: 0.586, acc: 0.506\n",
      "[425] loss: 0.240, auc: 0.586, acc: 0.506\n",
      "[426] loss: 0.240, auc: 0.585, acc: 0.506\n",
      "[427] loss: 0.240, auc: 0.586, acc: 0.505\n",
      "[428] loss: 0.240, auc: 0.586, acc: 0.506\n",
      "[429] loss: 0.240, auc: 0.587, acc: 0.506\n",
      "[430] loss: 0.240, auc: 0.587, acc: 0.506\n",
      "[431] loss: 0.240, auc: 0.587, acc: 0.506\n",
      "[432] loss: 0.239, auc: 0.586, acc: 0.506\n",
      "[433] loss: 0.240, auc: 0.589, acc: 0.506\n",
      "[434] loss: 0.240, auc: 0.588, acc: 0.506\n",
      "[435] loss: 0.240, auc: 0.589, acc: 0.506\n",
      "[436] loss: 0.240, auc: 0.585, acc: 0.505\n",
      "[437] loss: 0.240, auc: 0.587, acc: 0.506\n",
      "[438] loss: 0.240, auc: 0.587, acc: 0.507\n",
      "[439] loss: 0.240, auc: 0.588, acc: 0.507\n",
      "[440] loss: 0.240, auc: 0.587, acc: 0.507\n",
      "[441] loss: 0.239, auc: 0.588, acc: 0.506\n",
      "[442] loss: 0.240, auc: 0.588, acc: 0.505\n",
      "[443] loss: 0.240, auc: 0.588, acc: 0.506\n",
      "[444] loss: 0.240, auc: 0.588, acc: 0.506\n",
      "[445] loss: 0.240, auc: 0.587, acc: 0.506\n",
      "[446] loss: 0.240, auc: 0.588, acc: 0.506\n",
      "[447] loss: 0.239, auc: 0.587, acc: 0.505\n",
      "[448] loss: 0.240, auc: 0.588, acc: 0.506\n",
      "[449] loss: 0.240, auc: 0.587, acc: 0.505\n",
      "[450] loss: 0.240, auc: 0.588, acc: 0.505\n",
      "[451] loss: 0.239, auc: 0.589, acc: 0.506\n",
      "[452] loss: 0.240, auc: 0.588, acc: 0.505\n",
      "[453] loss: 0.239, auc: 0.588, acc: 0.507\n",
      "[454] loss: 0.240, auc: 0.588, acc: 0.506\n",
      "[455] loss: 0.240, auc: 0.587, acc: 0.505\n",
      "[456] loss: 0.240, auc: 0.588, acc: 0.505\n",
      "[457] loss: 0.240, auc: 0.588, acc: 0.506\n",
      "[458] loss: 0.240, auc: 0.588, acc: 0.506\n",
      "[459] loss: 0.240, auc: 0.588, acc: 0.506\n",
      "[460] loss: 0.240, auc: 0.587, acc: 0.505\n",
      "[461] loss: 0.240, auc: 0.589, acc: 0.506\n",
      "[462] loss: 0.240, auc: 0.588, acc: 0.506\n",
      "[463] loss: 0.240, auc: 0.589, acc: 0.506\n",
      "[464] loss: 0.240, auc: 0.590, acc: 0.506\n",
      "[465] loss: 0.240, auc: 0.590, acc: 0.507\n",
      "[466] loss: 0.240, auc: 0.589, acc: 0.506\n",
      "[467] loss: 0.240, auc: 0.590, acc: 0.506\n",
      "[468] loss: 0.240, auc: 0.589, acc: 0.507\n",
      "[469] loss: 0.240, auc: 0.589, acc: 0.508\n",
      "[470] loss: 0.240, auc: 0.589, acc: 0.508\n",
      "[471] loss: 0.240, auc: 0.590, acc: 0.507\n",
      "[472] loss: 0.240, auc: 0.589, acc: 0.506\n",
      "[473] loss: 0.240, auc: 0.590, acc: 0.507\n",
      "[474] loss: 0.239, auc: 0.590, acc: 0.506\n",
      "[475] loss: 0.240, auc: 0.590, acc: 0.508\n",
      "[476] loss: 0.240, auc: 0.588, acc: 0.505\n",
      "[477] loss: 0.239, auc: 0.590, acc: 0.508\n",
      "[478] loss: 0.240, auc: 0.591, acc: 0.507\n",
      "[479] loss: 0.239, auc: 0.590, acc: 0.506\n",
      "[480] loss: 0.240, auc: 0.590, acc: 0.506\n",
      "[481] loss: 0.240, auc: 0.591, acc: 0.506\n",
      "[482] loss: 0.240, auc: 0.591, acc: 0.507\n",
      "[483] loss: 0.239, auc: 0.590, acc: 0.506\n",
      "[484] loss: 0.240, auc: 0.590, acc: 0.508\n",
      "[485] loss: 0.240, auc: 0.590, acc: 0.506\n",
      "[486] loss: 0.240, auc: 0.590, acc: 0.506\n",
      "[487] loss: 0.240, auc: 0.591, acc: 0.506\n",
      "[488] loss: 0.239, auc: 0.592, acc: 0.507\n",
      "[489] loss: 0.240, auc: 0.592, acc: 0.507\n",
      "[490] loss: 0.240, auc: 0.593, acc: 0.506\n",
      "[491] loss: 0.240, auc: 0.592, acc: 0.507\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0e6225fe881b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpu_learning_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_drugs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdp_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/michael/Bureau/stages servier/code_deepdtnet/src/pu_learning.py\u001b[0m in \u001b[0;36mpu_learning_new\u001b[0;34m(k, x, y, P, n_epochs, batch_size, lr, print_step, train_size)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m             \u001b[0;31m# get the inputs; data is a list of [inputs, labels]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/michael/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_IterableDataset_len_called\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "S, H, W, train, test=pu_learning_new(3, embeddings_drugs, embeddings_targets, dp_net, batch_size=20, lr=5e-4, n_epochs=1000, train_size=0.5)\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 20)\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_drugs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 1, 0, 0],\n",
       "       [1, 0, 1, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 1, 1, 0],\n",
       "       [1, 1, 1, ..., 1, 0, 0],\n",
       "       [1, 0, 1, ..., 0, 1, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.4989, 0.5023, 0.4999, 0.4985, 0.4991, 0.4976, 0.5015, 0.5018, 0.4977,\n",
       "         0.4990], grad_fn=<SliceBackward>),\n",
       " array([0, 0, 1, 1, 1, 1, 0, 1, 1, 0]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S[train][:10], dp_net[train][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pu_learner\n",
    "\n",
    "pos_train, neg_train, pos_test, neg_test = get_train_test_masks(dp_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variables: 200\n",
      "Finding positive and negative examples...\n",
      "Number of positive examples: 3232\n",
      "Number of negative/unlabelled examples: 3249\n",
      "Going to minimize... Maximum number of iterations: 300000\n",
      "Objective: tensor(2194.1028, grad_fn=<AddBackward0>) (auc: 0.4977479742434077 )\n",
      "Objective: tensor(1615.1832, grad_fn=<AddBackward0>) (auc: 0.5343584869220994 )\n",
      "Objective: tensor(1612.0001, grad_fn=<AddBackward0>) (auc: 0.5418280834316118 )\n",
      "Objective: tensor(1609.8328, grad_fn=<AddBackward0>) (auc: 0.546409653084422 )\n",
      "Objective: tensor(1607.7227, grad_fn=<AddBackward0>) (auc: 0.5495191399333839 )\n",
      "Objective: tensor(1606.5410, grad_fn=<AddBackward0>) (auc: 0.5518189717171162 )\n",
      "Objective: tensor(1605.6490, grad_fn=<AddBackward0>) (auc: 0.5536380767578143 )\n",
      "Objective: tensor(1604.9060, grad_fn=<AddBackward0>) (auc: 0.55515501342378 )\n",
      "Objective: tensor(1603.9438, grad_fn=<AddBackward0>) (auc: 0.556498772280275 )\n",
      "Objective: tensor(1603.1584, grad_fn=<AddBackward0>) (auc: 0.5575622183063181 )\n",
      "Objective: tensor(1602.4175, grad_fn=<AddBackward0>) (auc: 0.5586973257575065 )\n",
      "Objective: tensor(1601.8335, grad_fn=<AddBackward0>) (auc: 0.5596036404194435 )\n",
      "Objective: tensor(1601.4069, grad_fn=<AddBackward0>) (auc: 0.560297303968624 )\n",
      "Objective: tensor(1601.0420, grad_fn=<AddBackward0>) (auc: 0.5609217821020329 )\n",
      "Objective: tensor(1600.7460, grad_fn=<AddBackward0>) (auc: 0.56143931567672 )\n",
      "Objective: tensor(1600.5044, grad_fn=<AddBackward0>) (auc: 0.5619586586428725 )\n",
      "Objective: tensor(1600.3062, grad_fn=<AddBackward0>) (auc: 0.5624104351224596 )\n",
      "Objective: tensor(1600.0964, grad_fn=<AddBackward0>) (auc: 0.5627034136931699 )\n",
      "Objective: tensor(1600.0145, grad_fn=<AddBackward0>) (auc: 0.5629488719301293 )\n",
      "Objective: tensor(1599.7510, grad_fn=<AddBackward0>) (auc: 0.5633343675434025 )\n",
      "Objective: tensor(1599.5939, grad_fn=<AddBackward0>) (auc: 0.5636319172083414 )\n",
      "Objective: tensor(1599.4489, grad_fn=<AddBackward0>) (auc: 0.5639137537368696 )\n",
      "Objective: tensor(1600.2249, grad_fn=<AddBackward0>) (auc: 0.5597114420583333 )\n",
      "Objective: tensor(1599.1857, grad_fn=<AddBackward0>) (auc: 0.5644927113902526 )\n",
      "Objective: tensor(1599.5095, grad_fn=<AddBackward0>) (auc: 0.5630696250026664 )\n",
      "Objective: tensor(1598.9574, grad_fn=<AddBackward0>) (auc: 0.5650302435021896 )\n",
      "Objective: tensor(1598.8350, grad_fn=<AddBackward0>) (auc: 0.5652792252909501 )\n",
      "Objective: tensor(1598.7291, grad_fn=<AddBackward0>) (auc: 0.5655582048855855 )\n",
      "Objective: tensor(1599.6074, grad_fn=<AddBackward0>) (auc: 0.5634489305925052 )\n",
      "Objective: tensor(1598.5360, grad_fn=<AddBackward0>) (auc: 0.5660007915611506 )\n",
      "Objective: tensor(1603.2249, grad_fn=<AddBackward0>) (auc: 0.5605490474601477 )\n",
      "Objective: tensor(1598.3571, grad_fn=<AddBackward0>) (auc: 0.5663640983211895 )\n",
      "Objective: tensor(1598.6510, grad_fn=<AddBackward0>) (auc: 0.5663859538654696 )\n",
      "Objective: tensor(1598.1871, grad_fn=<AddBackward0>) (auc: 0.5667069303883296 )\n",
      "Objective: tensor(1598.1158, grad_fn=<AddBackward0>) (auc: 0.5667784489667804 )\n",
      "Objective: tensor(1598.0397, grad_fn=<AddBackward0>) (auc: 0.5670588570283621 )\n",
      "Objective: tensor(1597.9617, grad_fn=<AddBackward0>) (auc: 0.5672849833459801 )\n",
      "Objective: tensor(1597.8933, grad_fn=<AddBackward0>) (auc: 0.5673357891537076 )\n",
      "Objective: tensor(1598.0758, grad_fn=<AddBackward0>) (auc: 0.567637052832707 )\n",
      "Objective: tensor(1597.7595, grad_fn=<AddBackward0>) (auc: 0.5677497112592147 )\n",
      "Objective: tensor(1597.7244, grad_fn=<AddBackward0>) (auc: 0.5678846537700861 )\n",
      "Objective: tensor(1597.6974, grad_fn=<AddBackward0>) (auc: 0.5678919865670777 )\n",
      "Objective: tensor(1597.5841, grad_fn=<AddBackward0>) (auc: 0.5680015023663031 )\n",
      "Objective: tensor(1597.5088, grad_fn=<AddBackward0>) (auc: 0.5683341447025589 )\n",
      "Objective: tensor(1597.5291, grad_fn=<AddBackward0>) (auc: 0.5681488725396087 )\n",
      "Objective: tensor(1597.4736, grad_fn=<AddBackward0>) (auc: 0.5683410965750315 )\n",
      "Objective: tensor(1597.3417, grad_fn=<AddBackward0>) (auc: 0.5687082125802608 )\n",
      "Objective: tensor(1597.2866, grad_fn=<AddBackward0>) (auc: 0.5688493451145669 )\n",
      "Objective: tensor(1597.3269, grad_fn=<AddBackward0>) (auc: 0.5683682374470134 )\n",
      "Objective: tensor(1597.2025, grad_fn=<AddBackward0>) (auc: 0.5689820973094539 )\n",
      "Objective: tensor(1597.1472, grad_fn=<AddBackward0>) (auc: 0.5692053190776142 )\n",
      "Objective: tensor(1597.0985, grad_fn=<AddBackward0>) (auc: 0.5690781379038181 )\n",
      "Objective: tensor(1598.9971, grad_fn=<AddBackward0>) (auc: 0.5689282917211388 )\n",
      "Objective: tensor(1599.9816, grad_fn=<AddBackward0>) (auc: 0.5674669224193887 )\n",
      "Objective: tensor(1596.9543, grad_fn=<AddBackward0>) (auc: 0.5696150986289764 )\n",
      "Objective: tensor(1597.4894, grad_fn=<AddBackward0>) (auc: 0.5692373167372139 )\n",
      "Objective: tensor(1596.8545, grad_fn=<AddBackward0>) (auc: 0.5695588170312876 )\n",
      "Objective: tensor(1596.7938, grad_fn=<AddBackward0>) (auc: 0.5698083225912619 )\n",
      "Objective: tensor(1596.7472, grad_fn=<AddBackward0>) (auc: 0.5698649851134698 )\n",
      "Objective: tensor(1599.8403, grad_fn=<AddBackward0>) (auc: 0.5658425650390524 )\n",
      "Objective: tensor(1597.0272, grad_fn=<AddBackward0>) (auc: 0.5700178310767364 )\n",
      "Objective: tensor(1596.6119, grad_fn=<AddBackward0>) (auc: 0.5701596302289509 )\n",
      "Objective: tensor(1597.4962, grad_fn=<AddBackward0>) (auc: 0.5702302917272337 )\n",
      "Objective: tensor(1596.5931, grad_fn=<AddBackward0>) (auc: 0.5704215158357941 )\n",
      "Objective: tensor(1596.4774, grad_fn=<AddBackward0>) (auc: 0.5703878516314235 )\n",
      "Objective: tensor(1597.6625, grad_fn=<AddBackward0>) (auc: 0.5693178822729918 )\n",
      "Objective: tensor(1596.3906, grad_fn=<AddBackward0>) (auc: 0.5705446020710105 )\n",
      "Objective: tensor(1596.4895, grad_fn=<AddBackward0>) (auc: 0.5702369102907521 )\n",
      "Objective: tensor(1604.4158, grad_fn=<AddBackward0>) (auc: 0.5652441802351981 )\n",
      "Objective: tensor(1596.2643, grad_fn=<AddBackward0>) (auc: 0.5708315810805457 )\n",
      "Objective: tensor(1596.2278, grad_fn=<AddBackward0>) (auc: 0.5709132417743159 )\n",
      "Objective: tensor(1596.2036, grad_fn=<AddBackward0>) (auc: 0.5708798632633346 )\n",
      "Objective: tensor(1596.1489, grad_fn=<AddBackward0>) (auc: 0.5710487556719661 )\n",
      "Objective: tensor(1596.1162, grad_fn=<AddBackward0>) (auc: 0.571105037269655 )\n",
      "Objective: tensor(1596.2197, grad_fn=<AddBackward0>) (auc: 0.5713528286692935 )\n",
      "Objective: tensor(1596.0350, grad_fn=<AddBackward0>) (auc: 0.5712550739145936 )\n",
      "Objective: tensor(1597.4832, grad_fn=<AddBackward0>) (auc: 0.568474467772262 )\n",
      "Objective: tensor(1595.9742, grad_fn=<AddBackward0>) (auc: 0.5714891996471115 )\n",
      "Objective: tensor(1595.9326, grad_fn=<AddBackward0>) (auc: 0.5715343392026184 )\n",
      "Objective: tensor(1595.8949, grad_fn=<AddBackward0>) (auc: 0.5716147142761367 )\n",
      "Objective: tensor(1595.8855, grad_fn=<AddBackward0>) (auc: 0.5716485689427668 )\n",
      "Objective: tensor(1595.8342, grad_fn=<AddBackward0>) (auc: 0.5717191828254848 )\n",
      "Objective: tensor(1595.8115, grad_fn=<AddBackward0>) (auc: 0.5717723694114563 )\n",
      "Objective: tensor(1595.7662, grad_fn=<AddBackward0>) (auc: 0.5718540777207914 )\n",
      "Objective: tensor(1595.7356, grad_fn=<AddBackward0>) (auc: 0.5719320720160659 )\n",
      "Objective: tensor(1595.7119, grad_fn=<AddBackward0>) (auc: 0.5719557845673764 )\n",
      "Objective: tensor(1595.6785, grad_fn=<AddBackward0>) (auc: 0.572044063824665 )\n",
      "Objective: tensor(1595.6555, grad_fn=<AddBackward0>) (auc: 0.5721099637664597 )\n",
      "Objective: tensor(1595.6279, grad_fn=<AddBackward0>) (auc: 0.5721681976023087 )\n",
      "Objective: tensor(1595.6017, grad_fn=<AddBackward0>) (auc: 0.5721907197644972 )\n",
      "Objective: tensor(1595.7324, grad_fn=<AddBackward0>) (auc: 0.5720701571542196 )\n",
      "Objective: tensor(1595.5737, grad_fn=<AddBackward0>) (auc: 0.5723258051220634 )\n",
      "Objective: tensor(1595.5299, grad_fn=<AddBackward0>) (auc: 0.5723540411520376 )\n",
      "Objective: tensor(1595.8696, grad_fn=<AddBackward0>) (auc: 0.5723945143821861 )\n",
      "Objective: tensor(1595.4934, grad_fn=<AddBackward0>) (auc: 0.5724666043474154 )\n",
      "Objective: tensor(1595.4749, grad_fn=<AddBackward0>) (auc: 0.5724021804881319 )\n",
      "Objective: tensor(1595.4508, grad_fn=<AddBackward0>) (auc: 0.5725111725161436 )\n",
      "Objective: tensor(1595.4209, grad_fn=<AddBackward0>) (auc: 0.5725563596872153 )\n",
      "Objective: tensor(1595.4053, grad_fn=<AddBackward0>) (auc: 0.5725974043041424 )\n",
      "Objective: tensor(1595.3800, grad_fn=<AddBackward0>) (auc: 0.5726276401878415 )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective: tensor(1595.3650, grad_fn=<AddBackward0>) (auc: 0.5726611615455174 )\n",
      "Objective: tensor(1595.3646, grad_fn=<AddBackward0>) (auc: 0.5726828742431029 )\n",
      "Objective: tensor(1595.4302, grad_fn=<AddBackward0>) (auc: 0.5726752081371572 )\n",
      "Objective: tensor(1598.5745, grad_fn=<AddBackward0>) (auc: 0.5719258819926314 )\n",
      "Objective: tensor(1595.2947, grad_fn=<AddBackward0>) (auc: 0.5727843430118635 )\n",
      "Objective: tensor(1595.3020, grad_fn=<AddBackward0>) (auc: 0.5727519644277448 )\n",
      "Objective: tensor(1595.2610, grad_fn=<AddBackward0>) (auc: 0.572846338477338 )\n",
      "Objective: tensor(1595.3341, grad_fn=<AddBackward0>) (auc: 0.5728953348936001 )\n",
      "Objective: tensor(1595.5817, grad_fn=<AddBackward0>) (auc: 0.5722739517719084 )\n",
      "Objective: tensor(1595.2316, grad_fn=<AddBackward0>) (auc: 0.5728952872780352 )\n",
      "Objective: tensor(1595.2115, grad_fn=<AddBackward0>) (auc: 0.5729645203093716 )\n",
      "Objective: tensor(1596.0989, grad_fn=<AddBackward0>) (auc: 0.5730572278141941 )\n",
      "Objective: tensor(1595.2958, grad_fn=<AddBackward0>) (auc: 0.573021039984885 )\n",
      "Objective: tensor(1595.1678, grad_fn=<AddBackward0>) (auc: 0.5730192305934194 )\n",
      "Objective: tensor(1595.1548, grad_fn=<AddBackward0>) (auc: 0.5730388005905853 )\n",
      "Objective: tensor(1595.1689, grad_fn=<AddBackward0>) (auc: 0.5731125571005854 )\n",
      "Objective: tensor(1595.9136, grad_fn=<AddBackward0>) (auc: 0.5730663223870863 )\n",
      "Objective: tensor(1595.1700, grad_fn=<AddBackward0>) (auc: 0.5731807425894944 )\n",
      "Objective: tensor(1612.5665, grad_fn=<AddBackward0>) (auc: 0.5658261852847335 )\n",
      "Objective: tensor(1595.1071, grad_fn=<AddBackward0>) (auc: 0.573173743101457 )\n",
      "Objective: tensor(1595.0931, grad_fn=<AddBackward0>) (auc: 0.5732018362847365 )\n",
      "Objective: tensor(1595.6593, grad_fn=<AddBackward0>) (auc: 0.5733476351444009 )\n",
      "Objective: tensor(1595.1307, grad_fn=<AddBackward0>) (auc: 0.5731156997278675 )\n",
      "Objective: tensor(1595.0588, grad_fn=<AddBackward0>) (auc: 0.5732704503137294 )\n",
      "Objective: tensor(1595.0593, grad_fn=<AddBackward0>) (auc: 0.5733165897960988 )\n",
      "Objective: tensor(1595.0446, grad_fn=<AddBackward0>) (auc: 0.573311685392916 )\n",
      "Objective: tensor(1595.0524, grad_fn=<AddBackward0>) (auc: 0.5733752997875964 )\n",
      "Objective: tensor(1595.0389, grad_fn=<AddBackward0>) (auc: 0.5733802994219088 )\n",
      "Objective: tensor(1603.4718, grad_fn=<AddBackward0>) (auc: 0.56686429983026 )\n",
      "Objective: tensor(1595.0023, grad_fn=<AddBackward0>) (auc: 0.5733745379385584 )\n",
      "Objective: tensor(1594.9976, grad_fn=<AddBackward0>) (auc: 0.5734020597350594 )\n",
      "Objective: tensor(1595.0303, grad_fn=<AddBackward0>) (auc: 0.5734168681757372 )\n",
      "Objective: tensor(1594.9998, grad_fn=<AddBackward0>) (auc: 0.5734521989248786 )\n",
      "Objective: tensor(1595.0118, grad_fn=<AddBackward0>) (auc: 0.5735310503003208 )\n",
      "Objective: tensor(1594.9967, grad_fn=<AddBackward0>) (auc: 0.5735190035624061 )\n",
      "Objective: tensor(1594.9556, grad_fn=<AddBackward0>) (auc: 0.5734728640800368 )\n",
      "Objective: tensor(1594.9567, grad_fn=<AddBackward0>) (auc: 0.5735356213945495 )\n",
      "Objective: tensor(1595.1293, grad_fn=<AddBackward0>) (auc: 0.5735653811225998 )\n",
      "Objective: tensor(1594.9406, grad_fn=<AddBackward0>) (auc: 0.5735447159674416 )\n",
      "Objective: tensor(1594.9437, grad_fn=<AddBackward0>) (auc: 0.5735705236036069 )\n",
      "Objective: tensor(1594.9303, grad_fn=<AddBackward0>) (auc: 0.573585236813155 )\n",
      "Objective: tensor(1594.9196, grad_fn=<AddBackward0>) (auc: 0.573568904674401 )\n",
      "Objective: tensor(1594.9337, grad_fn=<AddBackward0>) (auc: 0.5736434706490039 )\n",
      "Objective: tensor(1594.9615, grad_fn=<AddBackward0>) (auc: 0.5736870388908697 )\n",
      "Objective: tensor(1596.1508, grad_fn=<AddBackward0>) (auc: 0.5710729919944904 )\n",
      "Objective: tensor(1594.8898, grad_fn=<AddBackward0>) (auc: 0.5735811418745753 )\n",
      "Objective: tensor(1594.8988, grad_fn=<AddBackward0>) (auc: 0.5736914195228386 )\n",
      "Objective: tensor(1594.9102, grad_fn=<AddBackward0>) (auc: 0.5735841892707276 )\n",
      "Objective: tensor(1594.9182, grad_fn=<AddBackward0>) (auc: 0.5737430824107341 )\n",
      "Objective: tensor(1598.0049, grad_fn=<AddBackward0>) (auc: 0.5731587441985196 )\n",
      "Objective: tensor(1594.8679, grad_fn=<AddBackward0>) (auc: 0.5736716590634132 )\n",
      "Objective: tensor(1594.8574, grad_fn=<AddBackward0>) (auc: 0.5736464704295915 )\n",
      "Objective: tensor(1594.8804, grad_fn=<AddBackward0>) (auc: 0.5737685091223803 )\n",
      "Objective: tensor(1594.8507, grad_fn=<AddBackward0>) (auc: 0.5736901815181518 )\n",
      "Objective: tensor(1594.8400, grad_fn=<AddBackward0>) (auc: 0.5736672308158794 )\n",
      "Objective: tensor(1594.8333, grad_fn=<AddBackward0>) (auc: 0.573668944976215 )\n",
      "Objective: tensor(1594.8300, grad_fn=<AddBackward0>) (auc: 0.5736881816644268 )\n",
      "Objective: tensor(1594.8223, grad_fn=<AddBackward0>) (auc: 0.573693276529869 )\n",
      "Objective: tensor(1594.8192, grad_fn=<AddBackward0>) (auc: 0.5736978476240977 )\n",
      "Objective: tensor(1596.1073, grad_fn=<AddBackward0>) (auc: 0.5711257976559428 )\n",
      "Objective: tensor(1594.8092, grad_fn=<AddBackward0>) (auc: 0.573717608083523 )\n",
      "Objective: tensor(1594.8088, grad_fn=<AddBackward0>) (auc: 0.5737669378087393 )\n",
      "Objective: tensor(1594.9132, grad_fn=<AddBackward0>) (auc: 0.5737705089761054 )\n",
      "Objective: tensor(1594.7980, grad_fn=<AddBackward0>) (auc: 0.5737666997309149 )\n",
      "Objective: tensor(1598.7955, grad_fn=<AddBackward0>) (auc: 0.5734500086088942 )\n",
      "Objective: tensor(1595.1550, grad_fn=<AddBackward0>) (auc: 0.5690228086174268 )\n",
      "Objective: tensor(1594.7805, grad_fn=<AddBackward0>) (auc: 0.5737766513839749 )\n",
      "Objective: tensor(1594.7821, grad_fn=<AddBackward0>) (auc: 0.5738063158808956 )\n",
      "Objective: tensor(1594.8234, grad_fn=<AddBackward0>) (auc: 0.5737456060356728 )\n",
      "Objective: tensor(1594.7687, grad_fn=<AddBackward0>) (auc: 0.5738099346638266 )\n",
      "Objective: tensor(1594.7675, grad_fn=<AddBackward0>) (auc: 0.5738239812554663 )\n",
      "Objective: tensor(1594.7611, grad_fn=<AddBackward0>) (auc: 0.5738339805240912 )\n",
      "Objective: tensor(1595.4667, grad_fn=<AddBackward0>) (auc: 0.574045012707642 )\n",
      "Objective: tensor(1594.7512, grad_fn=<AddBackward0>) (auc: 0.5738505507406696 )\n",
      "Objective: tensor(1594.7510, grad_fn=<AddBackward0>) (auc: 0.5738755012966671 )\n",
      "Objective: tensor(1594.7465, grad_fn=<AddBackward0>) (auc: 0.5738711682802629 )\n",
      "Objective: tensor(1595.0736, grad_fn=<AddBackward0>) (auc: 0.5737029424895398 )\n",
      "Objective: tensor(1594.7371, grad_fn=<AddBackward0>) (auc: 0.573873549058507 )\n",
      "Objective: tensor(1594.7524, grad_fn=<AddBackward0>) (auc: 0.5738699302755761 )\n",
      "Objective: tensor(1594.7767, grad_fn=<AddBackward0>) (auc: 0.5739261166421351 )\n",
      "Objective: tensor(1594.7242, grad_fn=<AddBackward0>) (auc: 0.5738985472300693 )\n",
      "Objective: tensor(1594.7209, grad_fn=<AddBackward0>) (auc: 0.5738996423880616 )\n",
      "Objective: tensor(1594.7173, grad_fn=<AddBackward0>) (auc: 0.5739121652816251 )\n",
      "Objective: tensor(1594.7185, grad_fn=<AddBackward0>) (auc: 0.5739355921395464 )\n",
      "Objective: tensor(1594.7142, grad_fn=<AddBackward0>) (auc: 0.5739351635994624 )\n",
      "Objective: tensor(1594.8030, grad_fn=<AddBackward0>) (auc: 0.5739795413059311 )\n",
      "Objective: tensor(1595.2463, grad_fn=<AddBackward0>) (auc: 0.5739318305099208 )\n",
      "Objective: tensor(1594.7212, grad_fn=<AddBackward0>) (auc: 0.5739830172421674 )\n",
      "Objective: tensor(1594.7002, grad_fn=<AddBackward0>) (auc: 0.5739579714550402 )\n",
      "Objective: tensor(1602.3077, grad_fn=<AddBackward0>) (auc: 0.5714012536987771 )\n",
      "Objective: tensor(1594.7072, grad_fn=<AddBackward0>) (auc: 0.5740113008877066 )\n",
      "Objective: tensor(1594.6908, grad_fn=<AddBackward0>) (auc: 0.5739945402088685 )\n",
      "Objective: tensor(1597.7009, grad_fn=<AddBackward0>) (auc: 0.5725309805911339 )\n",
      "Objective: tensor(1597.1183, grad_fn=<AddBackward0>) (auc: 0.5691950341156 )\n",
      "Objective: tensor(1594.8618, grad_fn=<AddBackward0>) (auc: 0.5738019352489265 )\n",
      "Objective: tensor(1594.6775, grad_fn=<AddBackward0>) (auc: 0.5740054441732262 )\n",
      "Objective: tensor(1594.6750, grad_fn=<AddBackward0>) (auc: 0.5740038252440203 )\n",
      "Objective: tensor(1594.9640, grad_fn=<AddBackward0>) (auc: 0.5735910459120704 )\n",
      "Objective: tensor(1594.6719, grad_fn=<AddBackward0>) (auc: 0.5740500599575193 )\n",
      "Objective: tensor(1594.6847, grad_fn=<AddBackward0>) (auc: 0.57405777367903 )\n",
      "Objective: tensor(1594.9735, grad_fn=<AddBackward0>) (auc: 0.573708846819585 )\n",
      "Objective: tensor(1594.6681, grad_fn=<AddBackward0>) (auc: 0.5740465364057181 )\n",
      "Objective: tensor(1594.6562, grad_fn=<AddBackward0>) (auc: 0.5740342039744141 )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective: tensor(1594.6655, grad_fn=<AddBackward0>) (auc: 0.5741181025997336 )\n",
      "Objective: tensor(1594.6531, grad_fn=<AddBackward0>) (auc: 0.5740591545304115 )\n",
      "Objective: tensor(1594.6467, grad_fn=<AddBackward0>) (auc: 0.5740511551155115 )\n",
      "Objective: tensor(1594.6447, grad_fn=<AddBackward0>) (auc: 0.5740324421985135 )\n",
      "Objective: tensor(1594.6641, grad_fn=<AddBackward0>) (auc: 0.5740383941441236 )\n",
      "Objective: tensor(1594.6376, grad_fn=<AddBackward0>) (auc: 0.5740499647263896 )\n",
      "Objective: tensor(1594.6367, grad_fn=<AddBackward0>) (auc: 0.5740762009026388 )\n",
      "Objective: tensor(1594.6423, grad_fn=<AddBackward0>) (auc: 0.5741058653995594 )\n",
      "Objective: tensor(1594.6350, grad_fn=<AddBackward0>) (auc: 0.5740920568857439 )\n",
      "Objective: tensor(1594.6414, grad_fn=<AddBackward0>) (auc: 0.574114769510192 )\n",
      "Objective: tensor(1594.6603, grad_fn=<AddBackward0>) (auc: 0.5741316254201598 )\n",
      "Objective: tensor(1594.6239, grad_fn=<AddBackward0>) (auc: 0.5740806767657374 )\n",
      "Objective: tensor(1594.6212, grad_fn=<AddBackward0>) (auc: 0.5740916759612249 )\n",
      "Objective: tensor(1595.3547, grad_fn=<AddBackward0>) (auc: 0.5724981734669311 )\n",
      "Objective: tensor(1594.6196, grad_fn=<AddBackward0>) (auc: 0.574108769949017 )\n",
      "Objective: tensor(1597.6329, grad_fn=<AddBackward0>) (auc: 0.572517410155143 )\n",
      "Objective: tensor(1594.6541, grad_fn=<AddBackward0>) (auc: 0.5741461005518834 )\n",
      "Objective: tensor(1594.6089, grad_fn=<AddBackward0>) (auc: 0.5741066272485975 )\n",
      "Objective: tensor(1594.6078, grad_fn=<AddBackward0>) (auc: 0.5741022942321934 )\n",
      "Objective: tensor(1594.6846, grad_fn=<AddBackward0>) (auc: 0.5740411558468866 )\n",
      "Objective: tensor(1597.0303, grad_fn=<AddBackward0>) (auc: 0.5736737065327031 )\n",
      "Objective: tensor(1595.6439, grad_fn=<AddBackward0>) (auc: 0.5733539203989652 )\n",
      "Objective: tensor(1604.6217, grad_fn=<AddBackward0>) (auc: 0.5681286835400992 )\n",
      "Objective: tensor(1594.6130, grad_fn=<AddBackward0>) (auc: 0.5741264829391526 )\n",
      "Objective: tensor(1594.5955, grad_fn=<AddBackward0>) (auc: 0.5741265305547175 )\n",
      "Objective: tensor(1594.5912, grad_fn=<AddBackward0>) (auc: 0.5741217213826646 )\n",
      "Objective: tensor(1602.5682, grad_fn=<AddBackward0>) (auc: 0.5702390529911716 )\n",
      "Objective: tensor(1594.5870, grad_fn=<AddBackward0>) (auc: 0.574129149410786 )\n",
      "Objective: tensor(1594.5867, grad_fn=<AddBackward0>) (auc: 0.5741321015758086 )\n",
      "Objective: tensor(1594.5906, grad_fn=<AddBackward0>) (auc: 0.574136720285602 )\n",
      "Objective: tensor(1594.5905, grad_fn=<AddBackward0>) (auc: 0.5741686703296368 )\n",
      "Objective: tensor(1594.5796, grad_fn=<AddBackward0>) (auc: 0.5741289113329615 )\n",
      "Objective: tensor(1594.5983, grad_fn=<AddBackward0>) (auc: 0.5741557188959893 )\n",
      "Objective: tensor(1594.5876, grad_fn=<AddBackward0>) (auc: 0.5741708130300565 )\n",
      "Objective: tensor(1594.5842, grad_fn=<AddBackward0>) (auc: 0.574172574805957 )\n",
      "Objective: tensor(1594.8901, grad_fn=<AddBackward0>) (auc: 0.5741817169944141 )\n",
      "Objective: tensor(1594.5698, grad_fn=<AddBackward0>) (auc: 0.5741409104553115 )\n",
      "Objective: tensor(1605.0784, grad_fn=<AddBackward0>) (auc: 0.5686166954645603 )\n",
      "Objective: tensor(1594.5715, grad_fn=<AddBackward0>) (auc: 0.5741841930037879 )\n",
      "Objective: tensor(1594.7922, grad_fn=<AddBackward0>) (auc: 0.5740507741909925 )\n",
      "Objective: tensor(1594.6470, grad_fn=<AddBackward0>) (auc: 0.5741689560230261 )\n",
      "Objective: tensor(1594.5637, grad_fn=<AddBackward0>) (auc: 0.5741686703296369 )\n",
      "Objective: tensor(1594.5608, grad_fn=<AddBackward0>) (auc: 0.5741846691594367 )\n",
      "Objective: tensor(1594.5577, grad_fn=<AddBackward0>) (auc: 0.5741554808181649 )\n",
      "Objective: tensor(1595.3782, grad_fn=<AddBackward0>) (auc: 0.5740162529064541 )\n",
      "Objective: tensor(1594.5571, grad_fn=<AddBackward0>) (auc: 0.57417738397801 )\n",
      "Objective: tensor(1594.5519, grad_fn=<AddBackward0>) (auc: 0.5741613851482101 )\n",
      "Objective: tensor(1594.5521, grad_fn=<AddBackward0>) (auc: 0.5741831930769254 )\n",
      "Objective: tensor(1594.5782, grad_fn=<AddBackward0>) (auc: 0.5742199522930133 )\n",
      "Objective: tensor(1594.5460, grad_fn=<AddBackward0>) (auc: 0.574174574659682 )\n",
      "Objective: tensor(1594.5638, grad_fn=<AddBackward0>) (auc: 0.5741168169794819 )\n",
      "Objective: tensor(1594.5424, grad_fn=<AddBackward0>) (auc: 0.5741584329831875 )\n",
      "Objective: tensor(1594.8853, grad_fn=<AddBackward0>) (auc: 0.574114817125757 )\n",
      "Objective: tensor(1594.5405, grad_fn=<AddBackward0>) (auc: 0.5741862880886427 )\n",
      "Objective: tensor(1594.5972, grad_fn=<AddBackward0>) (auc: 0.5741655753179196 )\n",
      "Objective: tensor(1594.5760, grad_fn=<AddBackward0>) (auc: 0.5742263803942721 )\n",
      "Objective: tensor(1594.5383, grad_fn=<AddBackward0>) (auc: 0.5742060009325032 )\n",
      "Objective: tensor(1594.5364, grad_fn=<AddBackward0>) (auc: 0.5741730033460409 )\n",
      "Objective: tensor(1594.5309, grad_fn=<AddBackward0>) (auc: 0.574171812956919 )\n",
      "Objective: tensor(1594.5326, grad_fn=<AddBackward0>) (auc: 0.574192954267726 )\n",
      "Objective: tensor(1594.5344, grad_fn=<AddBackward0>) (auc: 0.5742039058476485 )\n",
      "Objective: tensor(1594.9974, grad_fn=<AddBackward0>) (auc: 0.573835647068862 )\n",
      "Objective: tensor(1594.5305, grad_fn=<AddBackward0>) (auc: 0.574213667038449 )\n",
      "Objective: tensor(1594.5233, grad_fn=<AddBackward0>) (auc: 0.574187002322116 )\n",
      "Objective: tensor(1594.5223, grad_fn=<AddBackward0>) (auc: 0.5742080484017931 )\n",
      "Objective: tensor(1595.0186, grad_fn=<AddBackward0>) (auc: 0.572852766578597 )\n",
      "Objective: tensor(1594.8134, grad_fn=<AddBackward0>) (auc: 0.573321684661541 )\n",
      "Objective: tensor(1594.5195, grad_fn=<AddBackward0>) (auc: 0.5741899068715736 )\n",
      "Objective: tensor(1594.5159, grad_fn=<AddBackward0>) (auc: 0.5741940494257182 )\n",
      "Objective: tensor(1594.5182, grad_fn=<AddBackward0>) (auc: 0.5742137146540138 )\n",
      "Objective: tensor(1594.5149, grad_fn=<AddBackward0>) (auc: 0.5742192856751049 )\n",
      "Objective: tensor(1594.5400, grad_fn=<AddBackward0>) (auc: 0.5742321418776226 )\n",
      "Objective: tensor(1594.5107, grad_fn=<AddBackward0>) (auc: 0.5741965730506569 )\n",
      "Objective: tensor(1594.5365, grad_fn=<AddBackward0>) (auc: 0.5741943827346724 )\n",
      "Objective: tensor(1595.1021, grad_fn=<AddBackward0>) (auc: 0.5737870791926838 )\n",
      "Objective: tensor(1594.9650, grad_fn=<AddBackward0>) (auc: 0.5739752082895271 )\n",
      "Objective: tensor(1594.5068, grad_fn=<AddBackward0>) (auc: 0.5742123814181972 )\n",
      "Objective: tensor(1594.5089, grad_fn=<AddBackward0>) (auc: 0.5742469027027356 )\n",
      "Objective: tensor(1594.5148, grad_fn=<AddBackward0>) (auc: 0.5742545211931165 )\n",
      "Objective: tensor(1597.1901, grad_fn=<AddBackward0>) (auc: 0.5734071546005016 )\n",
      "Objective: tensor(1594.5139, grad_fn=<AddBackward0>) (auc: 0.5742668060088557 )\n",
      "Objective: tensor(1594.4999, grad_fn=<AddBackward0>) (auc: 0.5742293325592946 )\n",
      "Objective: tensor(1594.8660, grad_fn=<AddBackward0>) (auc: 0.5741263877080228 )\n",
      "Objective: tensor(1594.4989, grad_fn=<AddBackward0>) (auc: 0.5742250947740203 )\n",
      "Objective: tensor(1594.4987, grad_fn=<AddBackward0>) (auc: 0.5742346178869964 )\n",
      "Objective: tensor(1594.4938, grad_fn=<AddBackward0>) (auc: 0.5742162382789525 )\n",
      "Objective: tensor(1605.6056, grad_fn=<AddBackward0>) (auc: 0.5684918950690083 )\n",
      "Objective: tensor(1594.4915, grad_fn=<AddBackward0>) (auc: 0.574236808202981 )\n",
      "Objective: tensor(1594.4994, grad_fn=<AddBackward0>) (auc: 0.5742700914828325 )\n",
      "Objective: tensor(1594.4977, grad_fn=<AddBackward0>) (auc: 0.5742625682235813 )\n",
      "Objective: tensor(1594.6625, grad_fn=<AddBackward0>) (auc: 0.5740061107911345 )\n",
      "Objective: tensor(1601.4939, grad_fn=<AddBackward0>) (auc: 0.5723332331501848 )\n",
      "Objective: tensor(1594.6884, grad_fn=<AddBackward0>) (auc: 0.5738809294710634 )\n",
      "Objective: tensor(1594.4844, grad_fn=<AddBackward0>) (auc: 0.5742526641860862 )\n",
      "Objective: tensor(1594.7382, grad_fn=<AddBackward0>) (auc: 0.5740081106448595 )\n",
      "Objective: tensor(1596.0487, grad_fn=<AddBackward0>) (auc: 0.5734292006070413 )\n",
      "Objective: tensor(1595.1783, grad_fn=<AddBackward0>) (auc: 0.5742898995578228 )\n",
      "\n",
      "\n",
      "Solved.\n",
      "Now computing Z=HW^T, then will compute S...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3871, 0.4088, 0.4067,  ..., 0.3826, 0.5974, 0.5191],\n",
       "         [0.4975, 0.5516, 0.5490,  ..., 0.4624, 0.4220, 0.4436],\n",
       "         [0.5569, 0.5417, 0.5454,  ..., 0.5413, 0.5931, 0.4601],\n",
       "         ...,\n",
       "         [0.5066, 0.5552, 0.5518,  ..., 0.4879, 0.4778, 0.5011],\n",
       "         [0.4658, 0.5621, 0.5572,  ..., 0.4510, 0.4936, 0.4817],\n",
       "         [0.4836, 0.5633, 0.5587,  ..., 0.4637, 0.4607, 0.4825]],\n",
       "        grad_fn=<MmBackward>),\n",
       " tensor([[ 0.4810, -1.4927, -1.5319,  1.3948, -0.7323],\n",
       "         [ 0.6244,  1.0643,  0.1159, -0.6640, -0.3744],\n",
       "         [-0.0813, -2.7470, -0.9164, -0.2611,  0.7909],\n",
       "         [ 0.8847, -1.5096, -1.8999,  0.8597, -1.3607],\n",
       "         [ 2.6611, -0.1185, -0.4285,  0.9679,  0.3567],\n",
       "         [ 0.5265, -0.2179,  0.1697, -2.0756,  0.9813],\n",
       "         [ 1.2622,  1.4758, -1.1352, -1.0883, -1.2910],\n",
       "         [-0.7924,  2.8336,  1.2583,  0.2510,  0.5547],\n",
       "         [-0.9089, -0.0950, -0.7031,  1.5239, -0.1418],\n",
       "         [ 0.3271,  0.2612,  0.3762,  0.2002, -0.8346],\n",
       "         [-0.5726, -2.0059,  1.8507,  1.0686,  0.1596],\n",
       "         [ 0.1358,  0.4363,  0.7505, -0.2460,  1.6815],\n",
       "         [-0.7709,  0.3010, -0.9032, -1.0815, -1.5444],\n",
       "         [-0.1825, -0.5014, -0.6542,  0.9028,  0.0697],\n",
       "         [-0.2601,  0.0654,  1.1473, -0.3000, -0.2002],\n",
       "         [-0.3800,  0.9315,  1.3192,  0.3657,  3.6719],\n",
       "         [ 0.5278, -0.4597, -0.2512,  0.3188,  0.5074],\n",
       "         [ 0.6664,  0.2943, -0.1942, -0.7102, -0.2640],\n",
       "         [ 0.5895,  0.0995, -0.0854,  0.0449,  0.5401],\n",
       "         [-1.5314,  0.8448,  0.2192, -0.9633,  0.2746]], requires_grad=True),\n",
       " tensor([[-0.3991,  2.0881,  1.0819, -1.1359,  1.5712],\n",
       "         [ 1.1736,  0.6016, -0.6197, -1.8834, -1.5960],\n",
       "         [-0.2560,  0.1994, -0.1181,  1.2941, -0.0706],\n",
       "         [ 0.2507, -2.2007,  0.6372,  0.8389, -0.2895],\n",
       "         [ 0.1771,  0.2048,  0.4374, -0.0770,  0.8280],\n",
       "         [-0.1983,  2.4423,  0.1774, -0.2431, -1.0397],\n",
       "         [ 0.0055, -0.4674,  0.7844, -0.6049,  2.0551],\n",
       "         [-0.0093,  0.0968, -0.2191,  0.2224, -0.2215],\n",
       "         [ 2.3240, -1.5794, -0.8757,  0.8761,  0.7773],\n",
       "         [-1.4049,  0.4605,  1.2199, -1.4965,  0.1678],\n",
       "         [-0.7301,  1.7783,  0.0695, -0.3196,  1.2060],\n",
       "         [ 1.2311,  0.9489,  1.4000,  0.9680,  1.0545],\n",
       "         [-1.4996, -1.0030, -0.1643,  0.5972,  0.5698],\n",
       "         [ 0.4485, -0.6832, -0.2065, -1.1321, -0.4099],\n",
       "         [-0.2988, -2.1682,  0.0057,  1.1727,  1.8660],\n",
       "         [-0.2483, -0.9650,  1.3585,  0.1282,  0.6247],\n",
       "         [ 0.5737,  1.2306,  0.9990,  1.0253,  0.2195],\n",
       "         [ 0.4349,  1.1407, -2.6845, -0.4591, -1.3619],\n",
       "         [-0.8743, -0.2874, -0.7026,  0.7402, -1.9052],\n",
       "         [-0.4776, -0.1291, -1.0863,  0.3160, -2.5787]], requires_grad=True))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pu_learner.pu_learning(embeddings_drugs, embeddings_targets, dp_net, pos_train, neg_train, k=5, alpha=1, gamma=0.02, lr=5e-3, maxiter=300000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.94896203, 0.00272402, 0.99999952, 0.12069056, 0.00230766,\n",
       "       0.0206879 , 0.99998558, 1.        , 0.99999964, 0.99995112,\n",
       "       0.99995661, 0.9999032 , 0.00431027, 0.9999994 , 0.01488216,\n",
       "       0.61676306, 0.0026521 , 0.24169302, 0.99999976, 0.99999976])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0693, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0449, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0a199f14c799>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../repr/X.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "np.savetxt('../repr/X.csv', X.detach().numpy(), delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
