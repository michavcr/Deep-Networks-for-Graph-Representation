{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../src/\")\n",
    "\n",
    "from utils import *\n",
    "from pu_learning import *\n",
    "from autoencoder import *\n",
    "from dgnr import *\n",
    "from random_graphs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50725"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_rand_drugs = 80\n",
    "N_rand_targets = 100\n",
    "\n",
    "density=0.2\n",
    "density_dp = 0.5\n",
    "#generate random matrices in M({0,1})\n",
    "#1. a drug-drug network (drug similarities)\n",
    "#2. a protein-protein network (protein similarities)\n",
    "#3. a drug-protein network (drug-target known relationships)\n",
    "dd_net = random_graph_with_fixed_components(density, [20,20,20,20])\n",
    "pp_net = random_graph_with_fixed_components(0.4, [10 for i in range(10)])\n",
    "dp_net = random_graph(density_dp, size=(N_rand_drugs,N_rand_targets))\n",
    "\n",
    "np.sum(dp_net)/(N_rand_drugs*N_rand_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../src/dgnr.py:86: RuntimeWarning: divide by zero encountered in log\n",
      "  P = np.log(P)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "      DropoutNoise-1                   [-1, 80]               0\n",
      "            Linear-2                   [-1, 60]           4,860\n",
      "           Sigmoid-3                   [-1, 60]               0\n",
      "        BasicBlock-4                   [-1, 60]               0\n",
      "            Linear-5                   [-1, 40]           2,440\n",
      "           Sigmoid-6                   [-1, 40]               0\n",
      "        BasicBlock-7                   [-1, 40]               0\n",
      "            Linear-8                   [-1, 20]             820\n",
      "           Sigmoid-9                   [-1, 20]               0\n",
      "       BasicBlock-10                   [-1, 20]               0\n",
      "           Linear-11                   [-1, 40]             840\n",
      "          Sigmoid-12                   [-1, 40]               0\n",
      "       BasicBlock-13                   [-1, 40]               0\n",
      "           Linear-14                   [-1, 60]           2,460\n",
      "          Sigmoid-15                   [-1, 60]               0\n",
      "       BasicBlock-16                   [-1, 60]               0\n",
      "           Linear-17                   [-1, 80]           4,880\n",
      "          Sigmoid-18                   [-1, 80]               0\n",
      "       BasicBlock-19                   [-1, 80]               0\n",
      "================================================================\n",
      "Total params: 16,300\n",
      "Trainable params: 16,300\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.06\n",
      "Estimated Total Size (MB): 0.07\n",
      "----------------------------------------------------------------\n",
      "[1,    80] loss: 0.471\n",
      "[2,    80] loss: 0.437\n",
      "[3,    80] loss: 0.435\n",
      "[4,    80] loss: 0.435\n",
      "[5,    80] loss: 0.435\n",
      "[6,    80] loss: 0.435\n",
      "[7,    80] loss: 0.435\n",
      "[8,    80] loss: 0.435\n",
      "[9,    80] loss: 0.435\n",
      "[10,    80] loss: 0.434\n",
      "[11,    80] loss: 0.434\n",
      "[12,    80] loss: 0.432\n",
      "[13,    80] loss: 0.428\n",
      "[14,    80] loss: 0.420\n",
      "[15,    80] loss: 0.410\n",
      "[16,    80] loss: 0.403\n",
      "[17,    80] loss: 0.398\n",
      "[18,    80] loss: 0.396\n",
      "[19,    80] loss: 0.395\n",
      "[20,    80] loss: 0.392\n",
      "[21,    80] loss: 0.391\n",
      "[22,    80] loss: 0.392\n",
      "[23,    80] loss: 0.392\n",
      "[24,    80] loss: 0.390\n",
      "[25,    80] loss: 0.390\n",
      "[26,    80] loss: 0.390\n",
      "[27,    80] loss: 0.389\n",
      "[28,    80] loss: 0.389\n",
      "[29,    80] loss: 0.388\n",
      "[30,    80] loss: 0.388\n",
      "[31,    80] loss: 0.387\n",
      "[32,    80] loss: 0.386\n",
      "[33,    80] loss: 0.385\n",
      "[34,    80] loss: 0.385\n",
      "[35,    80] loss: 0.384\n",
      "[36,    80] loss: 0.384\n",
      "[37,    80] loss: 0.383\n",
      "[38,    80] loss: 0.382\n",
      "[39,    80] loss: 0.380\n",
      "[40,    80] loss: 0.378\n",
      "[41,    80] loss: 0.375\n",
      "[42,    80] loss: 0.371\n",
      "[43,    80] loss: 0.367\n",
      "[44,    80] loss: 0.363\n",
      "[45,    80] loss: 0.359\n",
      "[46,    80] loss: 0.355\n",
      "[47,    80] loss: 0.352\n",
      "[48,    80] loss: 0.349\n",
      "[49,    80] loss: 0.346\n",
      "[50,    80] loss: 0.344\n",
      "[51,    80] loss: 0.342\n",
      "[52,    80] loss: 0.341\n",
      "[53,    80] loss: 0.339\n",
      "[54,    80] loss: 0.338\n",
      "[55,    80] loss: 0.337\n",
      "[56,    80] loss: 0.336\n",
      "[57,    80] loss: 0.336\n",
      "[58,    80] loss: 0.335\n",
      "[59,    80] loss: 0.334\n",
      "[60,    80] loss: 0.334\n",
      "[61,    80] loss: 0.333\n",
      "[62,    80] loss: 0.333\n",
      "[63,    80] loss: 0.332\n",
      "[64,    80] loss: 0.332\n",
      "[65,    80] loss: 0.333\n",
      "[66,    80] loss: 0.331\n",
      "[67,    80] loss: 0.331\n",
      "[68,    80] loss: 0.332\n",
      "[69,    80] loss: 0.330\n",
      "[70,    80] loss: 0.329\n",
      "[71,    80] loss: 0.328\n",
      "[72,    80] loss: 0.327\n",
      "[73,    80] loss: 0.327\n",
      "[74,    80] loss: 0.325\n",
      "[75,    80] loss: 0.324\n",
      "[76,    80] loss: 0.322\n",
      "[77,    80] loss: 0.320\n",
      "[78,    80] loss: 0.317\n",
      "[79,    80] loss: 0.315\n",
      "[80,    80] loss: 0.313\n",
      "[81,    80] loss: 0.310\n",
      "[82,    80] loss: 0.308\n",
      "[83,    80] loss: 0.305\n",
      "[84,    80] loss: 0.303\n",
      "[85,    80] loss: 0.300\n",
      "[86,    80] loss: 0.299\n",
      "[87,    80] loss: 0.297\n",
      "[88,    80] loss: 0.295\n",
      "[89,    80] loss: 0.294\n",
      "[90,    80] loss: 0.294\n",
      "[91,    80] loss: 0.293\n",
      "[92,    80] loss: 0.292\n",
      "[93,    80] loss: 0.291\n",
      "[94,    80] loss: 0.290\n",
      "[95,    80] loss: 0.289\n",
      "[96,    80] loss: 0.289\n",
      "[97,    80] loss: 0.290\n",
      "[98,    80] loss: 0.304\n",
      "[99,    80] loss: 0.289\n",
      "[100,    80] loss: 0.288\n",
      "Finished Training\n",
      "[*] Visualizing an example's output...\n",
      "tensor([[0.0000, 0.0000, 1.9770, 0.2502, 0.0000, 0.0000, 2.3943, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.7934, 0.0000, 0.0000, 0.0000,\n",
      "         0.4519, 0.0000, 0.0000, 0.0000, 0.0000, 2.6138, 2.0942, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.5818, 0.0239, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 2.1189, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "tensor([[8.9528e-01, 1.1329e-04, 9.2820e-01, 8.3162e-01, 9.1386e-01, 9.4696e-05,\n",
      "         9.5875e-01, 1.2532e-01, 6.8252e-04, 1.5061e-03, 5.6799e-04, 8.6703e-01,\n",
      "         8.2832e-05, 1.2060e-03, 8.5715e-01, 5.0961e-05, 1.6385e-04, 1.8530e-04,\n",
      "         9.4638e-01, 2.5899e-01, 3.9049e-01, 6.7441e-02, 3.1340e-04, 8.3843e-01,\n",
      "         8.7862e-01, 8.2086e-02, 3.3482e-02, 1.6293e-03, 2.1523e-04, 6.1666e-02,\n",
      "         4.4958e-04, 8.5411e-01, 1.0787e-04, 2.8140e-04, 1.0056e-01, 2.1347e-04,\n",
      "         1.3827e-04, 2.9427e-03, 1.5808e-04, 3.2689e-03, 8.7137e-02, 8.8609e-01,\n",
      "         6.1153e-01, 8.5577e-01, 5.0367e-04, 6.9629e-01, 2.4635e-04, 6.1531e-02,\n",
      "         8.1452e-02, 2.9174e-04, 5.7281e-04, 7.6081e-04, 1.9096e-04, 1.0823e-01,\n",
      "         7.0012e-02, 8.8133e-02, 2.3022e-03, 1.2142e-01, 6.7401e-01, 1.1234e-01,\n",
      "         5.7066e-02, 3.5010e-04, 5.4130e-03, 8.0104e-05, 8.4910e-01, 3.2617e-03,\n",
      "         3.0913e-04, 7.9266e-02, 3.3583e-03, 8.7769e-02, 8.7344e-02, 4.3529e-04,\n",
      "         8.6777e-02, 7.9998e-01, 2.7468e-04, 1.2503e-01, 7.3991e-04, 1.5610e-03,\n",
      "         2.5899e-03, 7.0781e-05]], grad_fn=<SigmoidBackward>)\n",
      "0.2653703\n",
      "[*] Getting the embeddings and visualizing t-SNE...\n",
      "'dngr_pipeline'  9832.81 ms\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings_drugs, _, _ = dngr_pipeline(dd_net, N_rand_drugs, [60, 40, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "      DropoutNoise-1                  [-1, 100]               0\n",
      "            Linear-2                   [-1, 60]           6,060\n",
      "           Sigmoid-3                   [-1, 60]               0\n",
      "        BasicBlock-4                   [-1, 60]               0\n",
      "            Linear-5                   [-1, 40]           2,440\n",
      "           Sigmoid-6                   [-1, 40]               0\n",
      "        BasicBlock-7                   [-1, 40]               0\n",
      "            Linear-8                   [-1, 20]             820\n",
      "           Sigmoid-9                   [-1, 20]               0\n",
      "       BasicBlock-10                   [-1, 20]               0\n",
      "           Linear-11                   [-1, 40]             840\n",
      "          Sigmoid-12                   [-1, 40]               0\n",
      "       BasicBlock-13                   [-1, 40]               0\n",
      "           Linear-14                   [-1, 60]           2,460\n",
      "          Sigmoid-15                   [-1, 60]               0\n",
      "       BasicBlock-16                   [-1, 60]               0\n",
      "           Linear-17                  [-1, 100]           6,100\n",
      "          Sigmoid-18                  [-1, 100]               0\n",
      "       BasicBlock-19                  [-1, 100]               0\n",
      "================================================================\n",
      "Total params: 18,720\n",
      "Trainable params: 18,720\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.07\n",
      "Estimated Total Size (MB): 0.08\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../src/dgnr.py:86: RuntimeWarning: divide by zero encountered in log\n",
      "  P = np.log(P)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.483\n",
      "[2,   100] loss: 0.447\n",
      "[3,   100] loss: 0.446\n",
      "[4,   100] loss: 0.446\n",
      "[5,   100] loss: 0.446\n",
      "[6,   100] loss: 0.446\n",
      "[7,   100] loss: 0.446\n",
      "[8,   100] loss: 0.446\n",
      "[9,   100] loss: 0.446\n",
      "[10,   100] loss: 0.445\n",
      "[11,   100] loss: 0.445\n",
      "[12,   100] loss: 0.444\n",
      "[13,   100] loss: 0.441\n",
      "[14,   100] loss: 0.436\n",
      "[15,   100] loss: 0.430\n",
      "[16,   100] loss: 0.424\n",
      "[17,   100] loss: 0.420\n",
      "[18,   100] loss: 0.418\n",
      "[19,   100] loss: 0.417\n",
      "[20,   100] loss: 0.416\n",
      "[21,   100] loss: 0.416\n",
      "[22,   100] loss: 0.416\n",
      "[23,   100] loss: 0.415\n",
      "[24,   100] loss: 0.414\n",
      "[25,   100] loss: 0.414\n",
      "[26,   100] loss: 0.414\n",
      "[27,   100] loss: 0.413\n",
      "[28,   100] loss: 0.412\n",
      "[29,   100] loss: 0.410\n",
      "[30,   100] loss: 0.408\n",
      "[31,   100] loss: 0.406\n",
      "[32,   100] loss: 0.402\n",
      "[33,   100] loss: 0.399\n",
      "[34,   100] loss: 0.396\n",
      "[35,   100] loss: 0.392\n",
      "[36,   100] loss: 0.389\n",
      "[37,   100] loss: 0.385\n",
      "[38,   100] loss: 0.382\n",
      "[39,   100] loss: 0.378\n",
      "[40,   100] loss: 0.375\n",
      "[41,   100] loss: 0.371\n",
      "[42,   100] loss: 0.368\n",
      "[43,   100] loss: 0.364\n",
      "[44,   100] loss: 0.361\n",
      "[45,   100] loss: 0.358\n",
      "[46,   100] loss: 0.355\n",
      "[47,   100] loss: 0.353\n",
      "[48,   100] loss: 0.350\n",
      "[49,   100] loss: 0.348\n",
      "[50,   100] loss: 0.346\n",
      "[51,   100] loss: 0.344\n",
      "[52,   100] loss: 0.341\n",
      "[53,   100] loss: 0.339\n",
      "[54,   100] loss: 0.337\n",
      "[55,   100] loss: 0.335\n",
      "[56,   100] loss: 0.333\n",
      "[57,   100] loss: 0.331\n",
      "[58,   100] loss: 0.331\n",
      "[59,   100] loss: 0.328\n",
      "[60,   100] loss: 0.326\n",
      "[61,   100] loss: 0.325\n",
      "[62,   100] loss: 0.324\n",
      "[63,   100] loss: 0.322\n",
      "[64,   100] loss: 0.321\n",
      "[65,   100] loss: 0.319\n",
      "[66,   100] loss: 0.319\n",
      "[67,   100] loss: 0.318\n",
      "[68,   100] loss: 0.317\n",
      "[69,   100] loss: 0.315\n",
      "[70,   100] loss: 0.314\n",
      "[71,   100] loss: 0.311\n",
      "[72,   100] loss: 0.309\n",
      "[73,   100] loss: 0.307\n",
      "[74,   100] loss: 0.304\n",
      "[75,   100] loss: 0.302\n",
      "[76,   100] loss: 0.300\n",
      "[77,   100] loss: 0.299\n",
      "[78,   100] loss: 0.297\n",
      "[79,   100] loss: 0.296\n",
      "[80,   100] loss: 0.295\n",
      "[81,   100] loss: 0.294\n",
      "[82,   100] loss: 0.293\n",
      "[83,   100] loss: 0.292\n",
      "[84,   100] loss: 0.291\n",
      "[85,   100] loss: 0.290\n",
      "[86,   100] loss: 0.289\n",
      "[87,   100] loss: 0.288\n",
      "[88,   100] loss: 0.290\n",
      "[89,   100] loss: 0.288\n",
      "[90,   100] loss: 0.287\n",
      "[91,   100] loss: 0.286\n",
      "[92,   100] loss: 0.285\n",
      "[93,   100] loss: 0.285\n",
      "[94,   100] loss: 0.284\n",
      "[95,   100] loss: 0.284\n",
      "[96,   100] loss: 0.284\n",
      "[97,   100] loss: 0.283\n",
      "[98,   100] loss: 0.282\n",
      "[99,   100] loss: 0.280\n",
      "[100,   100] loss: 0.279\n",
      "Finished Training\n",
      "[*] Visualizing an example's output...\n",
      "tensor([[0.0000, 0.0000, 0.0000, 2.7299, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.5918,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.6365,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.1806, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.2361, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 2.1075, 1.0785, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.6952, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.1214,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]])\n",
      "tensor([[9.5293e-01, 5.6826e-04, 1.0544e-02, 9.5104e-01, 1.0130e-04, 7.7563e-03,\n",
      "         1.3149e-01, 1.0904e-02, 9.8954e-03, 8.2676e-03, 5.9322e-04, 1.1258e-02,\n",
      "         8.5467e-03, 1.6544e-01, 9.3125e-03, 7.8266e-03, 1.0278e-02, 9.3206e-01,\n",
      "         1.2900e-01, 8.3176e-04, 1.0521e-03, 1.3556e-01, 7.7805e-03, 2.3301e-04,\n",
      "         7.3338e-03, 1.7138e-01, 1.7491e-01, 8.6297e-03, 1.0804e-02, 8.2325e-03,\n",
      "         1.0304e-02, 5.4076e-03, 1.0576e-02, 3.8450e-04, 8.6368e-05, 9.5622e-01,\n",
      "         1.2528e-01, 1.6287e-01, 4.4668e-04, 7.9918e-04, 4.4037e-04, 9.5008e-01,\n",
      "         1.3089e-01, 3.3963e-04, 9.4007e-03, 7.3841e-03, 1.5637e-01, 5.6049e-04,\n",
      "         5.3644e-04, 8.8399e-03, 8.3093e-03, 6.8557e-03, 9.1001e-01, 9.7295e-05,\n",
      "         9.9513e-03, 7.0475e-03, 7.0531e-03, 9.2680e-01, 9.4792e-01, 5.7949e-03,\n",
      "         5.3123e-04, 4.9400e-04, 7.4872e-04, 4.3130e-04, 7.9957e-04, 6.7780e-03,\n",
      "         8.7983e-03, 1.6610e-04, 1.5018e-01, 1.7309e-01, 1.0741e-02, 1.2196e-04,\n",
      "         1.0667e-02, 1.6718e-01, 9.3521e-01, 8.7868e-03, 9.7580e-03, 1.1954e-04,\n",
      "         6.4136e-04, 2.5229e-04, 9.5300e-01, 9.6859e-03, 7.2125e-03, 4.8079e-04,\n",
      "         1.1961e-01, 1.2166e-01, 1.6604e-01, 6.5172e-04, 7.2679e-03, 3.9403e-04,\n",
      "         1.2494e-03, 1.6903e-01, 8.2353e-03, 8.0916e-03, 1.2050e-01, 1.2499e-01,\n",
      "         1.2760e-01, 9.0748e-03, 7.6501e-03, 8.3533e-04]],\n",
      "       grad_fn=<SigmoidBackward>)\n",
      "0.16000457\n",
      "[*] Getting the embeddings and visualizing t-SNE...\n",
      "'dngr_pipeline'  11220.91 ms\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings_targets, _, _ = dngr_pipeline(pp_net, N_rand_targets, [60, 40, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spliting train and test sets...\n",
      "Building the train loader...\n",
      "Number of variables: 200\n",
      "Finding positive and negative examples...\n",
      "Number of train examples: 4130\n",
      "Number of positive examples in train set: 2037\n",
      "Number of negative/unlabelled examples in train set: 2093\n",
      "[1] loss: 2.236, auc: 0.503, acc: 0.500\n",
      "[2] loss: 0.845, auc: 0.501, acc: 0.494\n",
      "[3] loss: 0.642, auc: 0.514, acc: 0.518\n",
      "[4] loss: 0.582, auc: 0.521, acc: 0.523\n",
      "[5] loss: 0.552, auc: 0.523, acc: 0.528\n",
      "[6] loss: 0.544, auc: 0.519, acc: 0.509\n",
      "[7] loss: 0.541, auc: 0.520, acc: 0.515\n",
      "[8] loss: 0.530, auc: 0.504, acc: 0.504\n",
      "[9] loss: 0.531, auc: 0.513, acc: 0.512\n",
      "[10] loss: 0.522, auc: 0.497, acc: 0.489\n",
      "[11] loss: 0.518, auc: 0.513, acc: 0.491\n",
      "[12] loss: 0.511, auc: 0.521, acc: 0.513\n",
      "[13] loss: 0.511, auc: 0.518, acc: 0.523\n",
      "[14] loss: 0.510, auc: 0.525, acc: 0.515\n",
      "[15] loss: 0.509, auc: 0.519, acc: 0.520\n",
      "[16] loss: 0.506, auc: 0.523, acc: 0.528\n",
      "[17] loss: 0.503, auc: 0.512, acc: 0.508\n",
      "[18] loss: 0.510, auc: 0.516, acc: 0.517\n",
      "[19] loss: 0.503, auc: 0.512, acc: 0.506\n",
      "[20] loss: 0.504, auc: 0.517, acc: 0.519\n",
      "[21] loss: 0.506, auc: 0.506, acc: 0.503\n",
      "[22] loss: 0.502, auc: 0.539, acc: 0.525\n",
      "[23] loss: 0.500, auc: 0.538, acc: 0.531\n",
      "[24] loss: 0.497, auc: 0.536, acc: 0.522\n",
      "[25] loss: 0.495, auc: 0.526, acc: 0.526\n",
      "[26] loss: 0.497, auc: 0.504, acc: 0.507\n",
      "[27] loss: 0.498, auc: 0.531, acc: 0.521\n",
      "[28] loss: 0.503, auc: 0.542, acc: 0.525\n",
      "[29] loss: 0.499, auc: 0.525, acc: 0.514\n",
      "[30] loss: 0.495, auc: 0.535, acc: 0.530\n",
      "[31] loss: 0.499, auc: 0.538, acc: 0.524\n",
      "[32] loss: 0.491, auc: 0.530, acc: 0.527\n",
      "[33] loss: 0.496, auc: 0.531, acc: 0.507\n",
      "[34] loss: 0.492, auc: 0.532, acc: 0.525\n",
      "[35] loss: 0.498, auc: 0.533, acc: 0.534\n",
      "[36] loss: 0.498, auc: 0.522, acc: 0.515\n",
      "[37] loss: 0.492, auc: 0.525, acc: 0.515\n",
      "[38] loss: 0.489, auc: 0.528, acc: 0.526\n",
      "[39] loss: 0.495, auc: 0.543, acc: 0.526\n",
      "[40] loss: 0.491, auc: 0.531, acc: 0.524\n",
      "[41] loss: 0.495, auc: 0.537, acc: 0.523\n",
      "[42] loss: 0.492, auc: 0.544, acc: 0.525\n",
      "[43] loss: 0.494, auc: 0.542, acc: 0.532\n",
      "[44] loss: 0.488, auc: 0.530, acc: 0.523\n",
      "[45] loss: 0.492, auc: 0.535, acc: 0.529\n",
      "[46] loss: 0.491, auc: 0.540, acc: 0.532\n",
      "[47] loss: 0.491, auc: 0.537, acc: 0.527\n",
      "[48] loss: 0.489, auc: 0.538, acc: 0.533\n",
      "[49] loss: 0.491, auc: 0.536, acc: 0.524\n",
      "[50] loss: 0.492, auc: 0.542, acc: 0.524\n",
      "[51] loss: 0.490, auc: 0.539, acc: 0.533\n",
      "[52] loss: 0.489, auc: 0.541, acc: 0.531\n",
      "[53] loss: 0.489, auc: 0.538, acc: 0.522\n",
      "[54] loss: 0.490, auc: 0.538, acc: 0.520\n",
      "[55] loss: 0.493, auc: 0.538, acc: 0.528\n",
      "[56] loss: 0.486, auc: 0.540, acc: 0.520\n",
      "[57] loss: 0.492, auc: 0.544, acc: 0.537\n",
      "[58] loss: 0.490, auc: 0.546, acc: 0.535\n",
      "[59] loss: 0.490, auc: 0.542, acc: 0.523\n",
      "[60] loss: 0.490, auc: 0.540, acc: 0.534\n",
      "[61] loss: 0.489, auc: 0.542, acc: 0.533\n",
      "[62] loss: 0.491, auc: 0.540, acc: 0.526\n",
      "[63] loss: 0.492, auc: 0.540, acc: 0.526\n",
      "[64] loss: 0.487, auc: 0.544, acc: 0.507\n",
      "[65] loss: 0.489, auc: 0.546, acc: 0.525\n",
      "[66] loss: 0.489, auc: 0.532, acc: 0.518\n",
      "[67] loss: 0.488, auc: 0.543, acc: 0.510\n",
      "[68] loss: 0.487, auc: 0.541, acc: 0.533\n",
      "[69] loss: 0.489, auc: 0.538, acc: 0.527\n",
      "[70] loss: 0.489, auc: 0.539, acc: 0.531\n",
      "[71] loss: 0.489, auc: 0.545, acc: 0.528\n",
      "[72] loss: 0.490, auc: 0.548, acc: 0.535\n",
      "[73] loss: 0.491, auc: 0.534, acc: 0.520\n",
      "[74] loss: 0.491, auc: 0.539, acc: 0.528\n",
      "[75] loss: 0.488, auc: 0.542, acc: 0.526\n",
      "[76] loss: 0.489, auc: 0.541, acc: 0.534\n",
      "[77] loss: 0.485, auc: 0.543, acc: 0.521\n",
      "[78] loss: 0.490, auc: 0.545, acc: 0.524\n",
      "[79] loss: 0.491, auc: 0.543, acc: 0.524\n",
      "[80] loss: 0.488, auc: 0.532, acc: 0.514\n",
      "[81] loss: 0.491, auc: 0.539, acc: 0.533\n",
      "[82] loss: 0.485, auc: 0.544, acc: 0.533\n",
      "[83] loss: 0.490, auc: 0.537, acc: 0.534\n",
      "[84] loss: 0.489, auc: 0.538, acc: 0.530\n",
      "[85] loss: 0.491, auc: 0.540, acc: 0.534\n",
      "[86] loss: 0.490, auc: 0.543, acc: 0.521\n",
      "[87] loss: 0.489, auc: 0.538, acc: 0.529\n",
      "[88] loss: 0.486, auc: 0.535, acc: 0.522\n",
      "[89] loss: 0.486, auc: 0.536, acc: 0.526\n",
      "[90] loss: 0.487, auc: 0.546, acc: 0.525\n",
      "[91] loss: 0.489, auc: 0.548, acc: 0.540\n",
      "[92] loss: 0.485, auc: 0.533, acc: 0.528\n",
      "[93] loss: 0.484, auc: 0.546, acc: 0.536\n",
      "[94] loss: 0.492, auc: 0.543, acc: 0.534\n",
      "[95] loss: 0.484, auc: 0.548, acc: 0.516\n",
      "[96] loss: 0.490, auc: 0.546, acc: 0.524\n",
      "[97] loss: 0.483, auc: 0.531, acc: 0.521\n",
      "[98] loss: 0.485, auc: 0.541, acc: 0.522\n",
      "[99] loss: 0.486, auc: 0.546, acc: 0.526\n",
      "[100] loss: 0.484, auc: 0.537, acc: 0.531\n",
      "[101] loss: 0.486, auc: 0.535, acc: 0.526\n",
      "[102] loss: 0.490, auc: 0.537, acc: 0.531\n",
      "[103] loss: 0.485, auc: 0.538, acc: 0.523\n",
      "[104] loss: 0.486, auc: 0.533, acc: 0.521\n",
      "[105] loss: 0.488, auc: 0.531, acc: 0.520\n",
      "[106] loss: 0.489, auc: 0.548, acc: 0.539\n",
      "[107] loss: 0.487, auc: 0.541, acc: 0.531\n",
      "[108] loss: 0.490, auc: 0.538, acc: 0.533\n",
      "[109] loss: 0.487, auc: 0.537, acc: 0.512\n",
      "[110] loss: 0.486, auc: 0.540, acc: 0.534\n",
      "[111] loss: 0.481, auc: 0.535, acc: 0.529\n",
      "[112] loss: 0.488, auc: 0.548, acc: 0.531\n",
      "[113] loss: 0.483, auc: 0.547, acc: 0.523\n",
      "[114] loss: 0.490, auc: 0.542, acc: 0.534\n",
      "[115] loss: 0.488, auc: 0.538, acc: 0.535\n",
      "[116] loss: 0.483, auc: 0.534, acc: 0.515\n",
      "[117] loss: 0.489, auc: 0.541, acc: 0.535\n",
      "[118] loss: 0.484, auc: 0.548, acc: 0.525\n",
      "[119] loss: 0.486, auc: 0.544, acc: 0.525\n",
      "[120] loss: 0.488, auc: 0.539, acc: 0.525\n",
      "[121] loss: 0.485, auc: 0.541, acc: 0.531\n",
      "[122] loss: 0.486, auc: 0.537, acc: 0.523\n",
      "[123] loss: 0.486, auc: 0.548, acc: 0.532\n",
      "[124] loss: 0.486, auc: 0.534, acc: 0.524\n",
      "[125] loss: 0.487, auc: 0.546, acc: 0.525\n",
      "[126] loss: 0.486, auc: 0.541, acc: 0.528\n",
      "[127] loss: 0.488, auc: 0.543, acc: 0.529\n",
      "[128] loss: 0.487, auc: 0.540, acc: 0.541\n",
      "[129] loss: 0.489, auc: 0.544, acc: 0.525\n",
      "[130] loss: 0.483, auc: 0.536, acc: 0.510\n",
      "[131] loss: 0.488, auc: 0.542, acc: 0.532\n",
      "[132] loss: 0.487, auc: 0.541, acc: 0.537\n",
      "[133] loss: 0.485, auc: 0.547, acc: 0.524\n",
      "[134] loss: 0.486, auc: 0.533, acc: 0.525\n",
      "[135] loss: 0.486, auc: 0.529, acc: 0.517\n",
      "[136] loss: 0.489, auc: 0.536, acc: 0.530\n",
      "[137] loss: 0.487, auc: 0.538, acc: 0.537\n",
      "[138] loss: 0.487, auc: 0.545, acc: 0.515\n",
      "[139] loss: 0.488, auc: 0.543, acc: 0.526\n",
      "[140] loss: 0.483, auc: 0.546, acc: 0.516\n",
      "[141] loss: 0.487, auc: 0.536, acc: 0.537\n",
      "[142] loss: 0.487, auc: 0.543, acc: 0.524\n",
      "[143] loss: 0.489, auc: 0.542, acc: 0.529\n",
      "[144] loss: 0.485, auc: 0.534, acc: 0.537\n",
      "[145] loss: 0.485, auc: 0.541, acc: 0.515\n",
      "[146] loss: 0.485, auc: 0.535, acc: 0.524\n",
      "[147] loss: 0.486, auc: 0.546, acc: 0.524\n",
      "[148] loss: 0.486, auc: 0.541, acc: 0.532\n",
      "[149] loss: 0.486, auc: 0.551, acc: 0.523\n",
      "[150] loss: 0.485, auc: 0.542, acc: 0.530\n",
      "[151] loss: 0.486, auc: 0.552, acc: 0.524\n",
      "[152] loss: 0.487, auc: 0.531, acc: 0.531\n",
      "[153] loss: 0.485, auc: 0.541, acc: 0.528\n",
      "[154] loss: 0.487, auc: 0.550, acc: 0.524\n",
      "[155] loss: 0.489, auc: 0.537, acc: 0.525\n",
      "[156] loss: 0.488, auc: 0.544, acc: 0.533\n",
      "[157] loss: 0.487, auc: 0.538, acc: 0.531\n",
      "[158] loss: 0.486, auc: 0.538, acc: 0.523\n",
      "[159] loss: 0.486, auc: 0.536, acc: 0.518\n",
      "[160] loss: 0.487, auc: 0.532, acc: 0.522\n",
      "[161] loss: 0.486, auc: 0.542, acc: 0.517\n",
      "[162] loss: 0.489, auc: 0.534, acc: 0.533\n",
      "[163] loss: 0.485, auc: 0.540, acc: 0.529\n",
      "[164] loss: 0.485, auc: 0.539, acc: 0.523\n",
      "[165] loss: 0.483, auc: 0.536, acc: 0.522\n",
      "[166] loss: 0.484, auc: 0.541, acc: 0.538\n",
      "[167] loss: 0.485, auc: 0.549, acc: 0.518\n",
      "[168] loss: 0.487, auc: 0.539, acc: 0.524\n",
      "[169] loss: 0.487, auc: 0.548, acc: 0.522\n",
      "[170] loss: 0.486, auc: 0.542, acc: 0.533\n",
      "[171] loss: 0.483, auc: 0.538, acc: 0.520\n",
      "[172] loss: 0.485, auc: 0.552, acc: 0.518\n",
      "[173] loss: 0.490, auc: 0.538, acc: 0.531\n",
      "[174] loss: 0.487, auc: 0.537, acc: 0.533\n",
      "[175] loss: 0.487, auc: 0.535, acc: 0.526\n",
      "[176] loss: 0.483, auc: 0.532, acc: 0.523\n",
      "[177] loss: 0.487, auc: 0.538, acc: 0.530\n",
      "[178] loss: 0.487, auc: 0.543, acc: 0.533\n",
      "[179] loss: 0.486, auc: 0.541, acc: 0.525\n",
      "[180] loss: 0.485, auc: 0.549, acc: 0.524\n",
      "[181] loss: 0.485, auc: 0.534, acc: 0.523\n",
      "[182] loss: 0.486, auc: 0.543, acc: 0.525\n",
      "[183] loss: 0.486, auc: 0.543, acc: 0.525\n",
      "[184] loss: 0.484, auc: 0.535, acc: 0.529\n",
      "[185] loss: 0.486, auc: 0.547, acc: 0.525\n",
      "[186] loss: 0.484, auc: 0.537, acc: 0.528\n",
      "[187] loss: 0.486, auc: 0.536, acc: 0.531\n",
      "[188] loss: 0.486, auc: 0.539, acc: 0.514\n",
      "[189] loss: 0.486, auc: 0.545, acc: 0.527\n",
      "[190] loss: 0.486, auc: 0.541, acc: 0.533\n",
      "[191] loss: 0.484, auc: 0.543, acc: 0.534\n",
      "[192] loss: 0.482, auc: 0.536, acc: 0.523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[193] loss: 0.485, auc: 0.543, acc: 0.534\n",
      "[194] loss: 0.487, auc: 0.536, acc: 0.528\n",
      "[195] loss: 0.487, auc: 0.540, acc: 0.526\n",
      "[196] loss: 0.487, auc: 0.535, acc: 0.535\n",
      "[197] loss: 0.485, auc: 0.546, acc: 0.537\n",
      "[198] loss: 0.484, auc: 0.549, acc: 0.523\n",
      "[199] loss: 0.489, auc: 0.550, acc: 0.528\n",
      "[200] loss: 0.480, auc: 0.547, acc: 0.524\n",
      "[201] loss: 0.485, auc: 0.541, acc: 0.529\n",
      "[202] loss: 0.479, auc: 0.531, acc: 0.527\n",
      "[203] loss: 0.490, auc: 0.542, acc: 0.534\n",
      "[204] loss: 0.481, auc: 0.541, acc: 0.523\n",
      "[205] loss: 0.483, auc: 0.534, acc: 0.530\n",
      "[206] loss: 0.485, auc: 0.539, acc: 0.524\n",
      "[207] loss: 0.485, auc: 0.539, acc: 0.531\n",
      "[208] loss: 0.484, auc: 0.530, acc: 0.526\n",
      "[209] loss: 0.485, auc: 0.550, acc: 0.526\n",
      "[210] loss: 0.485, auc: 0.544, acc: 0.529\n",
      "[211] loss: 0.485, auc: 0.539, acc: 0.524\n",
      "[212] loss: 0.486, auc: 0.548, acc: 0.524\n",
      "[213] loss: 0.486, auc: 0.531, acc: 0.523\n",
      "[214] loss: 0.490, auc: 0.551, acc: 0.540\n",
      "[215] loss: 0.486, auc: 0.549, acc: 0.523\n",
      "[216] loss: 0.483, auc: 0.535, acc: 0.523\n",
      "[217] loss: 0.484, auc: 0.541, acc: 0.532\n",
      "[218] loss: 0.486, auc: 0.533, acc: 0.514\n",
      "[219] loss: 0.484, auc: 0.534, acc: 0.524\n",
      "[220] loss: 0.487, auc: 0.546, acc: 0.523\n",
      "[221] loss: 0.488, auc: 0.549, acc: 0.524\n",
      "[222] loss: 0.487, auc: 0.547, acc: 0.519\n",
      "[223] loss: 0.486, auc: 0.545, acc: 0.537\n",
      "[224] loss: 0.486, auc: 0.540, acc: 0.537\n",
      "[225] loss: 0.486, auc: 0.537, acc: 0.524\n",
      "[226] loss: 0.482, auc: 0.533, acc: 0.517\n",
      "[227] loss: 0.483, auc: 0.544, acc: 0.538\n",
      "[228] loss: 0.483, auc: 0.551, acc: 0.525\n",
      "[229] loss: 0.485, auc: 0.537, acc: 0.534\n",
      "[230] loss: 0.486, auc: 0.549, acc: 0.538\n",
      "[231] loss: 0.484, auc: 0.529, acc: 0.520\n",
      "[232] loss: 0.487, auc: 0.540, acc: 0.531\n",
      "[233] loss: 0.484, auc: 0.543, acc: 0.531\n",
      "[234] loss: 0.485, auc: 0.537, acc: 0.523\n",
      "[235] loss: 0.487, auc: 0.543, acc: 0.531\n",
      "[236] loss: 0.484, auc: 0.536, acc: 0.524\n",
      "[237] loss: 0.483, auc: 0.541, acc: 0.534\n",
      "[238] loss: 0.485, auc: 0.531, acc: 0.530\n",
      "[239] loss: 0.488, auc: 0.540, acc: 0.527\n",
      "[240] loss: 0.487, auc: 0.544, acc: 0.533\n",
      "[241] loss: 0.486, auc: 0.546, acc: 0.531\n",
      "[242] loss: 0.484, auc: 0.555, acc: 0.525\n",
      "[243] loss: 0.487, auc: 0.531, acc: 0.522\n",
      "[244] loss: 0.483, auc: 0.542, acc: 0.524\n",
      "[245] loss: 0.486, auc: 0.536, acc: 0.527\n",
      "[246] loss: 0.486, auc: 0.552, acc: 0.527\n",
      "[247] loss: 0.485, auc: 0.535, acc: 0.533\n",
      "[248] loss: 0.485, auc: 0.542, acc: 0.532\n",
      "[249] loss: 0.487, auc: 0.548, acc: 0.531\n",
      "[250] loss: 0.486, auc: 0.552, acc: 0.532\n",
      "[251] loss: 0.484, auc: 0.549, acc: 0.526\n",
      "[252] loss: 0.486, auc: 0.544, acc: 0.527\n",
      "[253] loss: 0.483, auc: 0.533, acc: 0.523\n",
      "[254] loss: 0.487, auc: 0.549, acc: 0.523\n",
      "[255] loss: 0.485, auc: 0.541, acc: 0.534\n",
      "[256] loss: 0.484, auc: 0.546, acc: 0.531\n",
      "[257] loss: 0.488, auc: 0.547, acc: 0.535\n",
      "[258] loss: 0.485, auc: 0.552, acc: 0.522\n",
      "[259] loss: 0.489, auc: 0.543, acc: 0.536\n",
      "[260] loss: 0.486, auc: 0.537, acc: 0.525\n",
      "[261] loss: 0.488, auc: 0.548, acc: 0.527\n",
      "[262] loss: 0.484, auc: 0.542, acc: 0.520\n",
      "[263] loss: 0.486, auc: 0.548, acc: 0.533\n",
      "[264] loss: 0.485, auc: 0.547, acc: 0.524\n",
      "[265] loss: 0.485, auc: 0.538, acc: 0.534\n",
      "[266] loss: 0.483, auc: 0.537, acc: 0.525\n",
      "[267] loss: 0.486, auc: 0.534, acc: 0.519\n",
      "[268] loss: 0.487, auc: 0.543, acc: 0.533\n",
      "[269] loss: 0.481, auc: 0.546, acc: 0.523\n",
      "[270] loss: 0.487, auc: 0.541, acc: 0.532\n",
      "[271] loss: 0.487, auc: 0.539, acc: 0.532\n",
      "[272] loss: 0.483, auc: 0.547, acc: 0.528\n",
      "[273] loss: 0.487, auc: 0.538, acc: 0.528\n",
      "[274] loss: 0.485, auc: 0.554, acc: 0.526\n",
      "[275] loss: 0.484, auc: 0.540, acc: 0.536\n",
      "[276] loss: 0.485, auc: 0.546, acc: 0.533\n",
      "[277] loss: 0.483, auc: 0.543, acc: 0.525\n",
      "[278] loss: 0.486, auc: 0.539, acc: 0.538\n",
      "[279] loss: 0.483, auc: 0.548, acc: 0.532\n",
      "[280] loss: 0.485, auc: 0.540, acc: 0.531\n",
      "[281] loss: 0.484, auc: 0.543, acc: 0.534\n",
      "[282] loss: 0.485, auc: 0.548, acc: 0.540\n",
      "[283] loss: 0.485, auc: 0.542, acc: 0.539\n",
      "[284] loss: 0.485, auc: 0.540, acc: 0.530\n",
      "[285] loss: 0.483, auc: 0.531, acc: 0.525\n",
      "[286] loss: 0.486, auc: 0.545, acc: 0.540\n",
      "[287] loss: 0.486, auc: 0.544, acc: 0.531\n",
      "[288] loss: 0.485, auc: 0.542, acc: 0.540\n",
      "[289] loss: 0.485, auc: 0.539, acc: 0.531\n",
      "[290] loss: 0.483, auc: 0.549, acc: 0.534\n",
      "[291] loss: 0.484, auc: 0.528, acc: 0.523\n",
      "[292] loss: 0.488, auc: 0.547, acc: 0.523\n",
      "[293] loss: 0.481, auc: 0.541, acc: 0.517\n",
      "[294] loss: 0.486, auc: 0.543, acc: 0.529\n",
      "[295] loss: 0.485, auc: 0.536, acc: 0.532\n",
      "[296] loss: 0.486, auc: 0.545, acc: 0.525\n",
      "[297] loss: 0.485, auc: 0.551, acc: 0.524\n",
      "[298] loss: 0.484, auc: 0.531, acc: 0.534\n",
      "[299] loss: 0.486, auc: 0.549, acc: 0.541\n",
      "[300] loss: 0.481, auc: 0.545, acc: 0.525\n",
      "[301] loss: 0.482, auc: 0.548, acc: 0.539\n",
      "[302] loss: 0.485, auc: 0.549, acc: 0.525\n",
      "[303] loss: 0.485, auc: 0.552, acc: 0.526\n",
      "[304] loss: 0.484, auc: 0.546, acc: 0.532\n",
      "[305] loss: 0.484, auc: 0.552, acc: 0.526\n",
      "[306] loss: 0.483, auc: 0.553, acc: 0.525\n",
      "[307] loss: 0.486, auc: 0.546, acc: 0.524\n",
      "[308] loss: 0.484, auc: 0.538, acc: 0.533\n",
      "[309] loss: 0.487, auc: 0.544, acc: 0.533\n",
      "[310] loss: 0.485, auc: 0.536, acc: 0.530\n",
      "[311] loss: 0.487, auc: 0.548, acc: 0.541\n",
      "[312] loss: 0.484, auc: 0.536, acc: 0.530\n",
      "[313] loss: 0.486, auc: 0.548, acc: 0.528\n",
      "[314] loss: 0.484, auc: 0.539, acc: 0.526\n",
      "[315] loss: 0.483, auc: 0.537, acc: 0.521\n",
      "[316] loss: 0.486, auc: 0.543, acc: 0.532\n",
      "[317] loss: 0.483, auc: 0.549, acc: 0.525\n",
      "[318] loss: 0.483, auc: 0.548, acc: 0.524\n",
      "[319] loss: 0.484, auc: 0.535, acc: 0.535\n",
      "[320] loss: 0.486, auc: 0.541, acc: 0.533\n",
      "[321] loss: 0.484, auc: 0.553, acc: 0.528\n",
      "[322] loss: 0.484, auc: 0.555, acc: 0.523\n",
      "[323] loss: 0.486, auc: 0.534, acc: 0.530\n",
      "[324] loss: 0.484, auc: 0.541, acc: 0.528\n",
      "[325] loss: 0.484, auc: 0.547, acc: 0.534\n",
      "[326] loss: 0.483, auc: 0.535, acc: 0.520\n",
      "[327] loss: 0.486, auc: 0.550, acc: 0.522\n",
      "[328] loss: 0.483, auc: 0.538, acc: 0.530\n",
      "[329] loss: 0.484, auc: 0.543, acc: 0.529\n",
      "[330] loss: 0.485, auc: 0.554, acc: 0.524\n",
      "[331] loss: 0.484, auc: 0.547, acc: 0.531\n",
      "[332] loss: 0.484, auc: 0.539, acc: 0.535\n",
      "[333] loss: 0.485, auc: 0.545, acc: 0.533\n",
      "[334] loss: 0.486, auc: 0.550, acc: 0.538\n",
      "[335] loss: 0.486, auc: 0.542, acc: 0.530\n",
      "[336] loss: 0.486, auc: 0.544, acc: 0.532\n",
      "[337] loss: 0.484, auc: 0.539, acc: 0.528\n",
      "[338] loss: 0.486, auc: 0.545, acc: 0.525\n",
      "[339] loss: 0.484, auc: 0.548, acc: 0.536\n",
      "[340] loss: 0.481, auc: 0.532, acc: 0.532\n",
      "[341] loss: 0.487, auc: 0.550, acc: 0.523\n",
      "[342] loss: 0.485, auc: 0.551, acc: 0.538\n",
      "[343] loss: 0.487, auc: 0.538, acc: 0.529\n",
      "[344] loss: 0.485, auc: 0.540, acc: 0.531\n",
      "[345] loss: 0.484, auc: 0.544, acc: 0.528\n",
      "[346] loss: 0.485, auc: 0.542, acc: 0.529\n",
      "[347] loss: 0.485, auc: 0.545, acc: 0.533\n",
      "[348] loss: 0.482, auc: 0.548, acc: 0.508\n",
      "[349] loss: 0.485, auc: 0.538, acc: 0.536\n",
      "[350] loss: 0.485, auc: 0.542, acc: 0.526\n",
      "[351] loss: 0.484, auc: 0.539, acc: 0.524\n",
      "[352] loss: 0.484, auc: 0.535, acc: 0.528\n",
      "[353] loss: 0.485, auc: 0.543, acc: 0.528\n",
      "[354] loss: 0.483, auc: 0.542, acc: 0.529\n",
      "[355] loss: 0.486, auc: 0.544, acc: 0.534\n",
      "[356] loss: 0.485, auc: 0.553, acc: 0.525\n",
      "[357] loss: 0.484, auc: 0.539, acc: 0.534\n",
      "[358] loss: 0.484, auc: 0.543, acc: 0.527\n",
      "[359] loss: 0.485, auc: 0.542, acc: 0.535\n",
      "[360] loss: 0.483, auc: 0.538, acc: 0.521\n",
      "[361] loss: 0.488, auc: 0.541, acc: 0.529\n",
      "[362] loss: 0.484, auc: 0.552, acc: 0.524\n",
      "[363] loss: 0.487, auc: 0.544, acc: 0.536\n",
      "[364] loss: 0.483, auc: 0.553, acc: 0.523\n",
      "[365] loss: 0.483, auc: 0.546, acc: 0.535\n",
      "[366] loss: 0.485, auc: 0.550, acc: 0.525\n",
      "[367] loss: 0.485, auc: 0.538, acc: 0.531\n",
      "[368] loss: 0.486, auc: 0.550, acc: 0.536\n",
      "[369] loss: 0.485, auc: 0.533, acc: 0.528\n",
      "[370] loss: 0.485, auc: 0.555, acc: 0.528\n",
      "[371] loss: 0.486, auc: 0.549, acc: 0.538\n",
      "[372] loss: 0.483, auc: 0.545, acc: 0.525\n",
      "[373] loss: 0.481, auc: 0.544, acc: 0.528\n",
      "[374] loss: 0.486, auc: 0.543, acc: 0.539\n",
      "[375] loss: 0.484, auc: 0.534, acc: 0.522\n",
      "[376] loss: 0.489, auc: 0.544, acc: 0.525\n",
      "[377] loss: 0.484, auc: 0.522, acc: 0.523\n",
      "[378] loss: 0.489, auc: 0.547, acc: 0.517\n",
      "[379] loss: 0.485, auc: 0.534, acc: 0.524\n",
      "[380] loss: 0.486, auc: 0.537, acc: 0.533\n",
      "[381] loss: 0.484, auc: 0.541, acc: 0.539\n",
      "[382] loss: 0.485, auc: 0.547, acc: 0.538\n",
      "[383] loss: 0.484, auc: 0.536, acc: 0.528\n",
      "[384] loss: 0.486, auc: 0.541, acc: 0.535\n",
      "[385] loss: 0.483, auc: 0.532, acc: 0.524\n",
      "[386] loss: 0.487, auc: 0.546, acc: 0.539\n",
      "[387] loss: 0.484, auc: 0.546, acc: 0.536\n",
      "[388] loss: 0.486, auc: 0.541, acc: 0.530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[389] loss: 0.486, auc: 0.548, acc: 0.528\n",
      "[390] loss: 0.482, auc: 0.540, acc: 0.525\n",
      "[391] loss: 0.485, auc: 0.548, acc: 0.527\n",
      "[392] loss: 0.484, auc: 0.545, acc: 0.536\n",
      "[393] loss: 0.484, auc: 0.548, acc: 0.523\n",
      "[394] loss: 0.483, auc: 0.550, acc: 0.510\n",
      "[395] loss: 0.484, auc: 0.547, acc: 0.538\n",
      "[396] loss: 0.486, auc: 0.548, acc: 0.538\n",
      "[397] loss: 0.482, auc: 0.549, acc: 0.538\n",
      "[398] loss: 0.483, auc: 0.550, acc: 0.544\n",
      "[399] loss: 0.484, auc: 0.538, acc: 0.527\n",
      "[400] loss: 0.483, auc: 0.556, acc: 0.508\n",
      "[401] loss: 0.487, auc: 0.542, acc: 0.524\n",
      "[402] loss: 0.485, auc: 0.543, acc: 0.534\n",
      "[403] loss: 0.484, auc: 0.546, acc: 0.525\n",
      "[404] loss: 0.485, auc: 0.533, acc: 0.533\n",
      "[405] loss: 0.485, auc: 0.542, acc: 0.539\n",
      "[406] loss: 0.486, auc: 0.539, acc: 0.538\n",
      "[407] loss: 0.483, auc: 0.536, acc: 0.524\n",
      "[408] loss: 0.486, auc: 0.543, acc: 0.538\n",
      "[409] loss: 0.483, auc: 0.545, acc: 0.526\n",
      "[410] loss: 0.486, auc: 0.550, acc: 0.534\n",
      "[411] loss: 0.483, auc: 0.540, acc: 0.530\n",
      "[412] loss: 0.483, auc: 0.544, acc: 0.537\n",
      "[413] loss: 0.484, auc: 0.539, acc: 0.533\n",
      "[414] loss: 0.482, auc: 0.544, acc: 0.535\n",
      "[415] loss: 0.483, auc: 0.546, acc: 0.535\n",
      "[416] loss: 0.485, auc: 0.550, acc: 0.527\n",
      "[417] loss: 0.485, auc: 0.541, acc: 0.520\n",
      "[418] loss: 0.482, auc: 0.537, acc: 0.526\n",
      "[419] loss: 0.487, auc: 0.548, acc: 0.526\n",
      "[420] loss: 0.485, auc: 0.539, acc: 0.532\n",
      "[421] loss: 0.484, auc: 0.540, acc: 0.538\n",
      "[422] loss: 0.483, auc: 0.539, acc: 0.533\n",
      "[423] loss: 0.487, auc: 0.548, acc: 0.539\n",
      "[424] loss: 0.484, auc: 0.552, acc: 0.524\n",
      "[425] loss: 0.485, auc: 0.551, acc: 0.530\n",
      "[426] loss: 0.483, auc: 0.539, acc: 0.528\n",
      "[427] loss: 0.484, auc: 0.549, acc: 0.524\n",
      "[428] loss: 0.484, auc: 0.542, acc: 0.534\n",
      "[429] loss: 0.482, auc: 0.552, acc: 0.528\n",
      "[430] loss: 0.483, auc: 0.538, acc: 0.535\n",
      "[431] loss: 0.484, auc: 0.540, acc: 0.521\n",
      "[432] loss: 0.481, auc: 0.538, acc: 0.530\n",
      "[433] loss: 0.486, auc: 0.542, acc: 0.538\n",
      "[434] loss: 0.486, auc: 0.554, acc: 0.525\n",
      "[435] loss: 0.483, auc: 0.538, acc: 0.522\n",
      "[436] loss: 0.485, auc: 0.549, acc: 0.544\n",
      "[437] loss: 0.483, auc: 0.546, acc: 0.542\n",
      "[438] loss: 0.485, auc: 0.542, acc: 0.537\n",
      "[439] loss: 0.483, auc: 0.537, acc: 0.533\n",
      "[440] loss: 0.482, auc: 0.546, acc: 0.536\n",
      "[441] loss: 0.485, auc: 0.545, acc: 0.538\n",
      "[442] loss: 0.483, auc: 0.537, acc: 0.528\n",
      "[443] loss: 0.483, auc: 0.548, acc: 0.538\n",
      "[444] loss: 0.484, auc: 0.553, acc: 0.537\n",
      "[445] loss: 0.484, auc: 0.536, acc: 0.533\n",
      "[446] loss: 0.485, auc: 0.540, acc: 0.534\n",
      "[447] loss: 0.484, auc: 0.536, acc: 0.517\n",
      "[448] loss: 0.488, auc: 0.552, acc: 0.529\n",
      "[449] loss: 0.482, auc: 0.535, acc: 0.534\n",
      "[450] loss: 0.484, auc: 0.539, acc: 0.535\n",
      "[451] loss: 0.484, auc: 0.546, acc: 0.525\n",
      "[452] loss: 0.483, auc: 0.548, acc: 0.541\n",
      "[453] loss: 0.484, auc: 0.546, acc: 0.532\n",
      "[454] loss: 0.483, auc: 0.545, acc: 0.535\n",
      "[455] loss: 0.485, auc: 0.554, acc: 0.516\n",
      "[456] loss: 0.488, auc: 0.541, acc: 0.535\n",
      "[457] loss: 0.486, auc: 0.536, acc: 0.530\n",
      "[458] loss: 0.483, auc: 0.545, acc: 0.536\n",
      "[459] loss: 0.483, auc: 0.533, acc: 0.535\n",
      "[460] loss: 0.484, auc: 0.546, acc: 0.537\n",
      "[461] loss: 0.484, auc: 0.544, acc: 0.536\n",
      "[462] loss: 0.484, auc: 0.541, acc: 0.525\n",
      "[463] loss: 0.484, auc: 0.541, acc: 0.522\n",
      "[464] loss: 0.485, auc: 0.544, acc: 0.522\n",
      "[465] loss: 0.484, auc: 0.540, acc: 0.524\n",
      "[466] loss: 0.482, auc: 0.540, acc: 0.527\n",
      "[467] loss: 0.485, auc: 0.541, acc: 0.525\n",
      "[468] loss: 0.485, auc: 0.549, acc: 0.534\n",
      "[469] loss: 0.483, auc: 0.540, acc: 0.531\n",
      "[470] loss: 0.484, auc: 0.551, acc: 0.524\n",
      "[471] loss: 0.486, auc: 0.539, acc: 0.534\n",
      "[472] loss: 0.485, auc: 0.544, acc: 0.529\n",
      "[473] loss: 0.484, auc: 0.541, acc: 0.529\n",
      "[474] loss: 0.483, auc: 0.542, acc: 0.536\n",
      "[475] loss: 0.485, auc: 0.541, acc: 0.535\n",
      "[476] loss: 0.481, auc: 0.553, acc: 0.527\n",
      "[477] loss: 0.484, auc: 0.540, acc: 0.535\n",
      "[478] loss: 0.484, auc: 0.544, acc: 0.533\n",
      "[479] loss: 0.483, auc: 0.540, acc: 0.535\n",
      "[480] loss: 0.484, auc: 0.542, acc: 0.535\n",
      "[481] loss: 0.481, auc: 0.540, acc: 0.524\n",
      "[482] loss: 0.486, auc: 0.548, acc: 0.525\n",
      "[483] loss: 0.484, auc: 0.554, acc: 0.526\n",
      "[484] loss: 0.484, auc: 0.543, acc: 0.534\n",
      "[485] loss: 0.484, auc: 0.548, acc: 0.532\n",
      "[486] loss: 0.484, auc: 0.544, acc: 0.532\n",
      "[487] loss: 0.484, auc: 0.543, acc: 0.533\n",
      "[488] loss: 0.485, auc: 0.550, acc: 0.539\n",
      "[489] loss: 0.481, auc: 0.546, acc: 0.535\n",
      "[490] loss: 0.485, auc: 0.548, acc: 0.536\n",
      "[491] loss: 0.483, auc: 0.545, acc: 0.536\n",
      "[492] loss: 0.483, auc: 0.552, acc: 0.527\n",
      "[493] loss: 0.481, auc: 0.538, acc: 0.523\n",
      "[494] loss: 0.485, auc: 0.538, acc: 0.525\n",
      "[495] loss: 0.484, auc: 0.541, acc: 0.525\n",
      "[496] loss: 0.485, auc: 0.547, acc: 0.529\n",
      "[497] loss: 0.482, auc: 0.538, acc: 0.529\n",
      "[498] loss: 0.483, auc: 0.548, acc: 0.532\n",
      "[499] loss: 0.484, auc: 0.537, acc: 0.534\n",
      "[500] loss: 0.484, auc: 0.545, acc: 0.532\n",
      "[501] loss: 0.485, auc: 0.552, acc: 0.529\n",
      "[502] loss: 0.484, auc: 0.544, acc: 0.529\n",
      "[503] loss: 0.483, auc: 0.549, acc: 0.529\n",
      "[504] loss: 0.483, auc: 0.554, acc: 0.526\n",
      "[505] loss: 0.484, auc: 0.547, acc: 0.537\n",
      "[506] loss: 0.483, auc: 0.536, acc: 0.526\n",
      "[507] loss: 0.482, auc: 0.537, acc: 0.524\n",
      "[508] loss: 0.483, auc: 0.542, acc: 0.536\n",
      "[509] loss: 0.483, auc: 0.548, acc: 0.526\n",
      "[510] loss: 0.482, auc: 0.551, acc: 0.540\n",
      "[511] loss: 0.483, auc: 0.543, acc: 0.531\n",
      "[512] loss: 0.484, auc: 0.546, acc: 0.535\n",
      "[513] loss: 0.485, auc: 0.550, acc: 0.534\n",
      "[514] loss: 0.483, auc: 0.543, acc: 0.536\n",
      "[515] loss: 0.484, auc: 0.551, acc: 0.530\n",
      "[516] loss: 0.485, auc: 0.549, acc: 0.531\n",
      "[517] loss: 0.481, auc: 0.541, acc: 0.532\n",
      "[518] loss: 0.484, auc: 0.550, acc: 0.535\n",
      "[519] loss: 0.483, auc: 0.551, acc: 0.527\n",
      "[520] loss: 0.483, auc: 0.542, acc: 0.532\n",
      "[521] loss: 0.485, auc: 0.552, acc: 0.533\n",
      "[522] loss: 0.484, auc: 0.547, acc: 0.538\n",
      "[523] loss: 0.481, auc: 0.543, acc: 0.533\n",
      "[524] loss: 0.485, auc: 0.540, acc: 0.532\n",
      "[525] loss: 0.485, auc: 0.546, acc: 0.538\n",
      "[526] loss: 0.485, auc: 0.544, acc: 0.535\n",
      "[527] loss: 0.485, auc: 0.546, acc: 0.537\n",
      "[528] loss: 0.484, auc: 0.545, acc: 0.536\n",
      "[529] loss: 0.481, auc: 0.545, acc: 0.523\n",
      "[530] loss: 0.485, auc: 0.543, acc: 0.535\n",
      "[531] loss: 0.483, auc: 0.548, acc: 0.536\n",
      "[532] loss: 0.485, auc: 0.541, acc: 0.533\n",
      "[533] loss: 0.484, auc: 0.543, acc: 0.538\n",
      "[534] loss: 0.481, auc: 0.542, acc: 0.532\n",
      "[535] loss: 0.484, auc: 0.551, acc: 0.517\n",
      "[536] loss: 0.486, auc: 0.553, acc: 0.537\n",
      "[537] loss: 0.484, auc: 0.546, acc: 0.540\n",
      "[538] loss: 0.483, auc: 0.549, acc: 0.533\n",
      "[539] loss: 0.485, auc: 0.544, acc: 0.539\n",
      "[540] loss: 0.482, auc: 0.552, acc: 0.529\n",
      "[541] loss: 0.485, auc: 0.548, acc: 0.533\n",
      "[542] loss: 0.484, auc: 0.540, acc: 0.527\n",
      "[543] loss: 0.485, auc: 0.557, acc: 0.526\n",
      "[544] loss: 0.484, auc: 0.541, acc: 0.526\n",
      "[545] loss: 0.485, auc: 0.548, acc: 0.525\n",
      "[546] loss: 0.481, auc: 0.544, acc: 0.532\n",
      "[547] loss: 0.483, auc: 0.544, acc: 0.530\n",
      "[548] loss: 0.482, auc: 0.551, acc: 0.527\n",
      "[549] loss: 0.485, auc: 0.548, acc: 0.525\n",
      "[550] loss: 0.481, auc: 0.555, acc: 0.510\n",
      "[551] loss: 0.484, auc: 0.539, acc: 0.531\n",
      "[552] loss: 0.485, auc: 0.552, acc: 0.532\n",
      "[553] loss: 0.482, auc: 0.543, acc: 0.528\n",
      "[554] loss: 0.484, auc: 0.554, acc: 0.525\n",
      "[555] loss: 0.483, auc: 0.540, acc: 0.536\n",
      "[556] loss: 0.483, auc: 0.553, acc: 0.540\n",
      "[557] loss: 0.482, auc: 0.547, acc: 0.535\n",
      "[558] loss: 0.484, auc: 0.554, acc: 0.536\n",
      "[559] loss: 0.486, auc: 0.548, acc: 0.537\n",
      "[560] loss: 0.485, auc: 0.551, acc: 0.534\n",
      "[561] loss: 0.484, auc: 0.548, acc: 0.528\n",
      "[562] loss: 0.485, auc: 0.538, acc: 0.529\n",
      "[563] loss: 0.483, auc: 0.548, acc: 0.527\n",
      "[564] loss: 0.484, auc: 0.546, acc: 0.532\n",
      "[565] loss: 0.481, auc: 0.546, acc: 0.529\n",
      "[566] loss: 0.485, auc: 0.551, acc: 0.521\n",
      "[567] loss: 0.483, auc: 0.550, acc: 0.546\n",
      "[568] loss: 0.484, auc: 0.552, acc: 0.524\n",
      "[569] loss: 0.483, auc: 0.546, acc: 0.523\n",
      "[570] loss: 0.482, auc: 0.538, acc: 0.534\n",
      "[571] loss: 0.484, auc: 0.542, acc: 0.529\n",
      "[572] loss: 0.485, auc: 0.555, acc: 0.537\n",
      "[573] loss: 0.483, auc: 0.549, acc: 0.530\n",
      "[574] loss: 0.482, auc: 0.538, acc: 0.538\n",
      "[575] loss: 0.484, auc: 0.541, acc: 0.526\n",
      "[576] loss: 0.485, auc: 0.542, acc: 0.538\n",
      "[577] loss: 0.482, auc: 0.545, acc: 0.538\n",
      "[578] loss: 0.485, auc: 0.540, acc: 0.530\n",
      "[579] loss: 0.483, auc: 0.547, acc: 0.531\n",
      "[580] loss: 0.483, auc: 0.550, acc: 0.528\n",
      "[581] loss: 0.483, auc: 0.549, acc: 0.532\n",
      "[582] loss: 0.485, auc: 0.540, acc: 0.532\n",
      "[583] loss: 0.486, auc: 0.547, acc: 0.535\n",
      "[584] loss: 0.485, auc: 0.545, acc: 0.538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[585] loss: 0.485, auc: 0.539, acc: 0.533\n",
      "[586] loss: 0.482, auc: 0.553, acc: 0.529\n",
      "[587] loss: 0.485, auc: 0.551, acc: 0.536\n",
      "[588] loss: 0.486, auc: 0.549, acc: 0.529\n",
      "[589] loss: 0.484, auc: 0.542, acc: 0.535\n",
      "[590] loss: 0.484, auc: 0.540, acc: 0.533\n",
      "[591] loss: 0.484, auc: 0.546, acc: 0.525\n",
      "[592] loss: 0.483, auc: 0.550, acc: 0.531\n",
      "[593] loss: 0.482, auc: 0.548, acc: 0.531\n",
      "[594] loss: 0.484, auc: 0.543, acc: 0.533\n",
      "[595] loss: 0.485, auc: 0.537, acc: 0.529\n",
      "[596] loss: 0.484, auc: 0.548, acc: 0.525\n",
      "[597] loss: 0.485, auc: 0.549, acc: 0.536\n",
      "[598] loss: 0.484, auc: 0.552, acc: 0.526\n",
      "[599] loss: 0.484, auc: 0.546, acc: 0.537\n",
      "[600] loss: 0.482, auc: 0.546, acc: 0.526\n",
      "[601] loss: 0.484, auc: 0.547, acc: 0.538\n",
      "[602] loss: 0.483, auc: 0.555, acc: 0.511\n",
      "[603] loss: 0.481, auc: 0.533, acc: 0.527\n",
      "[604] loss: 0.484, auc: 0.543, acc: 0.525\n",
      "[605] loss: 0.486, auc: 0.546, acc: 0.530\n",
      "[606] loss: 0.484, auc: 0.548, acc: 0.530\n",
      "[607] loss: 0.483, auc: 0.538, acc: 0.536\n",
      "[608] loss: 0.483, auc: 0.543, acc: 0.535\n",
      "[609] loss: 0.483, auc: 0.551, acc: 0.524\n",
      "[610] loss: 0.483, auc: 0.544, acc: 0.533\n",
      "[611] loss: 0.482, auc: 0.541, acc: 0.524\n",
      "[612] loss: 0.483, auc: 0.547, acc: 0.542\n",
      "[613] loss: 0.483, auc: 0.540, acc: 0.535\n",
      "[614] loss: 0.483, auc: 0.550, acc: 0.534\n",
      "[615] loss: 0.483, auc: 0.550, acc: 0.529\n",
      "[616] loss: 0.482, auc: 0.545, acc: 0.539\n",
      "[617] loss: 0.481, auc: 0.546, acc: 0.526\n",
      "[618] loss: 0.484, auc: 0.552, acc: 0.527\n",
      "[619] loss: 0.484, auc: 0.552, acc: 0.532\n",
      "[620] loss: 0.481, auc: 0.539, acc: 0.528\n",
      "[621] loss: 0.482, auc: 0.552, acc: 0.528\n",
      "[622] loss: 0.482, auc: 0.538, acc: 0.527\n",
      "[623] loss: 0.483, auc: 0.543, acc: 0.538\n",
      "[624] loss: 0.485, auc: 0.554, acc: 0.524\n",
      "[625] loss: 0.483, auc: 0.554, acc: 0.520\n",
      "[626] loss: 0.485, auc: 0.547, acc: 0.535\n",
      "[627] loss: 0.483, auc: 0.545, acc: 0.525\n",
      "[628] loss: 0.480, auc: 0.545, acc: 0.528\n",
      "[629] loss: 0.485, auc: 0.545, acc: 0.525\n",
      "[630] loss: 0.482, auc: 0.550, acc: 0.528\n",
      "[631] loss: 0.484, auc: 0.551, acc: 0.541\n",
      "[632] loss: 0.482, auc: 0.552, acc: 0.524\n",
      "[633] loss: 0.481, auc: 0.536, acc: 0.522\n",
      "[634] loss: 0.483, auc: 0.554, acc: 0.533\n",
      "[635] loss: 0.484, auc: 0.549, acc: 0.526\n",
      "[636] loss: 0.482, auc: 0.539, acc: 0.526\n",
      "[637] loss: 0.485, auc: 0.541, acc: 0.532\n",
      "[638] loss: 0.484, auc: 0.553, acc: 0.525\n",
      "[639] loss: 0.484, auc: 0.542, acc: 0.534\n",
      "[640] loss: 0.483, auc: 0.549, acc: 0.532\n",
      "[641] loss: 0.484, auc: 0.540, acc: 0.535\n",
      "[642] loss: 0.482, auc: 0.554, acc: 0.528\n",
      "[643] loss: 0.485, auc: 0.544, acc: 0.533\n",
      "[644] loss: 0.481, auc: 0.540, acc: 0.529\n",
      "[645] loss: 0.483, auc: 0.545, acc: 0.523\n",
      "[646] loss: 0.485, auc: 0.540, acc: 0.536\n",
      "[647] loss: 0.483, auc: 0.552, acc: 0.523\n",
      "[648] loss: 0.485, auc: 0.546, acc: 0.529\n",
      "[649] loss: 0.483, auc: 0.539, acc: 0.535\n",
      "[650] loss: 0.485, auc: 0.551, acc: 0.530\n",
      "[651] loss: 0.484, auc: 0.549, acc: 0.539\n",
      "[652] loss: 0.484, auc: 0.550, acc: 0.541\n",
      "[653] loss: 0.484, auc: 0.530, acc: 0.524\n",
      "[654] loss: 0.485, auc: 0.542, acc: 0.538\n",
      "[655] loss: 0.482, auc: 0.554, acc: 0.526\n",
      "[656] loss: 0.483, auc: 0.541, acc: 0.536\n",
      "[657] loss: 0.485, auc: 0.546, acc: 0.527\n",
      "[658] loss: 0.483, auc: 0.549, acc: 0.541\n",
      "[659] loss: 0.484, auc: 0.542, acc: 0.540\n",
      "[660] loss: 0.480, auc: 0.542, acc: 0.537\n",
      "[661] loss: 0.483, auc: 0.537, acc: 0.534\n",
      "[662] loss: 0.484, auc: 0.543, acc: 0.537\n",
      "[663] loss: 0.485, auc: 0.548, acc: 0.536\n",
      "[664] loss: 0.482, auc: 0.541, acc: 0.536\n",
      "[665] loss: 0.484, auc: 0.548, acc: 0.527\n",
      "[666] loss: 0.482, auc: 0.534, acc: 0.532\n",
      "[667] loss: 0.483, auc: 0.540, acc: 0.537\n",
      "[668] loss: 0.482, auc: 0.549, acc: 0.544\n",
      "[669] loss: 0.484, auc: 0.553, acc: 0.540\n",
      "[670] loss: 0.483, auc: 0.546, acc: 0.527\n",
      "[671] loss: 0.484, auc: 0.541, acc: 0.534\n",
      "[672] loss: 0.483, auc: 0.545, acc: 0.532\n",
      "[673] loss: 0.485, auc: 0.546, acc: 0.533\n",
      "[674] loss: 0.483, auc: 0.545, acc: 0.536\n",
      "[675] loss: 0.483, auc: 0.547, acc: 0.534\n",
      "[676] loss: 0.484, auc: 0.549, acc: 0.534\n",
      "[677] loss: 0.479, auc: 0.547, acc: 0.529\n",
      "[678] loss: 0.484, auc: 0.552, acc: 0.530\n",
      "[679] loss: 0.482, auc: 0.545, acc: 0.535\n",
      "[680] loss: 0.485, auc: 0.546, acc: 0.531\n",
      "[681] loss: 0.483, auc: 0.551, acc: 0.537\n",
      "[682] loss: 0.483, auc: 0.545, acc: 0.543\n",
      "[683] loss: 0.482, auc: 0.541, acc: 0.535\n",
      "[684] loss: 0.484, auc: 0.553, acc: 0.526\n",
      "[685] loss: 0.485, auc: 0.547, acc: 0.535\n",
      "[686] loss: 0.483, auc: 0.548, acc: 0.533\n",
      "[687] loss: 0.485, auc: 0.544, acc: 0.536\n",
      "[688] loss: 0.482, auc: 0.544, acc: 0.526\n",
      "[689] loss: 0.483, auc: 0.554, acc: 0.525\n",
      "[690] loss: 0.484, auc: 0.550, acc: 0.534\n",
      "[691] loss: 0.483, auc: 0.541, acc: 0.537\n",
      "[692] loss: 0.482, auc: 0.540, acc: 0.537\n",
      "[693] loss: 0.484, auc: 0.540, acc: 0.537\n",
      "[694] loss: 0.481, auc: 0.549, acc: 0.516\n",
      "[695] loss: 0.483, auc: 0.546, acc: 0.537\n",
      "[696] loss: 0.486, auc: 0.544, acc: 0.529\n",
      "[697] loss: 0.484, auc: 0.546, acc: 0.536\n",
      "[698] loss: 0.485, auc: 0.553, acc: 0.537\n",
      "[699] loss: 0.483, auc: 0.547, acc: 0.538\n",
      "[700] loss: 0.482, auc: 0.540, acc: 0.539\n",
      "[701] loss: 0.483, auc: 0.543, acc: 0.533\n",
      "[702] loss: 0.485, auc: 0.547, acc: 0.531\n",
      "[703] loss: 0.481, auc: 0.536, acc: 0.533\n",
      "[704] loss: 0.484, auc: 0.542, acc: 0.532\n",
      "[705] loss: 0.482, auc: 0.550, acc: 0.537\n",
      "[706] loss: 0.483, auc: 0.553, acc: 0.530\n",
      "[707] loss: 0.482, auc: 0.540, acc: 0.524\n",
      "[708] loss: 0.484, auc: 0.551, acc: 0.541\n",
      "[709] loss: 0.485, auc: 0.543, acc: 0.532\n",
      "[710] loss: 0.482, auc: 0.541, acc: 0.543\n",
      "[711] loss: 0.482, auc: 0.549, acc: 0.537\n",
      "[712] loss: 0.483, auc: 0.541, acc: 0.534\n",
      "[713] loss: 0.484, auc: 0.542, acc: 0.541\n",
      "[714] loss: 0.484, auc: 0.544, acc: 0.536\n",
      "[715] loss: 0.482, auc: 0.546, acc: 0.530\n",
      "[716] loss: 0.482, auc: 0.545, acc: 0.539\n",
      "[717] loss: 0.483, auc: 0.553, acc: 0.530\n",
      "[718] loss: 0.482, auc: 0.549, acc: 0.530\n",
      "[719] loss: 0.484, auc: 0.547, acc: 0.536\n",
      "[720] loss: 0.485, auc: 0.540, acc: 0.532\n",
      "[721] loss: 0.483, auc: 0.540, acc: 0.524\n",
      "[722] loss: 0.486, auc: 0.545, acc: 0.533\n",
      "[723] loss: 0.483, auc: 0.543, acc: 0.536\n",
      "[724] loss: 0.483, auc: 0.547, acc: 0.531\n",
      "[725] loss: 0.481, auc: 0.545, acc: 0.513\n",
      "[726] loss: 0.484, auc: 0.543, acc: 0.535\n",
      "[727] loss: 0.483, auc: 0.552, acc: 0.529\n",
      "[728] loss: 0.480, auc: 0.540, acc: 0.535\n",
      "[729] loss: 0.484, auc: 0.546, acc: 0.536\n",
      "[730] loss: 0.481, auc: 0.548, acc: 0.530\n",
      "[731] loss: 0.483, auc: 0.548, acc: 0.531\n",
      "[732] loss: 0.484, auc: 0.544, acc: 0.534\n",
      "[733] loss: 0.481, auc: 0.541, acc: 0.516\n",
      "[734] loss: 0.483, auc: 0.540, acc: 0.535\n",
      "[735] loss: 0.479, auc: 0.551, acc: 0.534\n",
      "[736] loss: 0.484, auc: 0.555, acc: 0.526\n",
      "[737] loss: 0.484, auc: 0.544, acc: 0.531\n",
      "[738] loss: 0.482, auc: 0.543, acc: 0.532\n",
      "[739] loss: 0.486, auc: 0.539, acc: 0.539\n",
      "[740] loss: 0.482, auc: 0.539, acc: 0.525\n",
      "[741] loss: 0.484, auc: 0.534, acc: 0.531\n",
      "[742] loss: 0.485, auc: 0.558, acc: 0.529\n",
      "[743] loss: 0.483, auc: 0.540, acc: 0.535\n",
      "[744] loss: 0.482, auc: 0.544, acc: 0.534\n",
      "[745] loss: 0.485, auc: 0.546, acc: 0.540\n",
      "[746] loss: 0.484, auc: 0.544, acc: 0.534\n",
      "[747] loss: 0.482, auc: 0.549, acc: 0.527\n",
      "[748] loss: 0.483, auc: 0.545, acc: 0.533\n",
      "[749] loss: 0.480, auc: 0.537, acc: 0.519\n",
      "[750] loss: 0.483, auc: 0.537, acc: 0.526\n",
      "[751] loss: 0.482, auc: 0.552, acc: 0.536\n",
      "[752] loss: 0.485, auc: 0.531, acc: 0.522\n",
      "[753] loss: 0.487, auc: 0.550, acc: 0.541\n",
      "[754] loss: 0.481, auc: 0.549, acc: 0.529\n",
      "[755] loss: 0.483, auc: 0.540, acc: 0.533\n",
      "[756] loss: 0.483, auc: 0.550, acc: 0.542\n",
      "[757] loss: 0.482, auc: 0.542, acc: 0.527\n",
      "[758] loss: 0.484, auc: 0.550, acc: 0.538\n",
      "[759] loss: 0.480, auc: 0.552, acc: 0.542\n",
      "[760] loss: 0.480, auc: 0.544, acc: 0.536\n",
      "[761] loss: 0.484, auc: 0.547, acc: 0.531\n",
      "[762] loss: 0.483, auc: 0.542, acc: 0.540\n",
      "[763] loss: 0.485, auc: 0.547, acc: 0.534\n",
      "[764] loss: 0.482, auc: 0.537, acc: 0.524\n",
      "[765] loss: 0.481, auc: 0.556, acc: 0.512\n",
      "[766] loss: 0.484, auc: 0.546, acc: 0.522\n",
      "[767] loss: 0.483, auc: 0.550, acc: 0.543\n",
      "[768] loss: 0.482, auc: 0.544, acc: 0.533\n",
      "[769] loss: 0.481, auc: 0.544, acc: 0.539\n",
      "[770] loss: 0.482, auc: 0.556, acc: 0.528\n",
      "[771] loss: 0.485, auc: 0.543, acc: 0.535\n",
      "[772] loss: 0.483, auc: 0.553, acc: 0.540\n",
      "[773] loss: 0.482, auc: 0.549, acc: 0.529\n",
      "[774] loss: 0.483, auc: 0.543, acc: 0.529\n",
      "[775] loss: 0.483, auc: 0.548, acc: 0.535\n",
      "[776] loss: 0.483, auc: 0.544, acc: 0.537\n",
      "[777] loss: 0.481, auc: 0.545, acc: 0.536\n",
      "[778] loss: 0.485, auc: 0.542, acc: 0.534\n",
      "[779] loss: 0.481, auc: 0.545, acc: 0.533\n",
      "[780] loss: 0.481, auc: 0.543, acc: 0.539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[781] loss: 0.484, auc: 0.547, acc: 0.537\n",
      "[782] loss: 0.484, auc: 0.544, acc: 0.534\n",
      "[783] loss: 0.484, auc: 0.545, acc: 0.535\n",
      "[784] loss: 0.482, auc: 0.543, acc: 0.540\n",
      "[785] loss: 0.483, auc: 0.549, acc: 0.533\n",
      "[786] loss: 0.485, auc: 0.544, acc: 0.538\n",
      "[787] loss: 0.483, auc: 0.547, acc: 0.534\n",
      "[788] loss: 0.482, auc: 0.545, acc: 0.537\n",
      "[789] loss: 0.482, auc: 0.540, acc: 0.535\n",
      "[790] loss: 0.483, auc: 0.547, acc: 0.535\n",
      "[791] loss: 0.483, auc: 0.549, acc: 0.524\n",
      "[792] loss: 0.483, auc: 0.551, acc: 0.528\n",
      "[793] loss: 0.482, auc: 0.546, acc: 0.536\n",
      "[794] loss: 0.483, auc: 0.543, acc: 0.530\n",
      "[795] loss: 0.484, auc: 0.549, acc: 0.532\n",
      "[796] loss: 0.480, auc: 0.552, acc: 0.533\n",
      "[797] loss: 0.483, auc: 0.550, acc: 0.527\n",
      "[798] loss: 0.482, auc: 0.545, acc: 0.534\n",
      "[799] loss: 0.484, auc: 0.553, acc: 0.536\n",
      "[800] loss: 0.484, auc: 0.548, acc: 0.537\n",
      "[801] loss: 0.482, auc: 0.540, acc: 0.539\n",
      "[802] loss: 0.482, auc: 0.551, acc: 0.526\n",
      "[803] loss: 0.485, auc: 0.545, acc: 0.540\n",
      "[804] loss: 0.483, auc: 0.549, acc: 0.524\n",
      "[805] loss: 0.478, auc: 0.549, acc: 0.536\n",
      "[806] loss: 0.481, auc: 0.551, acc: 0.526\n",
      "[807] loss: 0.480, auc: 0.544, acc: 0.535\n",
      "[808] loss: 0.484, auc: 0.546, acc: 0.537\n",
      "[809] loss: 0.484, auc: 0.548, acc: 0.532\n",
      "[810] loss: 0.482, auc: 0.538, acc: 0.532\n",
      "[811] loss: 0.483, auc: 0.544, acc: 0.537\n",
      "[812] loss: 0.483, auc: 0.546, acc: 0.539\n",
      "[813] loss: 0.484, auc: 0.541, acc: 0.526\n",
      "[814] loss: 0.480, auc: 0.540, acc: 0.534\n",
      "[815] loss: 0.484, auc: 0.548, acc: 0.537\n",
      "[816] loss: 0.482, auc: 0.550, acc: 0.543\n",
      "[817] loss: 0.482, auc: 0.550, acc: 0.533\n",
      "[818] loss: 0.484, auc: 0.546, acc: 0.531\n",
      "[819] loss: 0.483, auc: 0.543, acc: 0.534\n",
      "[820] loss: 0.482, auc: 0.547, acc: 0.533\n",
      "[821] loss: 0.486, auc: 0.541, acc: 0.533\n",
      "[822] loss: 0.484, auc: 0.554, acc: 0.528\n",
      "[823] loss: 0.483, auc: 0.540, acc: 0.533\n",
      "[824] loss: 0.483, auc: 0.548, acc: 0.533\n",
      "[825] loss: 0.482, auc: 0.544, acc: 0.532\n",
      "[826] loss: 0.484, auc: 0.557, acc: 0.525\n",
      "[827] loss: 0.482, auc: 0.550, acc: 0.527\n",
      "[828] loss: 0.484, auc: 0.548, acc: 0.535\n",
      "[829] loss: 0.480, auc: 0.551, acc: 0.538\n",
      "[830] loss: 0.482, auc: 0.542, acc: 0.532\n",
      "[831] loss: 0.484, auc: 0.551, acc: 0.543\n",
      "[832] loss: 0.482, auc: 0.542, acc: 0.530\n",
      "[833] loss: 0.478, auc: 0.545, acc: 0.520\n",
      "[834] loss: 0.486, auc: 0.540, acc: 0.525\n",
      "[835] loss: 0.484, auc: 0.541, acc: 0.530\n",
      "[836] loss: 0.484, auc: 0.553, acc: 0.536\n",
      "[837] loss: 0.484, auc: 0.548, acc: 0.544\n",
      "[838] loss: 0.482, auc: 0.545, acc: 0.533\n",
      "[839] loss: 0.481, auc: 0.538, acc: 0.526\n",
      "[840] loss: 0.483, auc: 0.546, acc: 0.536\n",
      "[841] loss: 0.482, auc: 0.543, acc: 0.534\n",
      "[842] loss: 0.484, auc: 0.538, acc: 0.533\n",
      "[843] loss: 0.482, auc: 0.549, acc: 0.536\n",
      "[844] loss: 0.480, auc: 0.540, acc: 0.534\n",
      "[845] loss: 0.483, auc: 0.536, acc: 0.525\n",
      "[846] loss: 0.484, auc: 0.545, acc: 0.534\n",
      "[847] loss: 0.481, auc: 0.542, acc: 0.536\n",
      "[848] loss: 0.482, auc: 0.555, acc: 0.530\n",
      "[849] loss: 0.483, auc: 0.547, acc: 0.544\n",
      "[850] loss: 0.478, auc: 0.536, acc: 0.519\n",
      "[851] loss: 0.485, auc: 0.543, acc: 0.537\n",
      "[852] loss: 0.483, auc: 0.549, acc: 0.517\n",
      "[853] loss: 0.483, auc: 0.550, acc: 0.526\n",
      "[854] loss: 0.482, auc: 0.541, acc: 0.534\n",
      "[855] loss: 0.481, auc: 0.549, acc: 0.526\n",
      "[856] loss: 0.485, auc: 0.543, acc: 0.542\n",
      "[857] loss: 0.484, auc: 0.539, acc: 0.526\n",
      "[858] loss: 0.482, auc: 0.549, acc: 0.533\n",
      "[859] loss: 0.483, auc: 0.545, acc: 0.533\n",
      "[860] loss: 0.484, auc: 0.555, acc: 0.531\n",
      "[861] loss: 0.483, auc: 0.546, acc: 0.533\n",
      "[862] loss: 0.481, auc: 0.551, acc: 0.535\n",
      "[863] loss: 0.481, auc: 0.531, acc: 0.516\n",
      "[864] loss: 0.486, auc: 0.554, acc: 0.508\n",
      "[865] loss: 0.484, auc: 0.549, acc: 0.536\n",
      "[866] loss: 0.482, auc: 0.552, acc: 0.530\n",
      "[867] loss: 0.483, auc: 0.545, acc: 0.537\n",
      "[868] loss: 0.485, auc: 0.553, acc: 0.529\n",
      "[869] loss: 0.481, auc: 0.554, acc: 0.535\n",
      "[870] loss: 0.483, auc: 0.547, acc: 0.525\n",
      "[871] loss: 0.483, auc: 0.547, acc: 0.529\n",
      "[872] loss: 0.481, auc: 0.546, acc: 0.538\n",
      "[873] loss: 0.483, auc: 0.551, acc: 0.542\n",
      "[874] loss: 0.482, auc: 0.539, acc: 0.534\n",
      "[875] loss: 0.483, auc: 0.537, acc: 0.538\n",
      "[876] loss: 0.483, auc: 0.546, acc: 0.538\n",
      "[877] loss: 0.483, auc: 0.549, acc: 0.542\n",
      "[878] loss: 0.481, auc: 0.541, acc: 0.527\n",
      "[879] loss: 0.482, auc: 0.546, acc: 0.531\n",
      "[880] loss: 0.482, auc: 0.547, acc: 0.536\n",
      "[881] loss: 0.484, auc: 0.540, acc: 0.534\n",
      "[882] loss: 0.482, auc: 0.547, acc: 0.543\n",
      "[883] loss: 0.481, auc: 0.551, acc: 0.524\n",
      "[884] loss: 0.483, auc: 0.554, acc: 0.527\n",
      "[885] loss: 0.485, auc: 0.547, acc: 0.531\n",
      "[886] loss: 0.483, auc: 0.537, acc: 0.533\n",
      "[887] loss: 0.483, auc: 0.541, acc: 0.537\n",
      "[888] loss: 0.484, auc: 0.544, acc: 0.538\n",
      "[889] loss: 0.481, auc: 0.551, acc: 0.534\n",
      "[890] loss: 0.482, auc: 0.539, acc: 0.534\n",
      "[891] loss: 0.483, auc: 0.553, acc: 0.536\n",
      "[892] loss: 0.482, auc: 0.554, acc: 0.529\n",
      "[893] loss: 0.483, auc: 0.547, acc: 0.546\n",
      "[894] loss: 0.484, auc: 0.546, acc: 0.537\n",
      "[895] loss: 0.482, auc: 0.539, acc: 0.524\n",
      "[896] loss: 0.483, auc: 0.547, acc: 0.543\n",
      "[897] loss: 0.480, auc: 0.547, acc: 0.540\n",
      "[898] loss: 0.482, auc: 0.549, acc: 0.545\n",
      "[899] loss: 0.482, auc: 0.548, acc: 0.538\n",
      "[900] loss: 0.483, auc: 0.543, acc: 0.535\n",
      "[901] loss: 0.484, auc: 0.547, acc: 0.538\n",
      "[902] loss: 0.482, auc: 0.553, acc: 0.533\n",
      "[903] loss: 0.482, auc: 0.544, acc: 0.538\n",
      "[904] loss: 0.480, auc: 0.550, acc: 0.524\n",
      "[905] loss: 0.484, auc: 0.548, acc: 0.528\n",
      "[906] loss: 0.481, auc: 0.548, acc: 0.532\n",
      "[907] loss: 0.481, auc: 0.548, acc: 0.536\n",
      "[908] loss: 0.484, auc: 0.540, acc: 0.534\n",
      "[909] loss: 0.482, auc: 0.540, acc: 0.533\n",
      "[910] loss: 0.482, auc: 0.539, acc: 0.527\n",
      "[911] loss: 0.483, auc: 0.538, acc: 0.537\n",
      "[912] loss: 0.484, auc: 0.548, acc: 0.537\n",
      "[913] loss: 0.482, auc: 0.542, acc: 0.535\n",
      "[914] loss: 0.483, auc: 0.543, acc: 0.544\n",
      "[915] loss: 0.483, auc: 0.544, acc: 0.536\n",
      "[916] loss: 0.483, auc: 0.544, acc: 0.532\n",
      "[917] loss: 0.482, auc: 0.539, acc: 0.534\n",
      "[918] loss: 0.484, auc: 0.554, acc: 0.529\n",
      "[919] loss: 0.482, auc: 0.549, acc: 0.535\n",
      "[920] loss: 0.483, auc: 0.544, acc: 0.533\n",
      "[921] loss: 0.482, auc: 0.538, acc: 0.525\n",
      "[922] loss: 0.482, auc: 0.553, acc: 0.539\n",
      "[923] loss: 0.482, auc: 0.543, acc: 0.536\n",
      "[924] loss: 0.484, auc: 0.554, acc: 0.530\n",
      "[925] loss: 0.480, auc: 0.539, acc: 0.520\n",
      "[926] loss: 0.483, auc: 0.553, acc: 0.531\n",
      "[927] loss: 0.483, auc: 0.538, acc: 0.524\n",
      "[928] loss: 0.481, auc: 0.538, acc: 0.531\n",
      "[929] loss: 0.478, auc: 0.545, acc: 0.534\n",
      "[930] loss: 0.484, auc: 0.549, acc: 0.529\n",
      "[931] loss: 0.484, auc: 0.542, acc: 0.526\n",
      "[932] loss: 0.482, auc: 0.543, acc: 0.536\n",
      "[933] loss: 0.481, auc: 0.544, acc: 0.529\n",
      "[934] loss: 0.484, auc: 0.545, acc: 0.537\n",
      "[935] loss: 0.484, auc: 0.540, acc: 0.533\n",
      "[936] loss: 0.481, auc: 0.545, acc: 0.530\n",
      "[937] loss: 0.485, auc: 0.551, acc: 0.543\n",
      "[938] loss: 0.484, auc: 0.548, acc: 0.533\n",
      "[939] loss: 0.481, auc: 0.537, acc: 0.534\n",
      "[940] loss: 0.484, auc: 0.543, acc: 0.538\n",
      "[941] loss: 0.483, auc: 0.539, acc: 0.530\n",
      "[942] loss: 0.484, auc: 0.546, acc: 0.534\n",
      "[943] loss: 0.483, auc: 0.539, acc: 0.535\n",
      "[944] loss: 0.482, auc: 0.550, acc: 0.521\n",
      "[945] loss: 0.484, auc: 0.546, acc: 0.539\n",
      "[946] loss: 0.482, auc: 0.543, acc: 0.533\n",
      "[947] loss: 0.482, auc: 0.545, acc: 0.528\n",
      "[948] loss: 0.481, auc: 0.540, acc: 0.532\n",
      "[949] loss: 0.483, auc: 0.543, acc: 0.541\n",
      "[950] loss: 0.483, auc: 0.548, acc: 0.537\n",
      "[951] loss: 0.482, auc: 0.540, acc: 0.526\n",
      "[952] loss: 0.481, auc: 0.549, acc: 0.527\n",
      "[953] loss: 0.484, auc: 0.548, acc: 0.529\n",
      "[954] loss: 0.481, auc: 0.546, acc: 0.534\n",
      "[955] loss: 0.484, auc: 0.554, acc: 0.529\n",
      "[956] loss: 0.483, auc: 0.542, acc: 0.529\n",
      "[957] loss: 0.484, auc: 0.557, acc: 0.532\n",
      "[958] loss: 0.484, auc: 0.543, acc: 0.544\n",
      "[959] loss: 0.482, auc: 0.546, acc: 0.537\n",
      "[960] loss: 0.480, auc: 0.548, acc: 0.544\n",
      "[961] loss: 0.483, auc: 0.543, acc: 0.539\n",
      "[962] loss: 0.482, auc: 0.541, acc: 0.536\n",
      "[963] loss: 0.483, auc: 0.538, acc: 0.522\n",
      "[964] loss: 0.481, auc: 0.544, acc: 0.542\n",
      "[965] loss: 0.484, auc: 0.547, acc: 0.535\n",
      "[966] loss: 0.481, auc: 0.552, acc: 0.540\n",
      "[967] loss: 0.484, auc: 0.543, acc: 0.532\n",
      "[968] loss: 0.484, auc: 0.544, acc: 0.535\n",
      "[969] loss: 0.480, auc: 0.538, acc: 0.524\n",
      "[970] loss: 0.484, auc: 0.543, acc: 0.536\n",
      "[971] loss: 0.481, auc: 0.539, acc: 0.533\n",
      "[972] loss: 0.483, auc: 0.547, acc: 0.536\n",
      "[973] loss: 0.481, auc: 0.546, acc: 0.542\n",
      "[974] loss: 0.484, auc: 0.545, acc: 0.535\n",
      "[975] loss: 0.480, auc: 0.541, acc: 0.537\n",
      "[976] loss: 0.482, auc: 0.543, acc: 0.537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[977] loss: 0.481, auc: 0.555, acc: 0.530\n",
      "[978] loss: 0.479, auc: 0.551, acc: 0.526\n",
      "[979] loss: 0.485, auc: 0.555, acc: 0.537\n",
      "[980] loss: 0.481, auc: 0.546, acc: 0.535\n",
      "[981] loss: 0.483, auc: 0.551, acc: 0.537\n",
      "[982] loss: 0.484, auc: 0.552, acc: 0.528\n",
      "[983] loss: 0.480, auc: 0.550, acc: 0.536\n",
      "[984] loss: 0.483, auc: 0.559, acc: 0.527\n",
      "[985] loss: 0.482, auc: 0.546, acc: 0.524\n",
      "[986] loss: 0.482, auc: 0.545, acc: 0.531\n",
      "[987] loss: 0.484, auc: 0.528, acc: 0.511\n",
      "[988] loss: 0.485, auc: 0.552, acc: 0.537\n",
      "[989] loss: 0.482, auc: 0.541, acc: 0.539\n",
      "[990] loss: 0.483, auc: 0.549, acc: 0.530\n",
      "[991] loss: 0.482, auc: 0.550, acc: 0.542\n",
      "[992] loss: 0.481, auc: 0.555, acc: 0.531\n",
      "[993] loss: 0.480, auc: 0.546, acc: 0.543\n",
      "[994] loss: 0.483, auc: 0.543, acc: 0.538\n",
      "[995] loss: 0.481, auc: 0.548, acc: 0.529\n",
      "[996] loss: 0.484, auc: 0.548, acc: 0.544\n",
      "[997] loss: 0.483, auc: 0.549, acc: 0.538\n",
      "[998] loss: 0.483, auc: 0.551, acc: 0.535\n",
      "[999] loss: 0.481, auc: 0.549, acc: 0.541\n",
      "[1000] loss: 0.480, auc: 0.549, acc: 0.526\n",
      "Finished Training\n",
      "Now computing Z=HW^T, then will compute S...\n",
      "torch.Size([80, 20]) torch.Size([20, 5]) torch.Size([5, 20]) torch.Size([100, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1832,  0.7100,  0.1094,  ...,  0.1094,  0.1095,  0.6616],\n",
       "        [ 0.1327, -0.0273,  0.0700,  ...,  0.0700,  0.0700, -0.0746],\n",
       "        [ 0.1828,  0.7104,  0.1093,  ...,  0.1093,  0.1093,  0.6616],\n",
       "        ...,\n",
       "        [ 0.3086,  0.0274,  0.1566,  ...,  0.1566,  0.1566,  0.1971],\n",
       "        [ 0.3104,  0.0247,  0.1575,  ...,  0.1575,  0.1575,  0.1946],\n",
       "        [ 0.1386, -0.0416,  0.0734,  ...,  0.0734,  0.0734, -0.0919]],\n",
       "       grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S, H, W, train, test=pu_learning_new(5, embeddings_drugs, embeddings_targets, dp_net, batch_size=1, lr=1e-3, n_epochs=1000, train_size=0.5)\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 20)\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_drugs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
